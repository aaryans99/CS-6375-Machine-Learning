{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaryans99/CS-6375-Machine-Learning/blob/main/Assignment%202/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6375 ML - Assignment 2**\n",
        "\n",
        "Aaryan Singh - axc230019\n",
        "\n",
        "Nikunj Gohil - ndg220000\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yY8usvnMhUjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "Dataset Information\n",
        "\n",
        "> The inputs are as follows\\\n",
        "X1=the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)\\\n",
        "X2=the house age (unit: year)\\\n",
        "X3=the distance to the nearest MRT station (unit: meter)\\\n",
        "X4=the number of convenience stores in the living circle on foot (integer)\\\n",
        "X5=the geographic coordinate, latitude. (unit: degree)\\\n",
        "X6=the geographic coordinate, longitude. (unit: degree)\n",
        "\n",
        "The output is as follow\n",
        "Y= house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)\n",
        "\n",
        "1. Importing Libraries\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8r22kOulfrhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "ux2UcXlFkq0v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Importing Dataset and Preprocessing"
      ],
      "metadata": {
        "id": "8ysdSg31lk_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/aaryans99/CS-6375-Machine-Learning/raw/main/Real%20estate%20valuation%20data%20set.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "H8Y8Yd7tlwMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348a3364-1f93-4c22-e4ae-57fb6e304bd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 414 entries, 0 to 413\n",
            "Data columns (total 8 columns):\n",
            " #   Column                                  Non-Null Count  Dtype  \n",
            "---  ------                                  --------------  -----  \n",
            " 0   No                                      414 non-null    int64  \n",
            " 1   X1 transaction date                     414 non-null    float64\n",
            " 2   X2 house age                            414 non-null    float64\n",
            " 3   X3 distance to the nearest MRT station  414 non-null    float64\n",
            " 4   X4 number of convenience stores         414 non-null    int64  \n",
            " 5   X5 latitude                             414 non-null    float64\n",
            " 6   X6 longitude                            414 non-null    float64\n",
            " 7   Y house price of unit area              414 non-null    float64\n",
            "dtypes: float64(6), int64(2)\n",
            "memory usage: 26.0 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for null values\n",
        "df.isna().sum()\n",
        "# remove unwanted column\n",
        "df = df.drop(columns=[\"No\"], axis=1)"
      ],
      "metadata": {
        "id": "QdEKwwpVm8uL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert TWD per Ping to USD per square meter\n",
        "exchange_rate_TWD_to_USD = 0.031\n",
        "ping_to_square_meter = 3.3\n",
        "df['Y house price of unit area'] = (df['Y house price of unit area'] * exchange_rate_TWD_to_USD) / ping_to_square_meter"
      ],
      "metadata": {
        "id": "zwh9rdEPndQu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heat Map\n",
        "corr_matrix = df.corr()\n",
        "plt.imshow(corr_matrix, cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.xticks(np.arange(len(corr_matrix.columns)), corr_matrix.columns, rotation='vertical')\n",
        "plt.yticks(np.arange(len(corr_matrix.columns)), corr_matrix.columns)\n",
        "plt.title('Correlation Plot')\n",
        "plt.show()\n",
        "M=corr_matrix\n",
        "M"
      ],
      "metadata": {
        "id": "ZGcTqb6OoZpq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "497a8571-8b3c-4203-a7ca-bed64a649dbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        X1 transaction date  X2 house age  \\\n",
              "X1 transaction date                                1.000000      0.017549   \n",
              "X2 house age                                       0.017549      1.000000   \n",
              "X3 distance to the nearest MRT station             0.060880      0.025622   \n",
              "X4 number of convenience stores                    0.009635      0.049593   \n",
              "X5 latitude                                        0.035058      0.054420   \n",
              "X6 longitude                                      -0.041082     -0.048520   \n",
              "Y house price of unit area                         0.087491     -0.210567   \n",
              "\n",
              "                                        X3 distance to the nearest MRT station  \\\n",
              "X1 transaction date                                                   0.060880   \n",
              "X2 house age                                                          0.025622   \n",
              "X3 distance to the nearest MRT station                                1.000000   \n",
              "X4 number of convenience stores                                      -0.602519   \n",
              "X5 latitude                                                          -0.591067   \n",
              "X6 longitude                                                         -0.806317   \n",
              "Y house price of unit area                                           -0.673613   \n",
              "\n",
              "                                        X4 number of convenience stores  \\\n",
              "X1 transaction date                                            0.009635   \n",
              "X2 house age                                                   0.049593   \n",
              "X3 distance to the nearest MRT station                        -0.602519   \n",
              "X4 number of convenience stores                                1.000000   \n",
              "X5 latitude                                                    0.444143   \n",
              "X6 longitude                                                   0.449099   \n",
              "Y house price of unit area                                     0.571005   \n",
              "\n",
              "                                        X5 latitude  X6 longitude  \\\n",
              "X1 transaction date                        0.035058     -0.041082   \n",
              "X2 house age                               0.054420     -0.048520   \n",
              "X3 distance to the nearest MRT station    -0.591067     -0.806317   \n",
              "X4 number of convenience stores            0.444143      0.449099   \n",
              "X5 latitude                                1.000000      0.412924   \n",
              "X6 longitude                               0.412924      1.000000   \n",
              "Y house price of unit area                 0.546307      0.523287   \n",
              "\n",
              "                                        Y house price of unit area  \n",
              "X1 transaction date                                       0.087491  \n",
              "X2 house age                                             -0.210567  \n",
              "X3 distance to the nearest MRT station                   -0.673613  \n",
              "X4 number of convenience stores                           0.571005  \n",
              "X5 latitude                                               0.546307  \n",
              "X6 longitude                                              0.523287  \n",
              "Y house price of unit area                                1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49544723-ffe2-4e9f-ad7a-b536368972d1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1 transaction date</th>\n",
              "      <th>X2 house age</th>\n",
              "      <th>X3 distance to the nearest MRT station</th>\n",
              "      <th>X4 number of convenience stores</th>\n",
              "      <th>X5 latitude</th>\n",
              "      <th>X6 longitude</th>\n",
              "      <th>Y house price of unit area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1 transaction date</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.017549</td>\n",
              "      <td>0.060880</td>\n",
              "      <td>0.009635</td>\n",
              "      <td>0.035058</td>\n",
              "      <td>-0.041082</td>\n",
              "      <td>0.087491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2 house age</th>\n",
              "      <td>0.017549</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.025622</td>\n",
              "      <td>0.049593</td>\n",
              "      <td>0.054420</td>\n",
              "      <td>-0.048520</td>\n",
              "      <td>-0.210567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3 distance to the nearest MRT station</th>\n",
              "      <td>0.060880</td>\n",
              "      <td>0.025622</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.602519</td>\n",
              "      <td>-0.591067</td>\n",
              "      <td>-0.806317</td>\n",
              "      <td>-0.673613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4 number of convenience stores</th>\n",
              "      <td>0.009635</td>\n",
              "      <td>0.049593</td>\n",
              "      <td>-0.602519</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.444143</td>\n",
              "      <td>0.449099</td>\n",
              "      <td>0.571005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5 latitude</th>\n",
              "      <td>0.035058</td>\n",
              "      <td>0.054420</td>\n",
              "      <td>-0.591067</td>\n",
              "      <td>0.444143</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.412924</td>\n",
              "      <td>0.546307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6 longitude</th>\n",
              "      <td>-0.041082</td>\n",
              "      <td>-0.048520</td>\n",
              "      <td>-0.806317</td>\n",
              "      <td>0.449099</td>\n",
              "      <td>0.412924</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.523287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Y house price of unit area</th>\n",
              "      <td>0.087491</td>\n",
              "      <td>-0.210567</td>\n",
              "      <td>-0.673613</td>\n",
              "      <td>0.571005</td>\n",
              "      <td>0.546307</td>\n",
              "      <td>0.523287</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49544723-ffe2-4e9f-ad7a-b536368972d1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49544723-ffe2-4e9f-ad7a-b536368972d1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49544723-ffe2-4e9f-ad7a-b536368972d1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1754f28a-35aa-4f98-b487-25a60e16f122\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1754f28a-35aa-4f98-b487-25a60e16f122')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1754f28a-35aa-4f98-b487-25a60e16f122 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing the features by removing the mean and scaling to unit variance.\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)"
      ],
      "metadata": {
        "id": "g1NKOaeno_FH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Splitting Data\n",
        "\n"
      ],
      "metadata": {
        "id": "4d8S92UrpIQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = scaled_df['Y house price of unit area']\n",
        "X = scaled_df[['X4 number of convenience stores', 'X5 latitude', 'X6 longitude']]\n",
        "scaled_df.info()\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=90)\n",
        "\n",
        "print(\"Training Set:\")\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(Y_train))\n",
        "print(\"\\nTesting Set:\")\n",
        "print(np.shape(X_test))\n",
        "print(np.shape(Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV2QqOlmpLiF",
        "outputId": "9933f413-0c3a-4f5a-874e-912378d4e787"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 414 entries, 0 to 413\n",
            "Data columns (total 7 columns):\n",
            " #   Column                                  Non-Null Count  Dtype  \n",
            "---  ------                                  --------------  -----  \n",
            " 0   X1 transaction date                     414 non-null    float64\n",
            " 1   X2 house age                            414 non-null    float64\n",
            " 2   X3 distance to the nearest MRT station  414 non-null    float64\n",
            " 3   X4 number of convenience stores         414 non-null    float64\n",
            " 4   X5 latitude                             414 non-null    float64\n",
            " 5   X6 longitude                            414 non-null    float64\n",
            " 6   Y house price of unit area              414 non-null    float64\n",
            "dtypes: float64(7)\n",
            "memory usage: 22.8 KB\n",
            "Training Set:\n",
            "(331, 3)\n",
            "(331,)\n",
            "\n",
            "Testing Set:\n",
            "(83, 3)\n",
            "(83,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Neural Net Creation\n",
        "*   Defining the activation functions, their derivatives and the loss function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gXfgj0IlqrGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1 - fx)\n",
        "\n",
        "def tanh(x):\n",
        "  # tanh activation function: f(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))\n",
        "  return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "def deriv_tanh(x):\n",
        "  # Derivative of tanh: f'(x) = (1 - f(x)**2)\n",
        "  fx = tanh(x)\n",
        "  return (1 - (fx)**2)\n",
        "\n",
        "def reLu(x):\n",
        "  # ReLu activation function: f(x) = 0 if x <= 0 else x\n",
        "  return max(0, x)\n",
        "\n",
        "def deriv_reLu(x):\n",
        "  # Derivative of ReLu: f'(x) = 0 if x <= 0 else 1\n",
        "  return max(0, 1)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "  # y_true and y_pred are numpy arrays of the same length.\n",
        "  return ((y_true - y_pred) ** 2).mean()"
      ],
      "metadata": {
        "id": "j-KixilG-HJC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Defining our Neural Network\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pwFzAeBwr8uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OurNeuralNetwork:\n",
        "  '''\n",
        "  A neural network with:\n",
        "    - 3 inputs\n",
        "    - a hidden layer with 2 neurons (h1, h2)\n",
        "    - an output layer with 1 neuron (o1)\n",
        "  '''\n",
        "  def __init__(self, act):\n",
        "    # Assigning activation function\n",
        "    if act == 'sigmoid':\n",
        "      self.act = sigmoid\n",
        "      self.deriv_act = deriv_sigmoid\n",
        "    elif act == 'tanh':\n",
        "      self.act = tanh\n",
        "      self.deriv_act = deriv_tanh\n",
        "    elif act == 'reLu':\n",
        "      self.act = reLu\n",
        "      self.deriv_act = deriv_reLu\n",
        "    else:\n",
        "      print(\"Invalid activation fuction\")\n",
        "      exit(1)\n",
        "\n",
        "    # Weights\n",
        "    # i/p -> hidden\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "\n",
        "    # hidden -> o/p\n",
        "    self.w7 = np.random.normal()\n",
        "    self.w8 = np.random.normal()\n",
        "\n",
        "    # Biases\n",
        "    # i/p\n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "\n",
        "    # hidden\n",
        "    self.b3 = np.random.normal()\n",
        "\n",
        "    print(\"Weights:\", self.w1, self.w2, self.w3, self.w4, self.w5, self.w6, self.w7, self.w8)\n",
        "    print(\"Bias:\", self.b1, self.b2, self.b3)\n",
        "\n",
        "    # Forward pass\n",
        "  def feedforward(self, x):\n",
        "    # x is a numpy array with 2 elements.\n",
        "    h1 = self.act(self.w1 * x[0] + self.w2 * x[1] + self.w3 * x[2] + self.b1)\n",
        "    h2 = self.act(self.w4 * x[0] + self.w5 * x[1] + self.w6 * x[2] + self.b2)\n",
        "    o1 = self.act(self.w7 * h1 + self.w8 * h2 + self.b3)\n",
        "    return o1\n",
        "\n",
        "  def train(self, data, all_y_trues, learn_rate, epochs):\n",
        "    '''\n",
        "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
        "    - all_y_trues is a numpy array with n elements.\n",
        "      Elements in all_y_trues correspond to those in data.\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # --- Do a feedforward (we'll need these values later)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.w3 * x[2] + self.b1\n",
        "        h1 = self.act(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w4 * x[0] + self.w5 * x[1] + self.w6 * x[2] + self.b2\n",
        "        h2 = self.act(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w7 * h1 + self.w8 * h2 + self.b3\n",
        "        o1 = self.act(sum_o1)\n",
        "        y_pred = o1\n",
        "\n",
        "        # --- Calculate partial derivatives.\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neuron o1\n",
        "        d_ypred_d_w7 = h1 * self.deriv_act(sum_o1)\n",
        "        d_ypred_d_w8 = h2 * self.deriv_act(sum_o1)\n",
        "        d_ypred_d_b3 = self.deriv_act(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w7 * self.deriv_act(sum_o1)\n",
        "        d_ypred_d_h2 = self.w8 * self.deriv_act(sum_o1)\n",
        "\n",
        "        # Neuron h1\n",
        "        d_h1_d_w1 = x[0] * self.deriv_act(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * self.deriv_act(sum_h1)\n",
        "        d_h1_d_w3 = x[2] * self.deriv_act(sum_h1)\n",
        "        d_h1_d_b1 = self.deriv_act(sum_h1)\n",
        "\n",
        "        # Neuron h2\n",
        "        d_h2_d_w4 = x[0] * self.deriv_act(sum_h2)\n",
        "        d_h2_d_w5 = x[1] * self.deriv_act(sum_h2)\n",
        "        d_h2_d_w6 = x[2] * self.deriv_act(sum_h2)\n",
        "        d_h2_d_b2 = self.deriv_act(sum_h2)\n",
        "\n",
        "        # --- Update weights and biases\n",
        "        # Neuron h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w3\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neuron h2\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w6\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neuron o1\n",
        "        self.w7 -= learn_rate * d_L_d_ypred * d_ypred_d_w7\n",
        "        self.w8 -= learn_rate * d_L_d_ypred * d_ypred_d_w8\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      # --- Calculate total loss at the end of each epoch\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "        loss = mse_loss(all_y_trues, y_preds)\n",
        "        r2_loss = r2_score(y_preds, all_y_trues)\n",
        "        print(\"Epoch %d loss: %.3f r2: %.3f\" % (epoch, loss, r2_loss))\n",
        "\n",
        "    return [self.w1, self.w2, self.w3, self.w4, self.w5, self.w6, self.w7, self.w8, self.b1, self.b2, self.b3]\n",
        "\n",
        "  def train_relu(self, data, all_y_trues, learn_rate, epochs):\n",
        "    '''\n",
        "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
        "    - all_y_trues is a numpy array with n elements.\n",
        "      Elements in all_y_trues correspond to those in data.\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # --- Do a feedforward (we'll need these values later)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.w3 * x[2] + self.b1\n",
        "        h1 = self.act(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w4 * x[0] + self.w5 * x[1] + self.w6 * x[2] + self.b2\n",
        "        h2 = self.act(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w7 * h1 + self.w8 * h2 + self.b3\n",
        "        o1 = sum_o1\n",
        "        y_pred = o1\n",
        "\n",
        "        # --- Calculate partial derivatives.\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neuron o1\n",
        "        d_ypred_d_w7 = h1 * self.deriv_act(sum_o1)\n",
        "        d_ypred_d_w8 = h2 * self.deriv_act(sum_o1)\n",
        "        d_ypred_d_b3 = self.deriv_act(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w7 * self.deriv_act(sum_o1)\n",
        "        d_ypred_d_h2 = self.w8 * self.deriv_act(sum_o1)\n",
        "\n",
        "        # Neuron h1\n",
        "        d_h1_d_w1 = x[0] * self.deriv_act(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * self.deriv_act(sum_h1)\n",
        "        d_h1_d_w3 = x[2] * self.deriv_act(sum_h1)\n",
        "        d_h1_d_b1 = self.deriv_act(sum_h1)\n",
        "\n",
        "        # Neuron h2\n",
        "        d_h2_d_w4 = x[0] * self.deriv_act(sum_h2)\n",
        "        d_h2_d_w5 = x[1] * self.deriv_act(sum_h2)\n",
        "        d_h2_d_w6 = x[2] * self.deriv_act(sum_h2)\n",
        "        d_h2_d_b2 = self.deriv_act(sum_h2)\n",
        "\n",
        "        # --- Update weights and biases\n",
        "        # Neuron h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w3\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neuron h2\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w6\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neuron o1\n",
        "        self.w7 -= learn_rate * d_L_d_ypred * d_ypred_d_w7\n",
        "        self.w8 -= learn_rate * d_L_d_ypred * d_ypred_d_w8\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      # --- Calculate total loss at the end of each epoch\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "        loss = mse_loss(all_y_trues, y_preds)\n",
        "        r2_loss = r2_score(y_preds, all_y_trues)\n",
        "        print(\"Epoch %d loss: %.3f r2: %.3f\" % (epoch, loss, r2_loss))\n",
        "\n",
        "    return [self.w1, self.w2, self.w3, self.w4, self.w5, self.w6, self.w7, self.w8, self.b1, self.b2, self.b3]\n",
        "\n",
        "  def test(self, data, all_y_trues, w):\n",
        "    self.w1 = w[0]\n",
        "    self.w2 = w[1]\n",
        "    self.w3 = w[2]\n",
        "    self.w4 = w[3]\n",
        "    self.w5 = w[4]\n",
        "    self.w6 = w[5]\n",
        "    self.w7 = w[6]\n",
        "    self.w8 = w[7]\n",
        "    self.b1 = w[8]\n",
        "    self.b2 = w[9]\n",
        "    self.b3 = w[10]\n",
        "    y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "    loss = mse_loss(all_y_trues, y_preds)\n",
        "    r2_loss = r2_score(y_preds, all_y_trues)\n",
        "    print(\"Loss: %.3f, r2: %.3f\" % (loss, r2_loss))"
      ],
      "metadata": {
        "id": "SDt5IYY5r_K-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Training neural network with different activation functions\n",
        "\n"
      ],
      "metadata": {
        "id": "UGit1ljssZPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = OurNeuralNetwork('sigmoid')\n",
        "for i in range(10,100,10):\n",
        "  weights = network.train(np.array(X_train), np.array(Y_train),i/100,1000)\n",
        "  print(\"Weights:\", weights[:8])\n",
        "  print(\"Bias:\",weights[8:])\n",
        "  print(\"Learning rate:\",i/100)\n",
        "  print(\"Performance for test dataset\")\n",
        "  network.test(np.array(X_test), np.array(Y_test), weights)\n",
        "  print(\"Performance for training dataset\")\n",
        "  network.test(np.array(X_train), np.array(Y_train), weights)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndiH2JV0se77",
        "outputId": "768ed156-a501-4a95-9168-0ffbd8172b4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: 1.4510544225332074 -0.6978176580503469 -0.31051244224054486 -0.0438419585524418 0.3824328619910194 0.9299155046593748 -0.6747740107697844 -0.16511132056218117\n",
            "Bias: 1.1658810692637949 -0.1998553497106449 0.35982090199060185\n",
            "Epoch 0 loss: 1.014 r2: -889.047\n",
            "Epoch 10 loss: 0.853 r2: -9.544\n",
            "Epoch 20 loss: 0.847 r2: -8.644\n",
            "Epoch 30 loss: 0.841 r2: -8.123\n",
            "Epoch 40 loss: 0.836 r2: -7.574\n",
            "Epoch 50 loss: 0.834 r2: -7.241\n",
            "Epoch 60 loss: 0.832 r2: -7.037\n",
            "Epoch 70 loss: 0.831 r2: -6.894\n",
            "Epoch 80 loss: 0.830 r2: -6.783\n",
            "Epoch 90 loss: 0.829 r2: -6.692\n",
            "Epoch 100 loss: 0.828 r2: -6.614\n",
            "Epoch 110 loss: 0.828 r2: -6.548\n",
            "Epoch 120 loss: 0.827 r2: -6.489\n",
            "Epoch 130 loss: 0.826 r2: -6.439\n",
            "Epoch 140 loss: 0.826 r2: -6.394\n",
            "Epoch 150 loss: 0.826 r2: -6.353\n",
            "Epoch 160 loss: 0.825 r2: -6.313\n",
            "Epoch 170 loss: 0.825 r2: -6.270\n",
            "Epoch 180 loss: 0.824 r2: -6.224\n",
            "Epoch 190 loss: 0.823 r2: -6.175\n",
            "Epoch 200 loss: 0.823 r2: -6.125\n",
            "Epoch 210 loss: 0.822 r2: -6.073\n",
            "Epoch 220 loss: 0.821 r2: -6.022\n",
            "Epoch 230 loss: 0.820 r2: -5.970\n",
            "Epoch 240 loss: 0.820 r2: -5.920\n",
            "Epoch 250 loss: 0.819 r2: -5.878\n",
            "Epoch 260 loss: 0.818 r2: -5.841\n",
            "Epoch 270 loss: 0.816 r2: -5.798\n",
            "Epoch 280 loss: 0.812 r2: -5.668\n",
            "Epoch 290 loss: 0.807 r2: -5.405\n",
            "Epoch 300 loss: 0.805 r2: -5.222\n",
            "Epoch 310 loss: 0.803 r2: -5.096\n",
            "Epoch 320 loss: 0.801 r2: -4.999\n",
            "Epoch 330 loss: 0.799 r2: -4.917\n",
            "Epoch 340 loss: 0.798 r2: -4.846\n",
            "Epoch 350 loss: 0.797 r2: -4.784\n",
            "Epoch 360 loss: 0.796 r2: -4.729\n",
            "Epoch 370 loss: 0.795 r2: -4.681\n",
            "Epoch 380 loss: 0.795 r2: -4.639\n",
            "Epoch 390 loss: 0.794 r2: -4.602\n",
            "Epoch 400 loss: 0.793 r2: -4.569\n",
            "Epoch 410 loss: 0.793 r2: -4.541\n",
            "Epoch 420 loss: 0.793 r2: -4.516\n",
            "Epoch 430 loss: 0.792 r2: -4.495\n",
            "Epoch 440 loss: 0.792 r2: -4.476\n",
            "Epoch 450 loss: 0.792 r2: -4.459\n",
            "Epoch 460 loss: 0.792 r2: -4.444\n",
            "Epoch 470 loss: 0.791 r2: -4.431\n",
            "Epoch 480 loss: 0.791 r2: -4.419\n",
            "Epoch 490 loss: 0.791 r2: -4.409\n",
            "Epoch 500 loss: 0.791 r2: -4.399\n",
            "Epoch 510 loss: 0.791 r2: -4.391\n",
            "Epoch 520 loss: 0.790 r2: -4.383\n",
            "Epoch 530 loss: 0.790 r2: -4.376\n",
            "Epoch 540 loss: 0.790 r2: -4.369\n",
            "Epoch 550 loss: 0.790 r2: -4.364\n",
            "Epoch 560 loss: 0.790 r2: -4.358\n",
            "Epoch 570 loss: 0.790 r2: -4.353\n",
            "Epoch 580 loss: 0.790 r2: -4.348\n",
            "Epoch 590 loss: 0.789 r2: -4.344\n",
            "Epoch 600 loss: 0.789 r2: -4.340\n",
            "Epoch 610 loss: 0.789 r2: -4.336\n",
            "Epoch 620 loss: 0.789 r2: -4.332\n",
            "Epoch 630 loss: 0.789 r2: -4.329\n",
            "Epoch 640 loss: 0.789 r2: -4.325\n",
            "Epoch 650 loss: 0.789 r2: -4.322\n",
            "Epoch 660 loss: 0.789 r2: -4.319\n",
            "Epoch 670 loss: 0.789 r2: -4.316\n",
            "Epoch 680 loss: 0.789 r2: -4.313\n",
            "Epoch 690 loss: 0.789 r2: -4.310\n",
            "Epoch 700 loss: 0.788 r2: -4.308\n",
            "Epoch 710 loss: 0.788 r2: -4.305\n",
            "Epoch 720 loss: 0.788 r2: -4.302\n",
            "Epoch 730 loss: 0.788 r2: -4.300\n",
            "Epoch 740 loss: 0.788 r2: -4.298\n",
            "Epoch 750 loss: 0.788 r2: -4.295\n",
            "Epoch 760 loss: 0.788 r2: -4.293\n",
            "Epoch 770 loss: 0.788 r2: -4.291\n",
            "Epoch 780 loss: 0.788 r2: -4.288\n",
            "Epoch 790 loss: 0.788 r2: -4.286\n",
            "Epoch 800 loss: 0.788 r2: -4.284\n",
            "Epoch 810 loss: 0.788 r2: -4.282\n",
            "Epoch 820 loss: 0.788 r2: -4.280\n",
            "Epoch 830 loss: 0.788 r2: -4.277\n",
            "Epoch 840 loss: 0.787 r2: -4.275\n",
            "Epoch 850 loss: 0.787 r2: -4.273\n",
            "Epoch 860 loss: 0.787 r2: -4.271\n",
            "Epoch 870 loss: 0.787 r2: -4.269\n",
            "Epoch 880 loss: 0.787 r2: -4.267\n",
            "Epoch 890 loss: 0.787 r2: -4.265\n",
            "Epoch 900 loss: 0.787 r2: -4.263\n",
            "Epoch 910 loss: 0.787 r2: -4.261\n",
            "Epoch 920 loss: 0.787 r2: -4.259\n",
            "Epoch 930 loss: 0.787 r2: -4.257\n",
            "Epoch 940 loss: 0.787 r2: -4.255\n",
            "Epoch 950 loss: 0.787 r2: -4.253\n",
            "Epoch 960 loss: 0.787 r2: -4.251\n",
            "Epoch 970 loss: 0.787 r2: -4.249\n",
            "Epoch 980 loss: 0.787 r2: -4.247\n",
            "Epoch 990 loss: 0.787 r2: -4.245\n",
            "Weights: [1.1388008210527873, -6.460533653284556, 18.053280831558837, 3.3053222671129054, -1.87758791930933, 14.311623294632408, -13.718566491023708, 11.100006426048129]\n",
            "Bias: [-14.650030795069302, -6.983598255166105, -7.786270471307159]\n",
            "Learning rate: 0.1\n",
            "Performance for test dataset\n",
            "Loss: 0.604, r2: -2.618\n",
            "Performance for training dataset\n",
            "Loss: 0.787, r2: -4.244\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.792 r2: -3.842\n",
            "Epoch 10 loss: 0.792 r2: -3.942\n",
            "Epoch 20 loss: 0.791 r2: -3.950\n",
            "Epoch 30 loss: 0.791 r2: -3.948\n",
            "Epoch 40 loss: 0.791 r2: -3.945\n",
            "Epoch 50 loss: 0.791 r2: -3.943\n",
            "Epoch 60 loss: 0.791 r2: -3.940\n",
            "Epoch 70 loss: 0.791 r2: -3.938\n",
            "Epoch 80 loss: 0.791 r2: -3.936\n",
            "Epoch 90 loss: 0.790 r2: -3.934\n",
            "Epoch 100 loss: 0.790 r2: -3.932\n",
            "Epoch 110 loss: 0.790 r2: -3.930\n",
            "Epoch 120 loss: 0.790 r2: -3.929\n",
            "Epoch 130 loss: 0.790 r2: -3.927\n",
            "Epoch 140 loss: 0.790 r2: -3.926\n",
            "Epoch 150 loss: 0.790 r2: -3.924\n",
            "Epoch 160 loss: 0.790 r2: -3.923\n",
            "Epoch 170 loss: 0.790 r2: -3.922\n",
            "Epoch 180 loss: 0.790 r2: -3.921\n",
            "Epoch 190 loss: 0.790 r2: -3.920\n",
            "Epoch 200 loss: 0.789 r2: -3.919\n",
            "Epoch 210 loss: 0.789 r2: -3.918\n",
            "Epoch 220 loss: 0.789 r2: -3.918\n",
            "Epoch 230 loss: 0.789 r2: -3.917\n",
            "Epoch 240 loss: 0.789 r2: -3.916\n",
            "Epoch 250 loss: 0.789 r2: -3.916\n",
            "Epoch 260 loss: 0.789 r2: -3.915\n",
            "Epoch 270 loss: 0.789 r2: -3.915\n",
            "Epoch 280 loss: 0.789 r2: -3.914\n",
            "Epoch 290 loss: 0.789 r2: -3.914\n",
            "Epoch 300 loss: 0.789 r2: -3.914\n",
            "Epoch 310 loss: 0.789 r2: -3.913\n",
            "Epoch 320 loss: 0.788 r2: -3.913\n",
            "Epoch 330 loss: 0.788 r2: -3.913\n",
            "Epoch 340 loss: 0.788 r2: -3.912\n",
            "Epoch 350 loss: 0.788 r2: -3.912\n",
            "Epoch 360 loss: 0.788 r2: -3.912\n",
            "Epoch 370 loss: 0.788 r2: -3.912\n",
            "Epoch 380 loss: 0.788 r2: -3.912\n",
            "Epoch 390 loss: 0.788 r2: -3.912\n",
            "Epoch 400 loss: 0.788 r2: -3.912\n",
            "Epoch 410 loss: 0.788 r2: -3.912\n",
            "Epoch 420 loss: 0.788 r2: -3.912\n",
            "Epoch 430 loss: 0.788 r2: -3.912\n",
            "Epoch 440 loss: 0.788 r2: -3.912\n",
            "Epoch 450 loss: 0.787 r2: -3.912\n",
            "Epoch 460 loss: 0.787 r2: -3.912\n",
            "Epoch 470 loss: 0.787 r2: -3.912\n",
            "Epoch 480 loss: 0.787 r2: -3.912\n",
            "Epoch 490 loss: 0.787 r2: -3.913\n",
            "Epoch 500 loss: 0.787 r2: -3.913\n",
            "Epoch 510 loss: 0.787 r2: -3.913\n",
            "Epoch 520 loss: 0.787 r2: -3.913\n",
            "Epoch 530 loss: 0.787 r2: -3.913\n",
            "Epoch 540 loss: 0.787 r2: -3.913\n",
            "Epoch 550 loss: 0.787 r2: -3.913\n",
            "Epoch 560 loss: 0.787 r2: -3.913\n",
            "Epoch 570 loss: 0.787 r2: -3.913\n",
            "Epoch 580 loss: 0.787 r2: -3.913\n",
            "Epoch 590 loss: 0.787 r2: -3.913\n",
            "Epoch 600 loss: 0.787 r2: -3.913\n",
            "Epoch 610 loss: 0.787 r2: -3.912\n",
            "Epoch 620 loss: 0.786 r2: -3.912\n",
            "Epoch 630 loss: 0.786 r2: -3.911\n",
            "Epoch 640 loss: 0.786 r2: -3.911\n",
            "Epoch 650 loss: 0.786 r2: -3.910\n",
            "Epoch 660 loss: 0.786 r2: -3.909\n",
            "Epoch 670 loss: 0.786 r2: -3.909\n",
            "Epoch 680 loss: 0.786 r2: -3.908\n",
            "Epoch 690 loss: 0.786 r2: -3.907\n",
            "Epoch 700 loss: 0.786 r2: -3.905\n",
            "Epoch 710 loss: 0.786 r2: -3.904\n",
            "Epoch 720 loss: 0.786 r2: -3.903\n",
            "Epoch 730 loss: 0.786 r2: -3.901\n",
            "Epoch 740 loss: 0.786 r2: -3.900\n",
            "Epoch 750 loss: 0.786 r2: -3.898\n",
            "Epoch 760 loss: 0.786 r2: -3.896\n",
            "Epoch 770 loss: 0.786 r2: -3.894\n",
            "Epoch 780 loss: 0.786 r2: -3.892\n",
            "Epoch 790 loss: 0.786 r2: -3.890\n",
            "Epoch 800 loss: 0.786 r2: -3.888\n",
            "Epoch 810 loss: 0.786 r2: -3.886\n",
            "Epoch 820 loss: 0.786 r2: -3.884\n",
            "Epoch 830 loss: 0.786 r2: -3.881\n",
            "Epoch 840 loss: 0.786 r2: -3.879\n",
            "Epoch 850 loss: 0.786 r2: -3.876\n",
            "Epoch 860 loss: 0.786 r2: -3.874\n",
            "Epoch 870 loss: 0.786 r2: -3.871\n",
            "Epoch 880 loss: 0.786 r2: -3.868\n",
            "Epoch 890 loss: 0.786 r2: -3.866\n",
            "Epoch 900 loss: 0.786 r2: -3.863\n",
            "Epoch 910 loss: 0.786 r2: -3.860\n",
            "Epoch 920 loss: 0.786 r2: -3.857\n",
            "Epoch 930 loss: 0.786 r2: -3.854\n",
            "Epoch 940 loss: 0.786 r2: -3.852\n",
            "Epoch 950 loss: 0.786 r2: -3.849\n",
            "Epoch 960 loss: 0.786 r2: -3.846\n",
            "Epoch 970 loss: 0.786 r2: -3.843\n",
            "Epoch 980 loss: 0.786 r2: -3.840\n",
            "Epoch 990 loss: 0.786 r2: -3.836\n",
            "Weights: [1.906057925515384, -10.351113189598477, 27.717298968691793, 5.974342890073872, -3.2622822396143363, 25.131402321420275, -14.789792700301081, 11.061212596036057]\n",
            "Bias: [-22.218554631713456, -12.908839989089456, -8.128869476985066]\n",
            "Learning rate: 0.2\n",
            "Performance for test dataset\n",
            "Loss: 0.611, r2: -2.445\n",
            "Performance for training dataset\n",
            "Loss: 0.786, r2: -3.834\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.790 r2: -3.635\n",
            "Epoch 10 loss: 0.788 r2: -3.726\n",
            "Epoch 20 loss: 0.788 r2: -3.728\n",
            "Epoch 30 loss: 0.788 r2: -3.729\n",
            "Epoch 40 loss: 0.788 r2: -3.729\n",
            "Epoch 50 loss: 0.788 r2: -3.729\n",
            "Epoch 60 loss: 0.788 r2: -3.727\n",
            "Epoch 70 loss: 0.788 r2: -3.724\n",
            "Epoch 80 loss: 0.788 r2: -3.721\n",
            "Epoch 90 loss: 0.788 r2: -3.718\n",
            "Epoch 100 loss: 0.788 r2: -3.714\n",
            "Epoch 110 loss: 0.788 r2: -3.711\n",
            "Epoch 120 loss: 0.788 r2: -3.708\n",
            "Epoch 130 loss: 0.788 r2: -3.706\n",
            "Epoch 140 loss: 0.788 r2: -3.703\n",
            "Epoch 150 loss: 0.788 r2: -3.701\n",
            "Epoch 160 loss: 0.788 r2: -3.699\n",
            "Epoch 170 loss: 0.788 r2: -3.697\n",
            "Epoch 180 loss: 0.788 r2: -3.695\n",
            "Epoch 190 loss: 0.788 r2: -3.693\n",
            "Epoch 200 loss: 0.788 r2: -3.691\n",
            "Epoch 210 loss: 0.788 r2: -3.689\n",
            "Epoch 220 loss: 0.788 r2: -3.687\n",
            "Epoch 230 loss: 0.788 r2: -3.685\n",
            "Epoch 240 loss: 0.788 r2: -3.682\n",
            "Epoch 250 loss: 0.788 r2: -3.680\n",
            "Epoch 260 loss: 0.788 r2: -3.678\n",
            "Epoch 270 loss: 0.788 r2: -3.676\n",
            "Epoch 280 loss: 0.788 r2: -3.673\n",
            "Epoch 290 loss: 0.788 r2: -3.671\n",
            "Epoch 300 loss: 0.788 r2: -3.668\n",
            "Epoch 310 loss: 0.788 r2: -3.665\n",
            "Epoch 320 loss: 0.788 r2: -3.662\n",
            "Epoch 330 loss: 0.788 r2: -3.659\n",
            "Epoch 340 loss: 0.788 r2: -3.656\n",
            "Epoch 350 loss: 0.788 r2: -3.652\n",
            "Epoch 360 loss: 0.789 r2: -3.647\n",
            "Epoch 370 loss: 0.789 r2: -3.641\n",
            "Epoch 380 loss: 0.790 r2: -3.630\n",
            "Epoch 390 loss: 0.790 r2: -3.618\n",
            "Epoch 400 loss: 0.791 r2: -3.611\n",
            "Epoch 410 loss: 0.791 r2: -3.607\n",
            "Epoch 420 loss: 0.791 r2: -3.603\n",
            "Epoch 430 loss: 0.791 r2: -3.599\n",
            "Epoch 440 loss: 0.791 r2: -3.595\n",
            "Epoch 450 loss: 0.791 r2: -3.592\n",
            "Epoch 460 loss: 0.791 r2: -3.588\n",
            "Epoch 470 loss: 0.791 r2: -3.585\n",
            "Epoch 480 loss: 0.791 r2: -3.582\n",
            "Epoch 490 loss: 0.791 r2: -3.578\n",
            "Epoch 500 loss: 0.791 r2: -3.575\n",
            "Epoch 510 loss: 0.790 r2: -3.573\n",
            "Epoch 520 loss: 0.790 r2: -3.570\n",
            "Epoch 530 loss: 0.790 r2: -3.567\n",
            "Epoch 540 loss: 0.790 r2: -3.565\n",
            "Epoch 550 loss: 0.790 r2: -3.562\n",
            "Epoch 560 loss: 0.790 r2: -3.560\n",
            "Epoch 570 loss: 0.790 r2: -3.558\n",
            "Epoch 580 loss: 0.790 r2: -3.556\n",
            "Epoch 590 loss: 0.790 r2: -3.554\n",
            "Epoch 600 loss: 0.790 r2: -3.553\n",
            "Epoch 610 loss: 0.790 r2: -3.551\n",
            "Epoch 620 loss: 0.790 r2: -3.550\n",
            "Epoch 630 loss: 0.790 r2: -3.548\n",
            "Epoch 640 loss: 0.790 r2: -3.547\n",
            "Epoch 650 loss: 0.790 r2: -3.546\n",
            "Epoch 660 loss: 0.790 r2: -3.545\n",
            "Epoch 670 loss: 0.790 r2: -3.544\n",
            "Epoch 680 loss: 0.790 r2: -3.544\n",
            "Epoch 690 loss: 0.790 r2: -3.543\n",
            "Epoch 700 loss: 0.789 r2: -3.543\n",
            "Epoch 710 loss: 0.789 r2: -3.543\n",
            "Epoch 720 loss: 0.789 r2: -3.543\n",
            "Epoch 730 loss: 0.789 r2: -3.543\n",
            "Epoch 740 loss: 0.789 r2: -3.544\n",
            "Epoch 750 loss: 0.789 r2: -3.547\n",
            "Epoch 760 loss: 0.788 r2: -3.554\n",
            "Epoch 770 loss: 0.786 r2: -3.572\n",
            "Epoch 780 loss: 0.786 r2: -3.585\n",
            "Epoch 790 loss: 0.786 r2: -3.588\n",
            "Epoch 800 loss: 0.786 r2: -3.588\n",
            "Epoch 810 loss: 0.786 r2: -3.587\n",
            "Epoch 820 loss: 0.786 r2: -3.587\n",
            "Epoch 830 loss: 0.786 r2: -3.587\n",
            "Epoch 840 loss: 0.786 r2: -3.586\n",
            "Epoch 850 loss: 0.786 r2: -3.586\n",
            "Epoch 860 loss: 0.786 r2: -3.585\n",
            "Epoch 870 loss: 0.786 r2: -3.585\n",
            "Epoch 880 loss: 0.786 r2: -3.585\n",
            "Epoch 890 loss: 0.786 r2: -3.584\n",
            "Epoch 900 loss: 0.786 r2: -3.584\n",
            "Epoch 910 loss: 0.786 r2: -3.584\n",
            "Epoch 920 loss: 0.786 r2: -3.583\n",
            "Epoch 930 loss: 0.785 r2: -3.583\n",
            "Epoch 940 loss: 0.785 r2: -3.583\n",
            "Epoch 950 loss: 0.785 r2: -3.582\n",
            "Epoch 960 loss: 0.785 r2: -3.582\n",
            "Epoch 970 loss: 0.785 r2: -3.582\n",
            "Epoch 980 loss: 0.785 r2: -3.582\n",
            "Epoch 990 loss: 0.785 r2: -3.581\n",
            "Weights: [2.6774684875690618, -14.409118789534155, 36.99940874918747, 8.287589264692375, -4.668050682796565, 36.471134712479625, -15.578138228137304, 11.068832588970436]\n",
            "Bias: [-29.247136367775795, -18.979536697209003, -8.174698319217422]\n",
            "Learning rate: 0.3\n",
            "Performance for test dataset\n",
            "Loss: 0.615, r2: -2.319\n",
            "Performance for training dataset\n",
            "Loss: 0.785, r2: -3.581\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.788 r2: -3.475\n",
            "Epoch 10 loss: 0.788 r2: -3.499\n",
            "Epoch 20 loss: 0.788 r2: -3.498\n",
            "Epoch 30 loss: 0.788 r2: -3.497\n",
            "Epoch 40 loss: 0.788 r2: -3.496\n",
            "Epoch 50 loss: 0.788 r2: -3.495\n",
            "Epoch 60 loss: 0.788 r2: -3.494\n",
            "Epoch 70 loss: 0.788 r2: -3.493\n",
            "Epoch 80 loss: 0.788 r2: -3.492\n",
            "Epoch 90 loss: 0.788 r2: -3.491\n",
            "Epoch 100 loss: 0.788 r2: -3.490\n",
            "Epoch 110 loss: 0.787 r2: -3.489\n",
            "Epoch 120 loss: 0.787 r2: -3.488\n",
            "Epoch 130 loss: 0.787 r2: -3.488\n",
            "Epoch 140 loss: 0.787 r2: -3.487\n",
            "Epoch 150 loss: 0.787 r2: -3.486\n",
            "Epoch 160 loss: 0.787 r2: -3.486\n",
            "Epoch 170 loss: 0.787 r2: -3.485\n",
            "Epoch 180 loss: 0.787 r2: -3.485\n",
            "Epoch 190 loss: 0.787 r2: -3.484\n",
            "Epoch 200 loss: 0.787 r2: -3.484\n",
            "Epoch 210 loss: 0.787 r2: -3.483\n",
            "Epoch 220 loss: 0.787 r2: -3.483\n",
            "Epoch 230 loss: 0.787 r2: -3.483\n",
            "Epoch 240 loss: 0.787 r2: -3.482\n",
            "Epoch 250 loss: 0.787 r2: -3.482\n",
            "Epoch 260 loss: 0.787 r2: -3.482\n",
            "Epoch 270 loss: 0.787 r2: -3.482\n",
            "Epoch 280 loss: 0.787 r2: -3.481\n",
            "Epoch 290 loss: 0.787 r2: -3.481\n",
            "Epoch 300 loss: 0.787 r2: -3.481\n",
            "Epoch 310 loss: 0.787 r2: -3.481\n",
            "Epoch 320 loss: 0.787 r2: -3.481\n",
            "Epoch 330 loss: 0.787 r2: -3.481\n",
            "Epoch 340 loss: 0.787 r2: -3.481\n",
            "Epoch 350 loss: 0.787 r2: -3.481\n",
            "Epoch 360 loss: 0.787 r2: -3.481\n",
            "Epoch 370 loss: 0.786 r2: -3.481\n",
            "Epoch 380 loss: 0.786 r2: -3.481\n",
            "Epoch 390 loss: 0.786 r2: -3.481\n",
            "Epoch 400 loss: 0.786 r2: -3.481\n",
            "Epoch 410 loss: 0.786 r2: -3.480\n",
            "Epoch 420 loss: 0.786 r2: -3.480\n",
            "Epoch 430 loss: 0.786 r2: -3.480\n",
            "Epoch 440 loss: 0.786 r2: -3.480\n",
            "Epoch 450 loss: 0.786 r2: -3.480\n",
            "Epoch 460 loss: 0.786 r2: -3.480\n",
            "Epoch 470 loss: 0.786 r2: -3.480\n",
            "Epoch 480 loss: 0.786 r2: -3.480\n",
            "Epoch 490 loss: 0.786 r2: -3.480\n",
            "Epoch 500 loss: 0.786 r2: -3.481\n",
            "Epoch 510 loss: 0.786 r2: -3.481\n",
            "Epoch 520 loss: 0.786 r2: -3.481\n",
            "Epoch 530 loss: 0.786 r2: -3.481\n",
            "Epoch 540 loss: 0.786 r2: -3.481\n",
            "Epoch 550 loss: 0.786 r2: -3.481\n",
            "Epoch 560 loss: 0.786 r2: -3.481\n",
            "Epoch 570 loss: 0.786 r2: -3.481\n",
            "Epoch 580 loss: 0.786 r2: -3.482\n",
            "Epoch 590 loss: 0.786 r2: -3.482\n",
            "Epoch 600 loss: 0.786 r2: -3.482\n",
            "Epoch 610 loss: 0.786 r2: -3.482\n",
            "Epoch 620 loss: 0.786 r2: -3.483\n",
            "Epoch 630 loss: 0.786 r2: -3.483\n",
            "Epoch 640 loss: 0.786 r2: -3.484\n",
            "Epoch 650 loss: 0.786 r2: -3.484\n",
            "Epoch 660 loss: 0.786 r2: -3.485\n",
            "Epoch 670 loss: 0.786 r2: -3.485\n",
            "Epoch 680 loss: 0.786 r2: -3.486\n",
            "Epoch 690 loss: 0.786 r2: -3.487\n",
            "Epoch 700 loss: 0.786 r2: -3.488\n",
            "Epoch 710 loss: 0.786 r2: -3.489\n",
            "Epoch 720 loss: 0.786 r2: -3.490\n",
            "Epoch 730 loss: 0.786 r2: -3.491\n",
            "Epoch 740 loss: 0.786 r2: -3.493\n",
            "Epoch 750 loss: 0.786 r2: -3.494\n",
            "Epoch 760 loss: 0.786 r2: -3.496\n",
            "Epoch 770 loss: 0.786 r2: -3.498\n",
            "Epoch 780 loss: 0.786 r2: -3.501\n",
            "Epoch 790 loss: 0.785 r2: -3.504\n",
            "Epoch 800 loss: 0.785 r2: -3.509\n",
            "Epoch 810 loss: 0.785 r2: -3.515\n",
            "Epoch 820 loss: 0.785 r2: -3.524\n",
            "Epoch 830 loss: 0.785 r2: -3.534\n",
            "Epoch 840 loss: 0.785 r2: -3.529\n",
            "Epoch 850 loss: 0.785 r2: -3.521\n",
            "Epoch 860 loss: 0.785 r2: -3.516\n",
            "Epoch 870 loss: 0.785 r2: -3.513\n",
            "Epoch 880 loss: 0.785 r2: -3.510\n",
            "Epoch 890 loss: 0.785 r2: -3.507\n",
            "Epoch 900 loss: 0.785 r2: -3.505\n",
            "Epoch 910 loss: 0.785 r2: -3.503\n",
            "Epoch 920 loss: 0.785 r2: -3.500\n",
            "Epoch 930 loss: 0.785 r2: -3.498\n",
            "Epoch 940 loss: 0.785 r2: -3.497\n",
            "Epoch 950 loss: 0.785 r2: -3.495\n",
            "Epoch 960 loss: 0.785 r2: -3.493\n",
            "Epoch 970 loss: 0.785 r2: -3.491\n",
            "Epoch 980 loss: 0.785 r2: -3.490\n",
            "Epoch 990 loss: 0.785 r2: -3.488\n",
            "Weights: [3.449187375113401, -18.80477278214618, 47.29677034610534, 9.533367372642395, -5.409790975661462, 43.60397714280841, -18.750957086283083, 10.96626445186552]\n",
            "Bias: [-37.1596133654802, -22.62747011108908, -8.188060808401723]\n",
            "Learning rate: 0.4\n",
            "Performance for test dataset\n",
            "Loss: 0.618, r2: -2.307\n",
            "Performance for training dataset\n",
            "Loss: 0.785, r2: -3.487\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.786 r2: -3.488\n",
            "Epoch 10 loss: 0.787 r2: -3.470\n",
            "Epoch 20 loss: 0.787 r2: -3.446\n",
            "Epoch 30 loss: 0.787 r2: -3.440\n",
            "Epoch 40 loss: 0.787 r2: -3.434\n",
            "Epoch 50 loss: 0.787 r2: -3.427\n",
            "Epoch 60 loss: 0.787 r2: -3.420\n",
            "Epoch 70 loss: 0.787 r2: -3.410\n",
            "Epoch 80 loss: 0.788 r2: -3.397\n",
            "Epoch 90 loss: 0.788 r2: -3.367\n",
            "Epoch 100 loss: 0.790 r2: -3.307\n",
            "Epoch 110 loss: 0.790 r2: -3.302\n",
            "Epoch 120 loss: 0.790 r2: -3.298\n",
            "Epoch 130 loss: 0.790 r2: -3.295\n",
            "Epoch 140 loss: 0.790 r2: -3.292\n",
            "Epoch 150 loss: 0.790 r2: -3.290\n",
            "Epoch 160 loss: 0.790 r2: -3.288\n",
            "Epoch 170 loss: 0.790 r2: -3.286\n",
            "Epoch 180 loss: 0.790 r2: -3.285\n",
            "Epoch 190 loss: 0.790 r2: -3.284\n",
            "Epoch 200 loss: 0.790 r2: -3.283\n",
            "Epoch 210 loss: 0.790 r2: -3.282\n",
            "Epoch 220 loss: 0.790 r2: -3.282\n",
            "Epoch 230 loss: 0.789 r2: -3.281\n",
            "Epoch 240 loss: 0.789 r2: -3.281\n",
            "Epoch 250 loss: 0.789 r2: -3.280\n",
            "Epoch 260 loss: 0.789 r2: -3.280\n",
            "Epoch 270 loss: 0.789 r2: -3.280\n",
            "Epoch 280 loss: 0.789 r2: -3.280\n",
            "Epoch 290 loss: 0.789 r2: -3.280\n",
            "Epoch 300 loss: 0.789 r2: -3.280\n",
            "Epoch 310 loss: 0.789 r2: -3.280\n",
            "Epoch 320 loss: 0.789 r2: -3.281\n",
            "Epoch 330 loss: 0.789 r2: -3.281\n",
            "Epoch 340 loss: 0.789 r2: -3.282\n",
            "Epoch 350 loss: 0.789 r2: -3.283\n",
            "Epoch 360 loss: 0.789 r2: -3.283\n",
            "Epoch 370 loss: 0.789 r2: -3.284\n",
            "Epoch 380 loss: 0.788 r2: -3.286\n",
            "Epoch 390 loss: 0.788 r2: -3.287\n",
            "Epoch 400 loss: 0.788 r2: -3.289\n",
            "Epoch 410 loss: 0.788 r2: -3.292\n",
            "Epoch 420 loss: 0.788 r2: -3.295\n",
            "Epoch 430 loss: 0.788 r2: -3.301\n",
            "Epoch 440 loss: 0.788 r2: -3.313\n",
            "Epoch 450 loss: 0.786 r2: -3.362\n",
            "Epoch 460 loss: 0.785 r2: -3.420\n",
            "Epoch 470 loss: 0.785 r2: -3.422\n",
            "Epoch 480 loss: 0.785 r2: -3.425\n",
            "Epoch 490 loss: 0.785 r2: -3.427\n",
            "Epoch 500 loss: 0.785 r2: -3.429\n",
            "Epoch 510 loss: 0.785 r2: -3.431\n",
            "Epoch 520 loss: 0.785 r2: -3.433\n",
            "Epoch 530 loss: 0.785 r2: -3.435\n",
            "Epoch 540 loss: 0.785 r2: -3.437\n",
            "Epoch 550 loss: 0.785 r2: -3.439\n",
            "Epoch 560 loss: 0.785 r2: -3.441\n",
            "Epoch 570 loss: 0.785 r2: -3.443\n",
            "Epoch 580 loss: 0.785 r2: -3.445\n",
            "Epoch 590 loss: 0.785 r2: -3.447\n",
            "Epoch 600 loss: 0.785 r2: -3.449\n",
            "Epoch 610 loss: 0.785 r2: -3.450\n",
            "Epoch 620 loss: 0.785 r2: -3.452\n",
            "Epoch 630 loss: 0.785 r2: -3.453\n",
            "Epoch 640 loss: 0.785 r2: -3.455\n",
            "Epoch 650 loss: 0.785 r2: -3.456\n",
            "Epoch 660 loss: 0.785 r2: -3.457\n",
            "Epoch 670 loss: 0.785 r2: -3.458\n",
            "Epoch 680 loss: 0.785 r2: -3.459\n",
            "Epoch 690 loss: 0.785 r2: -3.460\n",
            "Epoch 700 loss: 0.785 r2: -3.460\n",
            "Epoch 710 loss: 0.785 r2: -3.461\n",
            "Epoch 720 loss: 0.785 r2: -3.461\n",
            "Epoch 730 loss: 0.785 r2: -3.462\n",
            "Epoch 740 loss: 0.785 r2: -3.462\n",
            "Epoch 750 loss: 0.785 r2: -3.462\n",
            "Epoch 760 loss: 0.785 r2: -3.462\n",
            "Epoch 770 loss: 0.785 r2: -3.462\n",
            "Epoch 780 loss: 0.785 r2: -3.462\n",
            "Epoch 790 loss: 0.785 r2: -3.462\n",
            "Epoch 800 loss: 0.785 r2: -3.461\n",
            "Epoch 810 loss: 0.785 r2: -3.461\n",
            "Epoch 820 loss: 0.785 r2: -3.461\n",
            "Epoch 830 loss: 0.785 r2: -3.460\n",
            "Epoch 840 loss: 0.785 r2: -3.460\n",
            "Epoch 850 loss: 0.785 r2: -3.459\n",
            "Epoch 860 loss: 0.785 r2: -3.459\n",
            "Epoch 870 loss: 0.785 r2: -3.458\n",
            "Epoch 880 loss: 0.785 r2: -3.458\n",
            "Epoch 890 loss: 0.785 r2: -3.457\n",
            "Epoch 900 loss: 0.785 r2: -3.456\n",
            "Epoch 910 loss: 0.785 r2: -3.455\n",
            "Epoch 920 loss: 0.784 r2: -3.455\n",
            "Epoch 930 loss: 0.784 r2: -3.454\n",
            "Epoch 940 loss: 0.784 r2: -3.453\n",
            "Epoch 950 loss: 0.784 r2: -3.452\n",
            "Epoch 960 loss: 0.784 r2: -3.451\n",
            "Epoch 970 loss: 0.784 r2: -3.450\n",
            "Epoch 980 loss: 0.784 r2: -3.450\n",
            "Epoch 990 loss: 0.784 r2: -3.449\n",
            "Weights: [4.102433497113799, -22.833557798447853, 56.62164329171829, 11.482459314153994, -6.454762332187041, 52.408534497621325, -21.672525315840062, 10.841361832835657]\n",
            "Bias: [-44.18424389793141, -27.54166446452638, -8.085803184596227]\n",
            "Learning rate: 0.5\n",
            "Performance for test dataset\n",
            "Loss: 0.619, r2: -2.296\n",
            "Performance for training dataset\n",
            "Loss: 0.784, r2: -3.448\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.786 r2: -3.461\n",
            "Epoch 10 loss: 0.784 r2: -3.441\n",
            "Epoch 20 loss: 0.784 r2: -3.444\n",
            "Epoch 30 loss: 0.784 r2: -3.444\n",
            "Epoch 40 loss: 0.784 r2: -3.446\n",
            "Epoch 50 loss: 0.784 r2: -3.446\n",
            "Epoch 60 loss: 0.784 r2: -3.447\n",
            "Epoch 70 loss: 0.784 r2: -3.447\n",
            "Epoch 80 loss: 0.784 r2: -3.448\n",
            "Epoch 90 loss: 0.784 r2: -3.448\n",
            "Epoch 100 loss: 0.784 r2: -3.449\n",
            "Epoch 110 loss: 0.784 r2: -3.448\n",
            "Epoch 120 loss: 0.784 r2: -3.449\n",
            "Epoch 130 loss: 0.784 r2: -3.448\n",
            "Epoch 140 loss: 0.784 r2: -3.451\n",
            "Epoch 150 loss: 0.784 r2: -3.443\n",
            "Epoch 160 loss: 0.784 r2: -3.463\n",
            "Epoch 170 loss: 0.784 r2: -3.420\n",
            "Epoch 180 loss: 0.785 r2: -3.480\n",
            "Epoch 190 loss: 0.784 r2: -3.422\n",
            "Epoch 200 loss: 0.785 r2: -3.478\n",
            "Epoch 210 loss: 0.784 r2: -3.422\n",
            "Epoch 220 loss: 0.785 r2: -3.473\n",
            "Epoch 230 loss: 0.784 r2: -3.425\n",
            "Epoch 240 loss: 0.785 r2: -3.453\n",
            "Epoch 250 loss: 0.784 r2: -3.422\n",
            "Epoch 260 loss: 0.785 r2: -3.448\n",
            "Epoch 270 loss: 0.784 r2: -3.416\n",
            "Epoch 280 loss: 0.785 r2: -3.439\n",
            "Epoch 290 loss: 0.784 r2: -3.452\n",
            "Epoch 300 loss: 0.784 r2: -3.404\n",
            "Epoch 310 loss: 0.785 r2: -3.466\n",
            "Epoch 320 loss: 0.784 r2: -3.386\n",
            "Epoch 330 loss: 0.785 r2: -3.449\n",
            "Epoch 340 loss: 0.784 r2: -3.410\n",
            "Epoch 350 loss: 0.784 r2: -3.416\n",
            "Epoch 360 loss: 0.784 r2: -3.415\n",
            "Epoch 370 loss: 0.784 r2: -3.386\n",
            "Epoch 380 loss: 0.784 r2: -3.400\n",
            "Epoch 390 loss: 0.784 r2: -3.378\n",
            "Epoch 400 loss: 0.784 r2: -3.375\n",
            "Epoch 410 loss: 0.784 r2: -3.401\n",
            "Epoch 420 loss: 0.785 r2: -3.447\n",
            "Epoch 430 loss: 0.784 r2: -3.380\n",
            "Epoch 440 loss: 0.785 r2: -3.388\n",
            "Epoch 450 loss: 0.785 r2: -3.450\n",
            "Epoch 460 loss: 0.785 r2: -3.433\n",
            "Epoch 470 loss: 0.785 r2: -3.436\n",
            "Epoch 480 loss: 0.786 r2: -3.435\n",
            "Epoch 490 loss: 0.785 r2: -3.424\n",
            "Epoch 500 loss: 0.785 r2: -3.438\n",
            "Epoch 510 loss: 0.785 r2: -3.424\n",
            "Epoch 520 loss: 0.785 r2: -3.432\n",
            "Epoch 530 loss: 0.785 r2: -3.429\n",
            "Epoch 540 loss: 0.785 r2: -3.443\n",
            "Epoch 550 loss: 0.784 r2: -3.404\n",
            "Epoch 560 loss: 0.785 r2: -3.422\n",
            "Epoch 570 loss: 0.784 r2: -3.353\n",
            "Epoch 580 loss: 0.785 r2: -3.413\n",
            "Epoch 590 loss: 0.786 r2: -3.393\n",
            "Epoch 600 loss: 0.785 r2: -3.399\n",
            "Epoch 610 loss: 0.785 r2: -3.425\n",
            "Epoch 620 loss: 0.785 r2: -3.368\n",
            "Epoch 630 loss: 0.784 r2: -3.357\n",
            "Epoch 640 loss: 0.784 r2: -3.400\n",
            "Epoch 650 loss: 0.786 r2: -3.392\n",
            "Epoch 660 loss: 0.784 r2: -3.355\n",
            "Epoch 670 loss: 0.784 r2: -3.390\n",
            "Epoch 680 loss: 0.785 r2: -3.392\n",
            "Epoch 690 loss: 0.784 r2: -3.356\n",
            "Epoch 700 loss: 0.784 r2: -3.376\n",
            "Epoch 710 loss: 0.785 r2: -3.390\n",
            "Epoch 720 loss: 0.784 r2: -3.356\n",
            "Epoch 730 loss: 0.785 r2: -3.356\n",
            "Epoch 740 loss: 0.784 r2: -3.354\n",
            "Epoch 750 loss: 0.784 r2: -3.377\n",
            "Epoch 760 loss: 0.784 r2: -3.351\n",
            "Epoch 770 loss: 0.784 r2: -3.347\n",
            "Epoch 780 loss: 0.785 r2: -3.363\n",
            "Epoch 790 loss: 0.784 r2: -3.359\n",
            "Epoch 800 loss: 0.784 r2: -3.346\n",
            "Epoch 810 loss: 0.784 r2: -3.362\n",
            "Epoch 820 loss: 0.784 r2: -3.353\n",
            "Epoch 830 loss: 0.784 r2: -3.346\n",
            "Epoch 840 loss: 0.784 r2: -3.323\n",
            "Epoch 850 loss: 0.784 r2: -3.322\n",
            "Epoch 860 loss: 0.784 r2: -3.344\n",
            "Epoch 870 loss: 0.784 r2: -3.322\n",
            "Epoch 880 loss: 0.784 r2: -3.339\n",
            "Epoch 890 loss: 0.784 r2: -3.329\n",
            "Epoch 900 loss: 0.784 r2: -3.336\n",
            "Epoch 910 loss: 0.784 r2: -3.334\n",
            "Epoch 920 loss: 0.784 r2: -3.333\n",
            "Epoch 930 loss: 0.783 r2: -3.312\n",
            "Epoch 940 loss: 0.784 r2: -3.337\n",
            "Epoch 950 loss: 0.784 r2: -3.337\n",
            "Epoch 960 loss: 0.783 r2: -3.313\n",
            "Epoch 970 loss: 0.784 r2: -3.329\n",
            "Epoch 980 loss: 0.784 r2: -3.383\n",
            "Epoch 990 loss: 0.784 r2: -3.310\n",
            "Weights: [4.564381629712528, -26.38195780105419, 64.72078117399072, 13.917082496683387, -7.776468314835149, 63.86833600420308, -23.999964732054988, 11.084221165912494]\n",
            "Bias: [-50.45493489333638, -33.86424882961913, -8.252293369410094]\n",
            "Learning rate: 0.6\n",
            "Performance for test dataset\n",
            "Loss: 0.618, r2: -2.222\n",
            "Performance for training dataset\n",
            "Loss: 0.783, r2: -3.313\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.785 r2: -3.490\n",
            "Epoch 10 loss: 0.785 r2: -3.275\n",
            "Epoch 20 loss: 0.784 r2: -3.496\n",
            "Epoch 30 loss: 0.784 r2: -3.339\n",
            "Epoch 40 loss: 0.785 r2: -3.467\n",
            "Epoch 50 loss: 0.785 r2: -3.418\n",
            "Epoch 60 loss: 0.784 r2: -3.285\n",
            "Epoch 70 loss: 0.784 r2: -3.483\n",
            "Epoch 80 loss: 0.786 r2: -3.307\n",
            "Epoch 90 loss: 0.785 r2: -3.497\n",
            "Epoch 100 loss: 0.785 r2: -3.491\n",
            "Epoch 110 loss: 0.785 r2: -3.466\n",
            "Epoch 120 loss: 0.785 r2: -3.187\n",
            "Epoch 130 loss: 0.786 r2: -3.146\n",
            "Epoch 140 loss: 0.785 r2: -3.459\n",
            "Epoch 150 loss: 0.785 r2: -3.187\n",
            "Epoch 160 loss: 0.787 r2: -3.332\n",
            "Epoch 170 loss: 0.786 r2: -3.218\n",
            "Epoch 180 loss: 0.787 r2: -3.296\n",
            "Epoch 190 loss: 0.785 r2: -3.461\n",
            "Epoch 200 loss: 0.785 r2: -3.462\n",
            "Epoch 210 loss: 0.785 r2: -3.452\n",
            "Epoch 220 loss: 0.785 r2: -3.146\n",
            "Epoch 230 loss: 0.785 r2: -3.451\n",
            "Epoch 240 loss: 0.786 r2: -3.371\n",
            "Epoch 250 loss: 0.786 r2: -3.141\n",
            "Epoch 260 loss: 0.785 r2: -3.322\n",
            "Epoch 270 loss: 0.785 r2: -3.428\n",
            "Epoch 280 loss: 0.785 r2: -3.364\n",
            "Epoch 290 loss: 0.786 r2: -3.400\n",
            "Epoch 300 loss: 0.785 r2: -3.361\n",
            "Epoch 310 loss: 0.785 r2: -3.382\n",
            "Epoch 320 loss: 0.785 r2: -3.189\n",
            "Epoch 330 loss: 0.785 r2: -3.304\n",
            "Epoch 340 loss: 0.786 r2: -3.310\n",
            "Epoch 350 loss: 0.785 r2: -3.314\n",
            "Epoch 360 loss: 0.784 r2: -3.336\n",
            "Epoch 370 loss: 0.783 r2: -3.305\n",
            "Epoch 380 loss: 0.785 r2: -3.366\n",
            "Epoch 390 loss: 0.785 r2: -3.322\n",
            "Epoch 400 loss: 0.784 r2: -3.382\n",
            "Epoch 410 loss: 0.785 r2: -3.271\n",
            "Epoch 420 loss: 0.784 r2: -3.210\n",
            "Epoch 430 loss: 0.784 r2: -3.430\n",
            "Epoch 440 loss: 0.784 r2: -3.139\n",
            "Epoch 450 loss: 0.785 r2: -3.340\n",
            "Epoch 460 loss: 0.785 r2: -3.308\n",
            "Epoch 470 loss: 0.785 r2: -3.363\n",
            "Epoch 480 loss: 0.785 r2: -3.315\n",
            "Epoch 490 loss: 0.784 r2: -3.351\n",
            "Epoch 500 loss: 0.785 r2: -3.483\n",
            "Epoch 510 loss: 0.785 r2: -3.368\n",
            "Epoch 520 loss: 0.784 r2: -3.279\n",
            "Epoch 530 loss: 0.785 r2: -3.146\n",
            "Epoch 540 loss: 0.785 r2: -3.455\n",
            "Epoch 550 loss: 0.785 r2: -3.367\n",
            "Epoch 560 loss: 0.785 r2: -3.273\n",
            "Epoch 570 loss: 0.784 r2: -3.509\n",
            "Epoch 580 loss: 0.783 r2: -3.562\n",
            "Epoch 590 loss: 0.783 r2: -3.645\n",
            "Epoch 600 loss: 0.785 r2: -3.185\n",
            "Epoch 610 loss: 0.784 r2: -3.437\n",
            "Epoch 620 loss: 0.783 r2: -3.565\n",
            "Epoch 630 loss: 0.784 r2: -3.165\n",
            "Epoch 640 loss: 0.784 r2: -3.502\n",
            "Epoch 650 loss: 0.784 r2: -3.569\n",
            "Epoch 660 loss: 0.784 r2: -3.242\n",
            "Epoch 670 loss: 0.784 r2: -3.425\n",
            "Epoch 680 loss: 0.784 r2: -3.497\n",
            "Epoch 690 loss: 0.785 r2: -3.146\n",
            "Epoch 700 loss: 0.783 r2: -3.403\n",
            "Epoch 710 loss: 0.783 r2: -3.484\n",
            "Epoch 720 loss: 0.785 r2: -3.218\n",
            "Epoch 730 loss: 0.783 r2: -3.523\n",
            "Epoch 740 loss: 0.784 r2: -3.466\n",
            "Epoch 750 loss: 0.784 r2: -3.354\n",
            "Epoch 760 loss: 0.783 r2: -3.416\n",
            "Epoch 770 loss: 0.783 r2: -3.370\n",
            "Epoch 780 loss: 0.784 r2: -3.249\n",
            "Epoch 790 loss: 0.783 r2: -3.481\n",
            "Epoch 800 loss: 0.783 r2: -3.528\n",
            "Epoch 810 loss: 0.786 r2: -3.106\n",
            "Epoch 820 loss: 0.783 r2: -3.445\n",
            "Epoch 830 loss: 0.783 r2: -3.360\n",
            "Epoch 840 loss: 0.786 r2: -3.113\n",
            "Epoch 850 loss: 0.783 r2: -3.420\n",
            "Epoch 860 loss: 0.783 r2: -3.355\n",
            "Epoch 870 loss: 0.785 r2: -3.150\n",
            "Epoch 880 loss: 0.783 r2: -3.409\n",
            "Epoch 890 loss: 0.784 r2: -3.429\n",
            "Epoch 900 loss: 0.786 r2: -3.103\n",
            "Epoch 910 loss: 0.783 r2: -3.455\n",
            "Epoch 920 loss: 0.783 r2: -3.379\n",
            "Epoch 930 loss: 0.785 r2: -3.154\n",
            "Epoch 940 loss: 0.783 r2: -3.361\n",
            "Epoch 950 loss: 0.783 r2: -3.500\n",
            "Epoch 960 loss: 0.786 r2: -3.123\n",
            "Epoch 970 loss: 0.783 r2: -3.439\n",
            "Epoch 980 loss: 0.783 r2: -3.452\n",
            "Epoch 990 loss: 0.784 r2: -3.142\n",
            "Weights: [5.0113577782597565, -29.34169761489578, 71.75591378252321, 16.298682900436393, -9.235867241554361, 76.96863569484472, -25.721880778504215, 12.258074118042604]\n",
            "Bias: [-55.94122638063245, -40.66899457372479, -9.299647934327052]\n",
            "Learning rate: 0.7\n",
            "Performance for test dataset\n",
            "Loss: 0.618, r2: -2.136\n",
            "Performance for training dataset\n",
            "Loss: 0.784, r2: -3.141\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.785 r2: -3.304\n",
            "Epoch 10 loss: 0.785 r2: -3.166\n",
            "Epoch 20 loss: 0.784 r2: -3.306\n",
            "Epoch 30 loss: 0.787 r2: -3.076\n",
            "Epoch 40 loss: 0.789 r2: -4.556\n",
            "Epoch 50 loss: 0.797 r2: -3.837\n",
            "Epoch 60 loss: 0.796 r2: -3.716\n",
            "Epoch 70 loss: 0.796 r2: -3.789\n",
            "Epoch 80 loss: 0.798 r2: -4.003\n",
            "Epoch 90 loss: 0.798 r2: -4.587\n",
            "Epoch 100 loss: 0.797 r2: -3.866\n",
            "Epoch 110 loss: 0.798 r2: -4.603\n",
            "Epoch 120 loss: 0.797 r2: -3.884\n",
            "Epoch 130 loss: 0.797 r2: -3.782\n",
            "Epoch 140 loss: 0.797 r2: -3.830\n",
            "Epoch 150 loss: 0.796 r2: -3.756\n",
            "Epoch 160 loss: 0.797 r2: -3.806\n",
            "Epoch 170 loss: 0.797 r2: -3.793\n",
            "Epoch 180 loss: 0.798 r2: -4.631\n",
            "Epoch 190 loss: 0.797 r2: -3.706\n",
            "Epoch 200 loss: 0.798 r2: -4.620\n",
            "Epoch 210 loss: 0.797 r2: -3.714\n",
            "Epoch 220 loss: 0.797 r2: -4.601\n",
            "Epoch 230 loss: 0.797 r2: -3.717\n",
            "Epoch 240 loss: 0.797 r2: -4.594\n",
            "Epoch 250 loss: 0.797 r2: -3.714\n",
            "Epoch 260 loss: 0.797 r2: -4.581\n",
            "Epoch 270 loss: 0.797 r2: -3.706\n",
            "Epoch 280 loss: 0.797 r2: -4.560\n",
            "Epoch 290 loss: 0.797 r2: -3.706\n",
            "Epoch 300 loss: 0.797 r2: -4.483\n",
            "Epoch 310 loss: 0.797 r2: -3.731\n",
            "Epoch 320 loss: 0.797 r2: -4.520\n",
            "Epoch 330 loss: 0.797 r2: -3.904\n",
            "Epoch 340 loss: 0.796 r2: -3.880\n",
            "Epoch 350 loss: 0.795 r2: -4.003\n",
            "Epoch 360 loss: 0.795 r2: -3.991\n",
            "Epoch 370 loss: 0.795 r2: -3.985\n",
            "Epoch 380 loss: 0.795 r2: -3.982\n",
            "Epoch 390 loss: 0.795 r2: -3.980\n",
            "Epoch 400 loss: 0.795 r2: -3.978\n",
            "Epoch 410 loss: 0.795 r2: -3.976\n",
            "Epoch 420 loss: 0.795 r2: -3.974\n",
            "Epoch 430 loss: 0.795 r2: -3.971\n",
            "Epoch 440 loss: 0.795 r2: -3.969\n",
            "Epoch 450 loss: 0.795 r2: -3.967\n",
            "Epoch 460 loss: 0.795 r2: -3.965\n",
            "Epoch 470 loss: 0.795 r2: -3.962\n",
            "Epoch 480 loss: 0.795 r2: -3.960\n",
            "Epoch 490 loss: 0.795 r2: -3.957\n",
            "Epoch 500 loss: 0.795 r2: -3.955\n",
            "Epoch 510 loss: 0.795 r2: -3.953\n",
            "Epoch 520 loss: 0.795 r2: -3.950\n",
            "Epoch 530 loss: 0.795 r2: -3.947\n",
            "Epoch 540 loss: 0.795 r2: -3.945\n",
            "Epoch 550 loss: 0.795 r2: -3.942\n",
            "Epoch 560 loss: 0.795 r2: -3.939\n",
            "Epoch 570 loss: 0.795 r2: -3.937\n",
            "Epoch 580 loss: 0.795 r2: -3.934\n",
            "Epoch 590 loss: 0.795 r2: -3.931\n",
            "Epoch 600 loss: 0.795 r2: -3.928\n",
            "Epoch 610 loss: 0.795 r2: -3.925\n",
            "Epoch 620 loss: 0.795 r2: -3.922\n",
            "Epoch 630 loss: 0.795 r2: -3.919\n",
            "Epoch 640 loss: 0.795 r2: -3.915\n",
            "Epoch 650 loss: 0.795 r2: -3.912\n",
            "Epoch 660 loss: 0.795 r2: -3.909\n",
            "Epoch 670 loss: 0.795 r2: -3.905\n",
            "Epoch 680 loss: 0.795 r2: -3.901\n",
            "Epoch 690 loss: 0.795 r2: -3.898\n",
            "Epoch 700 loss: 0.795 r2: -3.894\n",
            "Epoch 710 loss: 0.795 r2: -3.890\n",
            "Epoch 720 loss: 0.795 r2: -3.887\n",
            "Epoch 730 loss: 0.795 r2: -3.883\n",
            "Epoch 740 loss: 0.795 r2: -3.880\n",
            "Epoch 750 loss: 0.795 r2: -3.877\n",
            "Epoch 760 loss: 0.795 r2: -3.875\n",
            "Epoch 770 loss: 0.795 r2: -3.872\n",
            "Epoch 780 loss: 0.795 r2: -3.870\n",
            "Epoch 790 loss: 0.795 r2: -3.868\n",
            "Epoch 800 loss: 0.795 r2: -3.866\n",
            "Epoch 810 loss: 0.795 r2: -3.864\n",
            "Epoch 820 loss: 0.795 r2: -3.863\n",
            "Epoch 830 loss: 0.795 r2: -3.861\n",
            "Epoch 840 loss: 0.795 r2: -3.860\n",
            "Epoch 850 loss: 0.795 r2: -3.859\n",
            "Epoch 860 loss: 0.795 r2: -3.857\n",
            "Epoch 870 loss: 0.795 r2: -3.856\n",
            "Epoch 880 loss: 0.795 r2: -3.855\n",
            "Epoch 890 loss: 0.795 r2: -3.854\n",
            "Epoch 900 loss: 0.795 r2: -3.853\n",
            "Epoch 910 loss: 0.795 r2: -3.852\n",
            "Epoch 920 loss: 0.795 r2: -3.851\n",
            "Epoch 930 loss: 0.795 r2: -3.850\n",
            "Epoch 940 loss: 0.795 r2: -3.849\n",
            "Epoch 950 loss: 0.795 r2: -3.848\n",
            "Epoch 960 loss: 0.795 r2: -3.847\n",
            "Epoch 970 loss: 0.795 r2: -3.846\n",
            "Epoch 980 loss: 0.795 r2: -3.845\n",
            "Epoch 990 loss: 0.795 r2: -3.844\n",
            "Weights: [5.361301035957451, -31.652855191021832, 77.62823463196732, 10.346222324206853, -11.925167358917546, 89.04025562877504, -26.827901702041423, 11.200261436212726]\n",
            "Bias: [-60.5147815496283, -45.13032603385748, -8.18737506403676]\n",
            "Learning rate: 0.8\n",
            "Performance for test dataset\n",
            "Loss: 0.599, r2: -2.326\n",
            "Performance for training dataset\n",
            "Loss: 0.795, r2: -3.844\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.796 r2: -3.742\n",
            "Epoch 10 loss: 0.799 r2: -4.428\n",
            "Epoch 20 loss: 0.798 r2: -4.296\n",
            "Epoch 30 loss: 0.796 r2: -4.047\n",
            "Epoch 40 loss: 0.798 r2: -4.248\n",
            "Epoch 50 loss: 0.797 r2: -3.852\n",
            "Epoch 60 loss: 0.797 r2: -3.592\n",
            "Epoch 70 loss: 0.797 r2: -3.708\n",
            "Epoch 80 loss: 0.799 r2: -3.632\n",
            "Epoch 90 loss: 0.798 r2: -4.541\n",
            "Epoch 100 loss: 0.797 r2: -3.901\n",
            "Epoch 110 loss: 0.799 r2: -4.639\n",
            "Epoch 120 loss: 0.798 r2: -3.943\n",
            "Epoch 130 loss: 0.798 r2: -4.584\n",
            "Epoch 140 loss: 0.798 r2: -3.863\n",
            "Epoch 150 loss: 0.797 r2: -4.597\n",
            "Epoch 160 loss: 0.797 r2: -3.788\n",
            "Epoch 170 loss: 0.796 r2: -4.263\n",
            "Epoch 180 loss: 0.796 r2: -3.900\n",
            "Epoch 190 loss: 0.796 r2: -3.964\n",
            "Epoch 200 loss: 0.795 r2: -3.977\n",
            "Epoch 210 loss: 0.795 r2: -3.979\n",
            "Epoch 220 loss: 0.795 r2: -3.978\n",
            "Epoch 230 loss: 0.795 r2: -3.978\n",
            "Epoch 240 loss: 0.795 r2: -3.977\n",
            "Epoch 250 loss: 0.795 r2: -3.976\n",
            "Epoch 260 loss: 0.795 r2: -3.976\n",
            "Epoch 270 loss: 0.795 r2: -3.975\n",
            "Epoch 280 loss: 0.795 r2: -3.974\n",
            "Epoch 290 loss: 0.795 r2: -3.973\n",
            "Epoch 300 loss: 0.795 r2: -3.973\n",
            "Epoch 310 loss: 0.795 r2: -3.972\n",
            "Epoch 320 loss: 0.795 r2: -3.971\n",
            "Epoch 330 loss: 0.795 r2: -3.970\n",
            "Epoch 340 loss: 0.795 r2: -3.969\n",
            "Epoch 350 loss: 0.795 r2: -3.968\n",
            "Epoch 360 loss: 0.795 r2: -3.967\n",
            "Epoch 370 loss: 0.795 r2: -3.966\n",
            "Epoch 380 loss: 0.795 r2: -3.965\n",
            "Epoch 390 loss: 0.795 r2: -3.964\n",
            "Epoch 400 loss: 0.795 r2: -3.963\n",
            "Epoch 410 loss: 0.795 r2: -3.962\n",
            "Epoch 420 loss: 0.795 r2: -3.960\n",
            "Epoch 430 loss: 0.795 r2: -3.959\n",
            "Epoch 440 loss: 0.795 r2: -3.958\n",
            "Epoch 450 loss: 0.795 r2: -3.956\n",
            "Epoch 460 loss: 0.795 r2: -3.955\n",
            "Epoch 470 loss: 0.795 r2: -3.953\n",
            "Epoch 480 loss: 0.795 r2: -3.952\n",
            "Epoch 490 loss: 0.795 r2: -3.950\n",
            "Epoch 500 loss: 0.795 r2: -3.948\n",
            "Epoch 510 loss: 0.795 r2: -3.947\n",
            "Epoch 520 loss: 0.795 r2: -3.945\n",
            "Epoch 530 loss: 0.795 r2: -3.943\n",
            "Epoch 540 loss: 0.795 r2: -3.941\n",
            "Epoch 550 loss: 0.795 r2: -3.940\n",
            "Epoch 560 loss: 0.795 r2: -3.938\n",
            "Epoch 570 loss: 0.795 r2: -3.936\n",
            "Epoch 580 loss: 0.795 r2: -3.934\n",
            "Epoch 590 loss: 0.795 r2: -3.932\n",
            "Epoch 600 loss: 0.795 r2: -3.930\n",
            "Epoch 610 loss: 0.795 r2: -3.928\n",
            "Epoch 620 loss: 0.795 r2: -3.926\n",
            "Epoch 630 loss: 0.795 r2: -3.924\n",
            "Epoch 640 loss: 0.795 r2: -3.922\n",
            "Epoch 650 loss: 0.795 r2: -3.919\n",
            "Epoch 660 loss: 0.795 r2: -3.917\n",
            "Epoch 670 loss: 0.795 r2: -3.915\n",
            "Epoch 680 loss: 0.795 r2: -3.913\n",
            "Epoch 690 loss: 0.795 r2: -3.911\n",
            "Epoch 700 loss: 0.795 r2: -3.909\n",
            "Epoch 710 loss: 0.795 r2: -3.906\n",
            "Epoch 720 loss: 0.795 r2: -3.904\n",
            "Epoch 730 loss: 0.795 r2: -3.902\n",
            "Epoch 740 loss: 0.795 r2: -3.900\n",
            "Epoch 750 loss: 0.795 r2: -3.898\n",
            "Epoch 760 loss: 0.795 r2: -3.895\n",
            "Epoch 770 loss: 0.795 r2: -3.893\n",
            "Epoch 780 loss: 0.795 r2: -3.891\n",
            "Epoch 790 loss: 0.795 r2: -3.888\n",
            "Epoch 800 loss: 0.795 r2: -3.886\n",
            "Epoch 810 loss: 0.796 r2: -3.884\n",
            "Epoch 820 loss: 0.796 r2: -3.881\n",
            "Epoch 830 loss: 0.796 r2: -3.878\n",
            "Epoch 840 loss: 0.796 r2: -3.875\n",
            "Epoch 850 loss: 0.796 r2: -3.872\n",
            "Epoch 860 loss: 0.796 r2: -3.869\n",
            "Epoch 870 loss: 0.796 r2: -3.865\n",
            "Epoch 880 loss: 0.796 r2: -3.860\n",
            "Epoch 890 loss: 0.796 r2: -3.855\n",
            "Epoch 900 loss: 0.796 r2: -3.853\n",
            "Epoch 910 loss: 0.795 r2: -3.852\n",
            "Epoch 920 loss: 0.795 r2: -3.851\n",
            "Epoch 930 loss: 0.795 r2: -3.850\n",
            "Epoch 940 loss: 0.795 r2: -3.850\n",
            "Epoch 950 loss: 0.795 r2: -3.849\n",
            "Epoch 960 loss: 0.795 r2: -3.849\n",
            "Epoch 970 loss: 0.795 r2: -3.848\n",
            "Epoch 980 loss: 0.795 r2: -3.848\n",
            "Epoch 990 loss: 0.795 r2: -3.847\n",
            "Weights: [5.633676052213697, -33.26193907058663, 81.60587686274049, 11.47188651207967, -13.109785897485846, 98.08619396085574, -27.654750881592612, 11.15895353034133]\n",
            "Bias: [-63.562999607599714, -49.94514355515076, -8.16022075633216]\n",
            "Learning rate: 0.9\n",
            "Performance for test dataset\n",
            "Loss: 0.599, r2: -2.317\n",
            "Performance for training dataset\n",
            "Loss: 0.795, r2: -3.847\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network = OurNeuralNetwork('tanh')\n",
        "for i in range(10,100,10):\n",
        "  weights = network.train(np.array(X_train), np.array(Y_train),i/100,1000)\n",
        "  print(\"Weights:\", weights[:8])\n",
        "  print(\"Bias:\",weights[8:])\n",
        "  print(\"Learning rate:\",i/100)\n",
        "  print(\"Loss for test dataset\")\n",
        "  network.test(np.array(X_test), np.array(Y_test), weights)\n",
        "  print(\"Loss for training dataset\")\n",
        "  network.test(np.array(X_train), np.array(Y_train), weights)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txRPkBxU7PUg",
        "outputId": "75ed706b-486b-466a-c01d-0e70a0444aef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: -0.6235693898118856 0.24242715076024066 -2.0535055744618833 -2.3136622898362154 0.4940283071658281 -0.7457430450670418 -0.05226760187237972 -0.28702845728856946\n",
            "Bias: -0.79247817048909 0.4644123068576251 1.2729274788883018\n",
            "Epoch 0 loss: 0.611 r2: -0.147\n",
            "Epoch 10 loss: 0.618 r2: -0.181\n",
            "Epoch 20 loss: 0.572 r2: -0.101\n",
            "Epoch 30 loss: 0.564 r2: 0.033\n",
            "Epoch 40 loss: 0.576 r2: 0.003\n",
            "Epoch 50 loss: 0.590 r2: 0.033\n",
            "Epoch 60 loss: 0.579 r2: 0.003\n",
            "Epoch 70 loss: 0.580 r2: 0.003\n",
            "Epoch 80 loss: 0.629 r2: 0.026\n",
            "Epoch 90 loss: 0.583 r2: 0.013\n",
            "Epoch 100 loss: 0.554 r2: -0.008\n",
            "Epoch 110 loss: 0.569 r2: 0.083\n",
            "Epoch 120 loss: 0.574 r2: 0.080\n",
            "Epoch 130 loss: 0.556 r2: 0.023\n",
            "Epoch 140 loss: 0.556 r2: 0.056\n",
            "Epoch 150 loss: 0.566 r2: 0.087\n",
            "Epoch 160 loss: 0.559 r2: 0.077\n",
            "Epoch 170 loss: 0.555 r2: 0.064\n",
            "Epoch 180 loss: 0.555 r2: 0.045\n",
            "Epoch 190 loss: 0.554 r2: 0.046\n",
            "Epoch 200 loss: 0.554 r2: 0.050\n",
            "Epoch 210 loss: 0.554 r2: 0.057\n",
            "Epoch 220 loss: 0.556 r2: 0.065\n",
            "Epoch 230 loss: 0.558 r2: 0.071\n",
            "Epoch 240 loss: 0.560 r2: 0.076\n",
            "Epoch 250 loss: 0.562 r2: 0.079\n",
            "Epoch 260 loss: 0.563 r2: 0.081\n",
            "Epoch 270 loss: 0.560 r2: 0.078\n",
            "Epoch 280 loss: 0.551 r2: 0.033\n",
            "Epoch 290 loss: 0.550 r2: 0.047\n",
            "Epoch 300 loss: 0.550 r2: 0.047\n",
            "Epoch 310 loss: 0.549 r2: 0.051\n",
            "Epoch 320 loss: 0.550 r2: 0.058\n",
            "Epoch 330 loss: 0.551 r2: 0.062\n",
            "Epoch 340 loss: 0.551 r2: 0.063\n",
            "Epoch 350 loss: 0.552 r2: 0.061\n",
            "Epoch 360 loss: 0.552 r2: 0.058\n",
            "Epoch 370 loss: 0.553 r2: 0.054\n",
            "Epoch 380 loss: 0.554 r2: 0.051\n",
            "Epoch 390 loss: 0.555 r2: 0.048\n",
            "Epoch 400 loss: 0.555 r2: 0.046\n",
            "Epoch 410 loss: 0.555 r2: 0.045\n",
            "Epoch 420 loss: 0.555 r2: 0.044\n",
            "Epoch 430 loss: 0.555 r2: 0.043\n",
            "Epoch 440 loss: 0.555 r2: 0.043\n",
            "Epoch 450 loss: 0.555 r2: 0.043\n",
            "Epoch 460 loss: 0.555 r2: 0.044\n",
            "Epoch 470 loss: 0.555 r2: 0.044\n",
            "Epoch 480 loss: 0.554 r2: 0.045\n",
            "Epoch 490 loss: 0.554 r2: 0.046\n",
            "Epoch 500 loss: 0.554 r2: 0.047\n",
            "Epoch 510 loss: 0.554 r2: 0.049\n",
            "Epoch 520 loss: 0.554 r2: 0.050\n",
            "Epoch 530 loss: 0.554 r2: 0.052\n",
            "Epoch 540 loss: 0.554 r2: 0.054\n",
            "Epoch 550 loss: 0.554 r2: 0.056\n",
            "Epoch 560 loss: 0.554 r2: 0.059\n",
            "Epoch 570 loss: 0.555 r2: 0.063\n",
            "Epoch 580 loss: 0.556 r2: 0.067\n",
            "Epoch 590 loss: 0.557 r2: 0.072\n",
            "Epoch 600 loss: 0.560 r2: 0.078\n",
            "Epoch 610 loss: 0.565 r2: 0.086\n",
            "Epoch 620 loss: 0.572 r2: 0.095\n",
            "Epoch 630 loss: 0.585 r2: 0.106\n",
            "Epoch 640 loss: 0.604 r2: 0.114\n",
            "Epoch 650 loss: 0.643 r2: 0.105\n",
            "Epoch 660 loss: 0.656 r2: 0.112\n",
            "Epoch 670 loss: 0.617 r2: 0.045\n",
            "Epoch 680 loss: 0.584 r2: 0.108\n",
            "Epoch 690 loss: 0.582 r2: 0.156\n",
            "Epoch 700 loss: 0.574 r2: 0.162\n",
            "Epoch 710 loss: 0.574 r2: 0.165\n",
            "Epoch 720 loss: 0.574 r2: 0.167\n",
            "Epoch 730 loss: 0.573 r2: 0.168\n",
            "Epoch 740 loss: 0.573 r2: 0.170\n",
            "Epoch 750 loss: 0.573 r2: 0.171\n",
            "Epoch 760 loss: 0.573 r2: 0.173\n",
            "Epoch 770 loss: 0.573 r2: 0.175\n",
            "Epoch 780 loss: 0.574 r2: 0.177\n",
            "Epoch 790 loss: 0.575 r2: 0.179\n",
            "Epoch 800 loss: 0.576 r2: 0.183\n",
            "Epoch 810 loss: 0.579 r2: 0.188\n",
            "Epoch 820 loss: 0.583 r2: 0.193\n",
            "Epoch 830 loss: 0.585 r2: 0.195\n",
            "Epoch 840 loss: 0.585 r2: 0.196\n",
            "Epoch 850 loss: 0.585 r2: 0.197\n",
            "Epoch 860 loss: 0.585 r2: 0.197\n",
            "Epoch 870 loss: 0.585 r2: 0.197\n",
            "Epoch 880 loss: 0.585 r2: 0.198\n",
            "Epoch 890 loss: 0.585 r2: 0.198\n",
            "Epoch 900 loss: 0.585 r2: 0.198\n",
            "Epoch 910 loss: 0.585 r2: 0.198\n",
            "Epoch 920 loss: 0.585 r2: 0.198\n",
            "Epoch 930 loss: 0.585 r2: 0.198\n",
            "Epoch 940 loss: 0.585 r2: 0.199\n",
            "Epoch 950 loss: 0.585 r2: 0.199\n",
            "Epoch 960 loss: 0.585 r2: 0.199\n",
            "Epoch 970 loss: 0.584 r2: 0.199\n",
            "Epoch 980 loss: 0.584 r2: 0.199\n",
            "Epoch 990 loss: 0.584 r2: 0.199\n",
            "Weights: [-15.023455321089536, -16.4135511770216, -11.499025004438797, -5.382449005805292, 6.616222006054594, 1.670474330460067, -2.655507974214441, 2.164019265174202]\n",
            "Bias: [-10.532795253786986, -6.525714581179933, 0.585475677009816]\n",
            "Learning rate: 0.1\n",
            "Loss for test dataset\n",
            "Loss: 0.533, r2: 0.205\n",
            "Loss for training dataset\n",
            "Loss: 0.584, r2: 0.199\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.610 r2: 0.210\n",
            "Epoch 10 loss: 0.631 r2: 0.191\n",
            "Epoch 20 loss: 0.620 r2: 0.194\n",
            "Epoch 30 loss: 0.621 r2: 0.182\n",
            "Epoch 40 loss: 0.620 r2: 0.186\n",
            "Epoch 50 loss: 0.634 r2: 0.186\n",
            "Epoch 60 loss: 0.622 r2: 0.196\n",
            "Epoch 70 loss: 0.616 r2: 0.195\n",
            "Epoch 80 loss: 0.616 r2: 0.187\n",
            "Epoch 90 loss: 0.619 r2: 0.195\n",
            "Epoch 100 loss: 0.618 r2: 0.194\n",
            "Epoch 110 loss: 0.618 r2: 0.199\n",
            "Epoch 120 loss: 0.615 r2: 0.192\n",
            "Epoch 130 loss: 0.617 r2: 0.194\n",
            "Epoch 140 loss: 0.618 r2: 0.187\n",
            "Epoch 150 loss: 0.619 r2: 0.182\n",
            "Epoch 160 loss: 0.617 r2: 0.185\n",
            "Epoch 170 loss: 0.617 r2: 0.187\n",
            "Epoch 180 loss: 0.617 r2: 0.196\n",
            "Epoch 190 loss: 0.617 r2: 0.205\n",
            "Epoch 200 loss: 0.616 r2: 0.194\n",
            "Epoch 210 loss: 0.617 r2: 0.191\n",
            "Epoch 220 loss: 0.618 r2: 0.198\n",
            "Epoch 230 loss: 0.610 r2: 0.211\n",
            "Epoch 240 loss: 0.616 r2: 0.202\n",
            "Epoch 250 loss: 0.623 r2: 0.209\n",
            "Epoch 260 loss: 0.618 r2: 0.206\n",
            "Epoch 270 loss: 0.619 r2: 0.192\n",
            "Epoch 280 loss: 0.616 r2: 0.192\n",
            "Epoch 290 loss: 0.621 r2: 0.189\n",
            "Epoch 300 loss: 0.616 r2: 0.195\n",
            "Epoch 310 loss: 0.614 r2: 0.218\n",
            "Epoch 320 loss: 0.618 r2: 0.201\n",
            "Epoch 330 loss: 0.619 r2: 0.205\n",
            "Epoch 340 loss: 0.621 r2: 0.208\n",
            "Epoch 350 loss: 0.620 r2: 0.203\n",
            "Epoch 360 loss: 0.622 r2: 0.208\n",
            "Epoch 370 loss: 0.618 r2: 0.201\n",
            "Epoch 380 loss: 0.615 r2: 0.200\n",
            "Epoch 390 loss: 0.620 r2: 0.193\n",
            "Epoch 400 loss: 0.621 r2: 0.207\n",
            "Epoch 410 loss: 0.619 r2: 0.204\n",
            "Epoch 420 loss: 0.617 r2: 0.195\n",
            "Epoch 430 loss: 0.617 r2: 0.204\n",
            "Epoch 440 loss: 0.617 r2: 0.204\n",
            "Epoch 450 loss: 0.620 r2: 0.195\n",
            "Epoch 460 loss: 0.613 r2: 0.207\n",
            "Epoch 470 loss: 0.618 r2: 0.208\n",
            "Epoch 480 loss: 0.623 r2: 0.204\n",
            "Epoch 490 loss: 0.625 r2: 0.214\n",
            "Epoch 500 loss: 0.621 r2: 0.203\n",
            "Epoch 510 loss: 0.618 r2: 0.195\n",
            "Epoch 520 loss: 0.619 r2: 0.205\n",
            "Epoch 530 loss: 0.618 r2: 0.212\n",
            "Epoch 540 loss: 0.617 r2: 0.198\n",
            "Epoch 550 loss: 0.622 r2: 0.204\n",
            "Epoch 560 loss: 0.622 r2: 0.207\n",
            "Epoch 570 loss: 0.620 r2: 0.210\n",
            "Epoch 580 loss: 0.621 r2: 0.190\n",
            "Epoch 590 loss: 0.623 r2: 0.204\n",
            "Epoch 600 loss: 0.619 r2: 0.196\n",
            "Epoch 610 loss: 0.620 r2: 0.200\n",
            "Epoch 620 loss: 0.617 r2: 0.199\n",
            "Epoch 630 loss: 0.620 r2: 0.204\n",
            "Epoch 640 loss: 0.622 r2: 0.208\n",
            "Epoch 650 loss: 0.621 r2: 0.198\n",
            "Epoch 660 loss: 0.621 r2: 0.194\n",
            "Epoch 670 loss: 0.623 r2: 0.210\n",
            "Epoch 680 loss: 0.620 r2: 0.201\n",
            "Epoch 690 loss: 0.623 r2: 0.197\n",
            "Epoch 700 loss: 0.620 r2: 0.206\n",
            "Epoch 710 loss: 0.624 r2: 0.217\n",
            "Epoch 720 loss: 0.623 r2: 0.206\n",
            "Epoch 730 loss: 0.618 r2: 0.209\n",
            "Epoch 740 loss: 0.622 r2: 0.212\n",
            "Epoch 750 loss: 0.624 r2: 0.208\n",
            "Epoch 760 loss: 0.626 r2: 0.218\n",
            "Epoch 770 loss: 0.621 r2: 0.207\n",
            "Epoch 780 loss: 0.623 r2: 0.215\n",
            "Epoch 790 loss: 0.678 r2: 0.211\n",
            "Epoch 800 loss: 0.621 r2: 0.198\n",
            "Epoch 810 loss: 0.620 r2: 0.211\n",
            "Epoch 820 loss: 0.621 r2: 0.213\n",
            "Epoch 830 loss: 0.621 r2: 0.208\n",
            "Epoch 840 loss: 0.619 r2: 0.205\n",
            "Epoch 850 loss: 0.682 r2: 0.219\n",
            "Epoch 860 loss: 0.621 r2: 0.212\n",
            "Epoch 870 loss: 0.621 r2: 0.212\n",
            "Epoch 880 loss: 0.619 r2: 0.207\n",
            "Epoch 890 loss: 0.620 r2: 0.197\n",
            "Epoch 900 loss: 0.618 r2: 0.208\n",
            "Epoch 910 loss: 0.624 r2: 0.205\n",
            "Epoch 920 loss: 0.620 r2: 0.212\n",
            "Epoch 930 loss: 0.621 r2: 0.211\n",
            "Epoch 940 loss: 0.622 r2: 0.202\n",
            "Epoch 950 loss: 0.624 r2: 0.209\n",
            "Epoch 960 loss: 0.624 r2: 0.211\n",
            "Epoch 970 loss: 0.623 r2: 0.208\n",
            "Epoch 980 loss: 0.620 r2: 0.222\n",
            "Epoch 990 loss: 0.623 r2: 0.212\n",
            "Weights: [-20.342376820823134, -23.19037103941059, -16.39077962287053, -7.06578304303611, 9.482661661143426, 1.049029409039068, -2.690219789671666, 2.283969491184744]\n",
            "Bias: [-14.86750654888172, -8.176353504596488, 1.0056271017845335]\n",
            "Learning rate: 0.2\n",
            "Loss for test dataset\n",
            "Loss: 0.547, r2: 0.232\n",
            "Loss for training dataset\n",
            "Loss: 0.621, r2: 0.203\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.633 r2: 0.225\n",
            "Epoch 10 loss: 0.622 r2: 0.171\n",
            "Epoch 20 loss: 0.639 r2: 0.210\n",
            "Epoch 30 loss: 0.640 r2: 0.209\n",
            "Epoch 40 loss: 0.638 r2: 0.205\n",
            "Epoch 50 loss: 0.626 r2: 0.196\n",
            "Epoch 60 loss: 0.635 r2: 0.180\n",
            "Epoch 70 loss: 0.632 r2: 0.177\n",
            "Epoch 80 loss: 0.819 r2: 0.027\n",
            "Epoch 90 loss: 0.702 r2: -0.048\n",
            "Epoch 100 loss: 0.634 r2: 0.155\n",
            "Epoch 110 loss: 0.643 r2: 0.164\n",
            "Epoch 120 loss: 0.652 r2: 0.175\n",
            "Epoch 130 loss: 0.729 r2: 0.219\n",
            "Epoch 140 loss: 0.653 r2: 0.193\n",
            "Epoch 150 loss: 0.766 r2: 0.074\n",
            "Epoch 160 loss: 0.765 r2: 0.076\n",
            "Epoch 170 loss: 0.765 r2: 0.078\n",
            "Epoch 180 loss: 0.765 r2: 0.079\n",
            "Epoch 190 loss: 0.765 r2: 0.079\n",
            "Epoch 200 loss: 0.765 r2: 0.079\n",
            "Epoch 210 loss: 0.765 r2: 0.080\n",
            "Epoch 220 loss: 0.765 r2: 0.080\n",
            "Epoch 230 loss: 0.765 r2: 0.080\n",
            "Epoch 240 loss: 0.765 r2: 0.080\n",
            "Epoch 250 loss: 0.765 r2: 0.081\n",
            "Epoch 260 loss: 0.765 r2: 0.081\n",
            "Epoch 270 loss: 0.765 r2: 0.081\n",
            "Epoch 280 loss: 0.765 r2: 0.081\n",
            "Epoch 290 loss: 0.765 r2: 0.081\n",
            "Epoch 300 loss: 0.765 r2: 0.081\n",
            "Epoch 310 loss: 0.765 r2: 0.081\n",
            "Epoch 320 loss: 0.765 r2: 0.082\n",
            "Epoch 330 loss: 0.765 r2: 0.082\n",
            "Epoch 340 loss: 0.765 r2: 0.082\n",
            "Epoch 350 loss: 0.765 r2: 0.082\n",
            "Epoch 360 loss: 0.765 r2: 0.082\n",
            "Epoch 370 loss: 0.765 r2: 0.082\n",
            "Epoch 380 loss: 0.765 r2: 0.082\n",
            "Epoch 390 loss: 0.765 r2: 0.082\n",
            "Epoch 400 loss: 0.765 r2: 0.082\n",
            "Epoch 410 loss: 0.765 r2: 0.082\n",
            "Epoch 420 loss: 0.765 r2: 0.082\n",
            "Epoch 430 loss: 0.765 r2: 0.082\n",
            "Epoch 440 loss: 0.765 r2: 0.082\n",
            "Epoch 450 loss: 0.765 r2: 0.082\n",
            "Epoch 460 loss: 0.765 r2: 0.082\n",
            "Epoch 470 loss: 0.765 r2: 0.083\n",
            "Epoch 480 loss: 0.765 r2: 0.082\n",
            "Epoch 490 loss: 0.765 r2: 0.083\n",
            "Epoch 500 loss: 0.765 r2: 0.083\n",
            "Epoch 510 loss: 0.765 r2: 0.083\n",
            "Epoch 520 loss: 0.765 r2: 0.083\n",
            "Epoch 530 loss: 0.765 r2: 0.083\n",
            "Epoch 540 loss: 0.765 r2: 0.083\n",
            "Epoch 550 loss: 0.764 r2: 0.083\n",
            "Epoch 560 loss: 0.764 r2: 0.084\n",
            "Epoch 570 loss: 0.760 r2: 0.082\n",
            "Epoch 580 loss: 0.673 r2: 0.114\n",
            "Epoch 590 loss: 0.761 r2: 0.047\n",
            "Epoch 600 loss: 0.778 r2: 0.023\n",
            "Epoch 610 loss: 0.599 r2: 0.088\n",
            "Epoch 620 loss: 0.635 r2: 0.167\n",
            "Epoch 630 loss: 0.689 r2: 0.214\n",
            "Epoch 640 loss: 0.694 r2: 0.214\n",
            "Epoch 650 loss: 0.692 r2: 0.230\n",
            "Epoch 660 loss: 0.732 r2: 0.044\n",
            "Epoch 670 loss: 0.728 r2: 0.046\n",
            "Epoch 680 loss: 0.723 r2: 0.059\n",
            "Epoch 690 loss: 0.710 r2: -0.030\n",
            "Epoch 700 loss: 0.816 r2: 0.148\n",
            "Epoch 710 loss: 0.816 r2: 0.148\n",
            "Epoch 720 loss: 0.816 r2: 0.148\n",
            "Epoch 730 loss: 0.816 r2: 0.148\n",
            "Epoch 740 loss: 0.814 r2: 0.147\n",
            "Epoch 750 loss: 0.726 r2: 0.059\n",
            "Epoch 760 loss: 0.732 r2: 0.073\n",
            "Epoch 770 loss: 0.733 r2: 0.074\n",
            "Epoch 780 loss: 0.733 r2: 0.075\n",
            "Epoch 790 loss: 0.733 r2: 0.075\n",
            "Epoch 800 loss: 0.733 r2: 0.075\n",
            "Epoch 810 loss: 0.733 r2: 0.075\n",
            "Epoch 820 loss: 0.733 r2: 0.075\n",
            "Epoch 830 loss: 0.733 r2: 0.075\n",
            "Epoch 840 loss: 0.733 r2: 0.075\n",
            "Epoch 850 loss: 0.733 r2: 0.075\n",
            "Epoch 860 loss: 0.733 r2: 0.076\n",
            "Epoch 870 loss: 0.733 r2: 0.076\n",
            "Epoch 880 loss: 0.733 r2: 0.076\n",
            "Epoch 890 loss: 0.733 r2: 0.076\n",
            "Epoch 900 loss: 0.733 r2: 0.076\n",
            "Epoch 910 loss: 0.733 r2: 0.076\n",
            "Epoch 920 loss: 0.733 r2: 0.076\n",
            "Epoch 930 loss: 0.733 r2: 0.076\n",
            "Epoch 940 loss: 0.733 r2: 0.076\n",
            "Epoch 950 loss: 0.733 r2: 0.076\n",
            "Epoch 960 loss: 0.733 r2: 0.076\n",
            "Epoch 970 loss: 0.733 r2: 0.076\n",
            "Epoch 980 loss: 0.733 r2: 0.076\n",
            "Epoch 990 loss: 0.733 r2: 0.076\n",
            "Weights: [-21.232563044478372, -30.317011584730835, -22.455431866860927, -11.418238373173637, 9.009954606668668, -2.7591879111840507, -5.152736506620786, 4.741863775057652]\n",
            "Bias: [-8.419139069956747, -20.88113210935916, 0.7172431632752854]\n",
            "Learning rate: 0.3\n",
            "Loss for test dataset\n",
            "Loss: 0.618, r2: 0.212\n",
            "Loss for training dataset\n",
            "Loss: 0.733, r2: 0.076\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.809 r2: 0.144\n",
            "Epoch 10 loss: 0.817 r2: 0.148\n",
            "Epoch 20 loss: 0.817 r2: 0.148\n",
            "Epoch 30 loss: 0.817 r2: 0.148\n",
            "Epoch 40 loss: 0.817 r2: 0.148\n",
            "Epoch 50 loss: 0.817 r2: 0.148\n",
            "Epoch 60 loss: 0.817 r2: 0.148\n",
            "Epoch 70 loss: 0.816 r2: 0.148\n",
            "Epoch 80 loss: 0.816 r2: 0.148\n",
            "Epoch 90 loss: 0.816 r2: 0.148\n",
            "Epoch 100 loss: 0.816 r2: 0.148\n",
            "Epoch 110 loss: 0.722 r2: 0.053\n",
            "Epoch 120 loss: 0.816 r2: 0.148\n",
            "Epoch 130 loss: 0.817 r2: 0.148\n",
            "Epoch 140 loss: 0.816 r2: 0.148\n",
            "Epoch 150 loss: 0.816 r2: 0.148\n",
            "Epoch 160 loss: 0.817 r2: 0.148\n",
            "Epoch 170 loss: 0.816 r2: 0.148\n",
            "Epoch 180 loss: 0.816 r2: 0.148\n",
            "Epoch 190 loss: 0.816 r2: 0.148\n",
            "Epoch 200 loss: 0.816 r2: 0.148\n",
            "Epoch 210 loss: 0.816 r2: 0.148\n",
            "Epoch 220 loss: 0.816 r2: 0.148\n",
            "Epoch 230 loss: 0.815 r2: 0.148\n",
            "Epoch 240 loss: 0.775 r2: 0.124\n",
            "Epoch 250 loss: 0.816 r2: 0.148\n",
            "Epoch 260 loss: 0.816 r2: 0.148\n",
            "Epoch 270 loss: 0.816 r2: 0.148\n",
            "Epoch 280 loss: 0.816 r2: 0.148\n",
            "Epoch 290 loss: 0.816 r2: 0.148\n",
            "Epoch 300 loss: 0.816 r2: 0.148\n",
            "Epoch 310 loss: 0.816 r2: 0.148\n",
            "Epoch 320 loss: 0.816 r2: 0.148\n",
            "Epoch 330 loss: 0.816 r2: 0.148\n",
            "Epoch 340 loss: 0.816 r2: 0.148\n",
            "Epoch 350 loss: 0.808 r2: 0.145\n",
            "Epoch 360 loss: 0.816 r2: 0.148\n",
            "Epoch 370 loss: 0.816 r2: 0.148\n",
            "Epoch 380 loss: 0.816 r2: 0.148\n",
            "Epoch 390 loss: 0.816 r2: 0.148\n",
            "Epoch 400 loss: 0.816 r2: 0.148\n",
            "Epoch 410 loss: 0.816 r2: 0.148\n",
            "Epoch 420 loss: 0.817 r2: 0.148\n",
            "Epoch 430 loss: 0.817 r2: 0.148\n",
            "Epoch 440 loss: 0.816 r2: 0.148\n",
            "Epoch 450 loss: 0.816 r2: 0.148\n",
            "Epoch 460 loss: 0.816 r2: 0.148\n",
            "Epoch 470 loss: 0.815 r2: 0.148\n",
            "Epoch 480 loss: 0.813 r2: 0.145\n",
            "Epoch 490 loss: 0.817 r2: 0.148\n",
            "Epoch 500 loss: 0.817 r2: 0.148\n",
            "Epoch 510 loss: 0.816 r2: 0.148\n",
            "Epoch 520 loss: 0.807 r2: 0.143\n",
            "Epoch 530 loss: 0.816 r2: 0.148\n",
            "Epoch 540 loss: 0.816 r2: 0.148\n",
            "Epoch 550 loss: 0.816 r2: 0.148\n",
            "Epoch 560 loss: 0.721 r2: 0.051\n",
            "Epoch 570 loss: 0.816 r2: 0.148\n",
            "Epoch 580 loss: 0.816 r2: 0.148\n",
            "Epoch 590 loss: 0.816 r2: 0.148\n",
            "Epoch 600 loss: 0.816 r2: 0.148\n",
            "Epoch 610 loss: 0.816 r2: 0.148\n",
            "Epoch 620 loss: 0.816 r2: 0.148\n",
            "Epoch 630 loss: 0.816 r2: 0.148\n",
            "Epoch 640 loss: 0.817 r2: 0.148\n",
            "Epoch 650 loss: 0.773 r2: 0.121\n",
            "Epoch 660 loss: 0.701 r2: -0.012\n",
            "Epoch 670 loss: 0.793 r2: 0.135\n",
            "Epoch 680 loss: 0.807 r2: 0.143\n",
            "Epoch 690 loss: 0.816 r2: 0.148\n",
            "Epoch 700 loss: 0.816 r2: 0.148\n",
            "Epoch 710 loss: 0.816 r2: 0.148\n",
            "Epoch 720 loss: 0.816 r2: 0.148\n",
            "Epoch 730 loss: 0.816 r2: 0.148\n",
            "Epoch 740 loss: 0.816 r2: 0.148\n",
            "Epoch 750 loss: 0.807 r2: 0.144\n",
            "Epoch 760 loss: 0.814 r2: 0.147\n",
            "Epoch 770 loss: 0.700 r2: -0.026\n",
            "Epoch 780 loss: 0.816 r2: 0.148\n",
            "Epoch 790 loss: 0.711 r2: 0.027\n",
            "Epoch 800 loss: 0.816 r2: 0.148\n",
            "Epoch 810 loss: 0.806 r2: 0.144\n",
            "Epoch 820 loss: 0.816 r2: 0.148\n",
            "Epoch 830 loss: 0.816 r2: 0.148\n",
            "Epoch 840 loss: 0.817 r2: 0.148\n",
            "Epoch 850 loss: 0.806 r2: 0.142\n",
            "Epoch 860 loss: 0.803 r2: 0.142\n",
            "Epoch 870 loss: 0.816 r2: 0.148\n",
            "Epoch 880 loss: 0.818 r2: 0.149\n",
            "Epoch 890 loss: 0.817 r2: 0.148\n",
            "Epoch 900 loss: 0.816 r2: 0.148\n",
            "Epoch 910 loss: 0.816 r2: 0.148\n",
            "Epoch 920 loss: 0.816 r2: 0.148\n",
            "Epoch 930 loss: 0.816 r2: 0.148\n",
            "Epoch 940 loss: 0.816 r2: 0.148\n",
            "Epoch 950 loss: 0.816 r2: 0.148\n",
            "Epoch 960 loss: 0.816 r2: 0.148\n",
            "Epoch 970 loss: 0.816 r2: 0.148\n",
            "Epoch 980 loss: 0.816 r2: 0.148\n",
            "Epoch 990 loss: 0.816 r2: 0.148\n",
            "Weights: [-21.988881937953018, -30.148702255392557, -22.27740855905862, -11.169871646911007, 8.940174112690139, -2.715638079989772, -4.826736001923518, 3.319960007980694]\n",
            "Bias: [-7.703652614581108, -21.227611283141876, 1.9465072281616043]\n",
            "Learning rate: 0.4\n",
            "Loss for test dataset\n",
            "Loss: 0.685, r2: 0.276\n",
            "Loss for training dataset\n",
            "Loss: 0.815, r2: 0.148\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.817 r2: 0.144\n",
            "Epoch 10 loss: 0.813 r2: 0.136\n",
            "Epoch 20 loss: 0.815 r2: 0.146\n",
            "Epoch 30 loss: 2.063 r2: -1291702214.567\n",
            "Epoch 40 loss: 2.063 r2: -837917957.305\n",
            "Epoch 50 loss: 2.063 r2: -483950622.630\n",
            "Epoch 60 loss: 2.063 r2: -228278238.809\n",
            "Epoch 70 loss: 2.063 r2: -68896584.275\n",
            "Epoch 80 loss: 2.063 r2: -2751469.720\n",
            "Epoch 90 loss: 2.063 r2: -54838783.061\n",
            "Epoch 100 loss: 2.062 r2: -328095.307\n",
            "Epoch 110 loss: 2.063 r2: -477468180556003840.000\n",
            "Epoch 120 loss: 2.063 r2: -477183882129926976.000\n",
            "Epoch 130 loss: 2.063 r2: -476899623816774720.000\n",
            "Epoch 140 loss: 2.063 r2: -476615560601204032.000\n",
            "Epoch 150 loss: 2.063 r2: -476331441776502720.000\n",
            "Epoch 160 loss: 2.063 r2: -476047600212969280.000\n",
            "Epoch 170 loss: 2.063 r2: -475763777625961216.000\n",
            "Epoch 180 loss: 2.063 r2: -475480146702278336.000\n",
            "Epoch 190 loss: 2.063 r2: -475196614794006464.000\n",
            "Epoch 200 loss: 2.063 r2: -474913166357101120.000\n",
            "Epoch 210 loss: 2.063 r2: -474629844638755200.000\n",
            "Epoch 220 loss: 2.063 r2: -474346612518470848.000\n",
            "Epoch 230 loss: 2.063 r2: -474063550864888768.000\n",
            "Epoch 240 loss: 2.063 r2: -473780516122755456.000\n",
            "Epoch 250 loss: 2.063 r2: -473497719197912832.000\n",
            "Epoch 260 loss: 2.063 r2: -473214915907971456.000\n",
            "Epoch 270 loss: 2.063 r2: -472932297900243904.000\n",
            "Epoch 280 loss: 2.063 r2: -472649789660856576.000\n",
            "Epoch 290 loss: 2.063 r2: -472367297916049472.000\n",
            "Epoch 300 loss: 2.063 r2: -472085036230677248.000\n",
            "Epoch 310 loss: 2.063 r2: -471802839642719360.000\n",
            "Epoch 320 loss: 2.063 r2: -471520788558352576.000\n",
            "Epoch 330 loss: 2.063 r2: -471238838412394112.000\n",
            "Epoch 340 loss: 2.063 r2: -470956978399224320.000\n",
            "Epoch 350 loss: 2.063 r2: -470675238548915520.000\n",
            "Epoch 360 loss: 2.063 r2: -470393625137006208.000\n",
            "Epoch 370 loss: 2.063 r2: -470112182616987328.000\n",
            "Epoch 380 loss: 2.063 r2: -469830799148376768.000\n",
            "Epoch 390 loss: 2.063 r2: -469549505082255104.000\n",
            "Epoch 400 loss: 2.063 r2: -469268327544021056.000\n",
            "Epoch 410 loss: 2.063 r2: -468987261721204096.000\n",
            "Epoch 420 loss: 2.063 r2: -468706337342157568.000\n",
            "Epoch 430 loss: 2.063 r2: -468425545897953472.000\n",
            "Epoch 440 loss: 2.063 r2: -468144816915764800.000\n",
            "Epoch 450 loss: 2.063 r2: -467864271950702016.000\n",
            "Epoch 460 loss: 2.063 r2: -467583776370573056.000\n",
            "Epoch 470 loss: 2.063 r2: -467303438801094400.000\n",
            "Epoch 480 loss: 2.063 r2: -467023194243241088.000\n",
            "Epoch 490 loss: 2.063 r2: -466743008205791232.000\n",
            "Epoch 500 loss: 2.063 r2: -466463037096090304.000\n",
            "Epoch 510 loss: 2.063 r2: -466183056564888960.000\n",
            "Epoch 520 loss: 2.063 r2: -465903237292727488.000\n",
            "Epoch 530 loss: 2.063 r2: -465623579920869632.000\n",
            "Epoch 540 loss: 2.063 r2: -465344069220178496.000\n",
            "Epoch 550 loss: 2.063 r2: -465064539684695680.000\n",
            "Epoch 560 loss: 2.063 r2: -464785223716129984.000\n",
            "Epoch 570 loss: 2.063 r2: -464506030798174592.000\n",
            "Epoch 580 loss: 2.063 r2: -464226922880736000.000\n",
            "Epoch 590 loss: 2.063 r2: -463947943975797312.000\n",
            "Epoch 600 loss: 2.063 r2: -463668991710708160.000\n",
            "Epoch 610 loss: 2.063 r2: -463390222981788736.000\n",
            "Epoch 620 loss: 2.063 r2: -463111597722955072.000\n",
            "Epoch 630 loss: 2.063 r2: -462833043262252992.000\n",
            "Epoch 640 loss: 2.063 r2: -462554570374144256.000\n",
            "Epoch 650 loss: 2.063 r2: -462276232265777664.000\n",
            "Epoch 660 loss: 2.063 r2: -461998037379133184.000\n",
            "Epoch 670 loss: 2.063 r2: -461720011736532480.000\n",
            "Epoch 680 loss: 2.063 r2: -461441991226710912.000\n",
            "Epoch 690 loss: 2.063 r2: -461164100891065024.000\n",
            "Epoch 700 loss: 2.063 r2: -460886363850510464.000\n",
            "Epoch 710 loss: 2.063 r2: -460608702737791488.000\n",
            "Epoch 720 loss: 2.063 r2: -460331190157683904.000\n",
            "Epoch 730 loss: 2.063 r2: -460053828622659968.000\n",
            "Epoch 740 loss: 2.063 r2: -459776529009131712.000\n",
            "Epoch 750 loss: 2.063 r2: -459499251694887296.000\n",
            "Epoch 760 loss: 2.063 r2: -459222229693590592.000\n",
            "Epoch 770 loss: 2.063 r2: -458945253697443776.000\n",
            "Epoch 780 loss: 2.063 r2: -458668402330067584.000\n",
            "Epoch 790 loss: 2.063 r2: -458391623796612480.000\n",
            "Epoch 800 loss: 2.063 r2: -458115024842604480.000\n",
            "Epoch 810 loss: 2.063 r2: -457838461464436864.000\n",
            "Epoch 820 loss: 2.063 r2: -457562062443685376.000\n",
            "Epoch 830 loss: 2.063 r2: -457285804638458496.000\n",
            "Epoch 840 loss: 2.063 r2: -457009639757447360.000\n",
            "Epoch 850 loss: 2.063 r2: -456733607501302080.000\n",
            "Epoch 860 loss: 2.063 r2: -456457658491835072.000\n",
            "Epoch 870 loss: 2.063 r2: -456181840325504896.000\n",
            "Epoch 880 loss: 2.063 r2: -455906102364864576.000\n",
            "Epoch 890 loss: 2.063 r2: -455630476093260928.000\n",
            "Epoch 900 loss: 2.063 r2: -455355004725718400.000\n",
            "Epoch 910 loss: 2.063 r2: -455079574617042304.000\n",
            "Epoch 920 loss: 2.063 r2: -454804311532378368.000\n",
            "Epoch 930 loss: 2.063 r2: -454529099925923072.000\n",
            "Epoch 940 loss: 2.063 r2: -454254095328081088.000\n",
            "Epoch 950 loss: 2.063 r2: -453979182106249152.000\n",
            "Epoch 960 loss: 2.063 r2: -453704337150728320.000\n",
            "Epoch 970 loss: 2.063 r2: -453429656095901440.000\n",
            "Epoch 980 loss: 2.063 r2: -453155028077269504.000\n",
            "Epoch 990 loss: 2.063 r2: -452880489493705088.000\n",
            "Weights: [-21.94583746785975, -30.17314128657689, -22.26677802053178, -7.807904205547288, 7.255327559002954, -2.052693465284317, -0.2174774097821736, -0.26123017417465877]\n",
            "Bias: [-7.744450460987464, -24.842639716091618, 9.661630553395629]\n",
            "Learning rate: 0.5\n",
            "Loss for test dataset\n",
            "Loss: 1.749, r2: -371489086280676288.000\n",
            "Loss for training dataset\n",
            "Loss: 2.063, r2: -452633583814154240.000\n",
            "\n",
            "\n",
            "Epoch 0 loss: 2.063 r2: -452600707951081024.000\n",
            "Epoch 10 loss: 2.063 r2: -452271568073544640.000\n",
            "Epoch 20 loss: 2.063 r2: -451942671947719104.000\n",
            "Epoch 30 loss: 2.063 r2: -451613904706454144.000\n",
            "Epoch 40 loss: 2.063 r2: -451285211596967680.000\n",
            "Epoch 50 loss: 2.063 r2: -450956791355523904.000\n",
            "Epoch 60 loss: 2.063 r2: -450628592608802496.000\n",
            "Epoch 70 loss: 2.063 r2: -450300449497489984.000\n",
            "Epoch 80 loss: 2.063 r2: -449972472390233152.000\n",
            "Epoch 90 loss: 2.063 r2: -449644626608912896.000\n",
            "Epoch 100 loss: 2.063 r2: -449317015659723776.000\n",
            "Epoch 110 loss: 2.063 r2: -448989526245046656.000\n",
            "Epoch 120 loss: 2.063 r2: -448662192966857920.000\n",
            "Epoch 130 loss: 2.063 r2: -448335029683923008.000\n",
            "Epoch 140 loss: 2.063 r2: -448008080961517696.000\n",
            "Epoch 150 loss: 2.063 r2: -447681216817745344.000\n",
            "Epoch 160 loss: 2.063 r2: -447354566484617664.000\n",
            "Epoch 170 loss: 2.063 r2: -447028048949178880.000\n",
            "Epoch 180 loss: 2.063 r2: -446701698795573376.000\n",
            "Epoch 190 loss: 2.063 r2: -446375499861526784.000\n",
            "Epoch 200 loss: 2.063 r2: -446049476141278912.000\n",
            "Epoch 210 loss: 2.063 r2: -445723621214614208.000\n",
            "Epoch 220 loss: 2.063 r2: -445397920569608000.000\n",
            "Epoch 230 loss: 2.063 r2: -445072366517924672.000\n",
            "Epoch 240 loss: 2.063 r2: -444746984999954560.000\n",
            "Epoch 250 loss: 2.063 r2: -444421759725485376.000\n",
            "Epoch 260 loss: 2.063 r2: -444096725930376960.000\n",
            "Epoch 270 loss: 2.063 r2: -443771841562425536.000\n",
            "Epoch 280 loss: 2.063 r2: -443447087131179200.000\n",
            "Epoch 290 loss: 2.063 r2: -443122467560935296.000\n",
            "Epoch 300 loss: 2.063 r2: -442798063090842688.000\n",
            "Epoch 310 loss: 2.063 r2: -442473828069782592.000\n",
            "Epoch 320 loss: 2.063 r2: -442149728226566208.000\n",
            "Epoch 330 loss: 2.063 r2: -441825795167243968.000\n",
            "Epoch 340 loss: 2.063 r2: -441501978106474496.000\n",
            "Epoch 350 loss: 2.063 r2: -441178377411290880.000\n",
            "Epoch 360 loss: 2.063 r2: -440854965031358784.000\n",
            "Epoch 370 loss: 2.063 r2: -440531667172002560.000\n",
            "Epoch 380 loss: 2.063 r2: -440208523733570368.000\n",
            "Epoch 390 loss: 2.063 r2: -439885574566990272.000\n",
            "Epoch 400 loss: 2.063 r2: -439562734688853440.000\n",
            "Epoch 410 loss: 2.063 r2: -439240072551156352.000\n",
            "Epoch 420 loss: 2.063 r2: -438917616923742592.000\n",
            "Epoch 430 loss: 2.063 r2: -438595261908896768.000\n",
            "Epoch 440 loss: 2.063 r2: -438273101242178304.000\n",
            "Epoch 450 loss: 2.063 r2: -437951131872532480.000\n",
            "Epoch 460 loss: 2.063 r2: -437629236301207040.000\n",
            "Epoch 470 loss: 2.063 r2: -437307596906803968.000\n",
            "Epoch 480 loss: 2.063 r2: -436986049850272064.000\n",
            "Epoch 490 loss: 2.063 r2: -436664653913452480.000\n",
            "Epoch 500 loss: 2.063 r2: -436343459049358400.000\n",
            "Epoch 510 loss: 2.063 r2: -436022416982196864.000\n",
            "Epoch 520 loss: 2.063 r2: -435701509943501312.000\n",
            "Epoch 530 loss: 2.063 r2: -435380835148267776.000\n",
            "Epoch 540 loss: 2.063 r2: -435060235031362368.000\n",
            "Epoch 550 loss: 2.063 r2: -434739832917992320.000\n",
            "Epoch 560 loss: 2.063 r2: -434419585762611776.000\n",
            "Epoch 570 loss: 2.063 r2: -434099492144344832.000\n",
            "Epoch 580 loss: 2.063 r2: -433779553997819136.000\n",
            "Epoch 590 loss: 2.063 r2: -433459793608346048.000\n",
            "Epoch 600 loss: 2.063 r2: -433140205444074560.000\n",
            "Epoch 610 loss: 2.063 r2: -432820719177386624.000\n",
            "Epoch 620 loss: 2.063 r2: -432501441265271104.000\n",
            "Epoch 630 loss: 2.063 r2: -432182280258798208.000\n",
            "Epoch 640 loss: 2.063 r2: -431863339494390784.000\n",
            "Epoch 650 loss: 2.063 r2: -431544568771881152.000\n",
            "Epoch 660 loss: 2.063 r2: -431225898774273408.000\n",
            "Epoch 670 loss: 2.063 r2: -430907366958421632.000\n",
            "Epoch 680 loss: 2.063 r2: -430589086474040320.000\n",
            "Epoch 690 loss: 2.063 r2: -430270883276871808.000\n",
            "Epoch 700 loss: 2.063 r2: -429952879370757248.000\n",
            "Epoch 710 loss: 2.063 r2: -429634993723445952.000\n",
            "Epoch 720 loss: 2.063 r2: -429317324863252096.000\n",
            "Epoch 730 loss: 2.063 r2: -428999806158920640.000\n",
            "Epoch 740 loss: 2.063 r2: -428682428680497344.000\n",
            "Epoch 750 loss: 2.063 r2: -428365191689015552.000\n",
            "Epoch 760 loss: 2.063 r2: -428048147225349760.000\n",
            "Epoch 770 loss: 2.063 r2: -427731223163803392.000\n",
            "Epoch 780 loss: 2.063 r2: -427414514564000128.000\n",
            "Epoch 790 loss: 2.063 r2: -427097929020034752.000\n",
            "Epoch 800 loss: 2.063 r2: -426781515126841344.000\n",
            "Epoch 810 loss: 2.063 r2: -426465221050932864.000\n",
            "Epoch 820 loss: 2.063 r2: -426149086563009920.000\n",
            "Epoch 830 loss: 2.063 r2: -425833170350989824.000\n",
            "Epoch 840 loss: 2.063 r2: -425517407386558208.000\n",
            "Epoch 850 loss: 2.063 r2: -425201784003906304.000\n",
            "Epoch 860 loss: 2.063 r2: -424886284971954368.000\n",
            "Epoch 870 loss: 2.063 r2: -424571016158915776.000\n",
            "Epoch 880 loss: 2.063 r2: -424255815389820224.000\n",
            "Epoch 890 loss: 2.063 r2: -423940828601008192.000\n",
            "Epoch 900 loss: 2.063 r2: -423625998988124672.000\n",
            "Epoch 910 loss: 2.063 r2: -423311367798076928.000\n",
            "Epoch 920 loss: 2.063 r2: -422996818141663552.000\n",
            "Epoch 930 loss: 2.063 r2: -422682467851736256.000\n",
            "Epoch 940 loss: 2.063 r2: -422368276826025472.000\n",
            "Epoch 950 loss: 2.063 r2: -422054206615246656.000\n",
            "Epoch 960 loss: 2.063 r2: -421740316190327552.000\n",
            "Epoch 970 loss: 2.063 r2: -421426585973343360.000\n",
            "Epoch 980 loss: 2.063 r2: -421113050844596352.000\n",
            "Epoch 990 loss: 2.063 r2: -420799621661174208.000\n",
            "Weights: [-21.945837702059077, -30.173141182931868, -22.266778036366663, -7.807904233038503, 7.25532758558364, -2.05269347273506, -0.22105096015263329, -0.2561754850559854]\n",
            "Bias: [-7.744450237748653, -24.842639696327826, 9.656560341210936]\n",
            "Learning rate: 0.6\n",
            "Loss for test dataset\n",
            "Loss: 1.749, r2: -348817245710986688.000\n",
            "Loss for training dataset\n",
            "Loss: 2.063, r2: -420517713338852032.000\n",
            "\n",
            "\n",
            "Epoch 0 loss: 2.063 r2: -420481139986403072.000\n",
            "Epoch 10 loss: 2.063 r2: -420115863342700352.000\n",
            "Epoch 20 loss: 2.063 r2: -419750822864357248.000\n",
            "Epoch 30 loss: 2.063 r2: -419386028249980736.000\n",
            "Epoch 40 loss: 2.063 r2: -419021397932687168.000\n",
            "Epoch 50 loss: 2.063 r2: -418657008503110592.000\n",
            "Epoch 60 loss: 2.063 r2: -418292812235476864.000\n",
            "Epoch 70 loss: 2.063 r2: -417928827992619520.000\n",
            "Epoch 80 loss: 2.063 r2: -417565092586408576.000\n",
            "Epoch 90 loss: 2.063 r2: -417201563734614848.000\n",
            "Epoch 100 loss: 2.063 r2: -416838202392375872.000\n",
            "Epoch 110 loss: 2.063 r2: -416475118372937664.000\n",
            "Epoch 120 loss: 2.063 r2: -416112244417671616.000\n",
            "Epoch 130 loss: 2.063 r2: -415749571318397632.000\n",
            "Epoch 140 loss: 2.063 r2: -415387106301356352.000\n",
            "Epoch 150 loss: 2.063 r2: -415024848206670784.000\n",
            "Epoch 160 loss: 2.063 r2: -414662761404785088.000\n",
            "Epoch 170 loss: 2.063 r2: -414300970970816192.000\n",
            "Epoch 180 loss: 2.063 r2: -413939385845687168.000\n",
            "Epoch 190 loss: 2.063 r2: -413577955690067776.000\n",
            "Epoch 200 loss: 2.063 r2: -413216787652148160.000\n",
            "Epoch 210 loss: 2.063 r2: -412855844403903552.000\n",
            "Epoch 220 loss: 2.063 r2: -412495088231382976.000\n",
            "Epoch 230 loss: 2.063 r2: -412134579953616192.000\n",
            "Epoch 240 loss: 2.063 r2: -411774203749482432.000\n",
            "Epoch 250 loss: 2.063 r2: -411414078569265664.000\n",
            "Epoch 260 loss: 2.063 r2: -411054225083349888.000\n",
            "Epoch 270 loss: 2.063 r2: -410694512760896192.000\n",
            "Epoch 280 loss: 2.063 r2: -410335043974365376.000\n",
            "Epoch 290 loss: 2.063 r2: -409975799589502144.000\n",
            "Epoch 300 loss: 2.063 r2: -409616769186459136.000\n",
            "Epoch 310 loss: 2.063 r2: -409257926779814208.000\n",
            "Epoch 320 loss: 2.063 r2: -408899313286683776.000\n",
            "Epoch 330 loss: 2.063 r2: -408540871290994752.000\n",
            "Epoch 340 loss: 2.063 r2: -408182697484042560.000\n",
            "Epoch 350 loss: 2.063 r2: -407824728003207552.000\n",
            "Epoch 360 loss: 2.063 r2: -407466952544325760.000\n",
            "Epoch 370 loss: 2.063 r2: -407109406129642752.000\n",
            "Epoch 380 loss: 2.063 r2: -406752104470341824.000\n",
            "Epoch 390 loss: 2.063 r2: -406394959074357504.000\n",
            "Epoch 400 loss: 2.063 r2: -406038081742082816.000\n",
            "Epoch 410 loss: 2.063 r2: -405681321778708928.000\n",
            "Epoch 420 loss: 2.063 r2: -405324843476354112.000\n",
            "Epoch 430 loss: 2.063 r2: -404968597260019584.000\n",
            "Epoch 440 loss: 2.063 r2: -404612545755322944.000\n",
            "Epoch 450 loss: 2.063 r2: -404256703834877824.000\n",
            "Epoch 460 loss: 2.063 r2: -403901056434574720.000\n",
            "Epoch 470 loss: 2.063 r2: -403545601432634176.000\n",
            "Epoch 480 loss: 2.063 r2: -403190375836236928.000\n",
            "Epoch 490 loss: 2.063 r2: -402835381498534080.000\n",
            "Epoch 500 loss: 2.063 r2: -402480636089719232.000\n",
            "Epoch 510 loss: 2.063 r2: -402126049892580864.000\n",
            "Epoch 520 loss: 2.063 r2: -401771710692183296.000\n",
            "Epoch 530 loss: 2.063 r2: -401417513937607040.000\n",
            "Epoch 540 loss: 2.063 r2: -401063582785346112.000\n",
            "Epoch 550 loss: 2.063 r2: -400709852475042944.000\n",
            "Epoch 560 loss: 2.063 r2: -400356327948389632.000\n",
            "Epoch 570 loss: 2.063 r2: -400003056425451200.000\n",
            "Epoch 580 loss: 2.063 r2: -399649960731020992.000\n",
            "Epoch 590 loss: 2.063 r2: -399297062825427008.000\n",
            "Epoch 600 loss: 2.063 r2: -398944377864279808.000\n",
            "Epoch 610 loss: 2.063 r2: -398591909505769984.000\n",
            "Epoch 620 loss: 2.063 r2: -398239671482672192.000\n",
            "Epoch 630 loss: 2.063 r2: -397887683568743680.000\n",
            "Epoch 640 loss: 2.063 r2: -397535804442981440.000\n",
            "Epoch 650 loss: 2.063 r2: -397184209741729728.000\n",
            "Epoch 660 loss: 2.063 r2: -396832833362393664.000\n",
            "Epoch 670 loss: 2.063 r2: -396481615418698496.000\n",
            "Epoch 680 loss: 2.063 r2: -396130635277543680.000\n",
            "Epoch 690 loss: 2.063 r2: -395779867085328384.000\n",
            "Epoch 700 loss: 2.063 r2: -395429332546823872.000\n",
            "Epoch 710 loss: 2.063 r2: -395078996355825216.000\n",
            "Epoch 720 loss: 2.063 r2: -394728828227965248.000\n",
            "Epoch 730 loss: 2.063 r2: -394378919966643136.000\n",
            "Epoch 740 loss: 2.063 r2: -394029215757217856.000\n",
            "Epoch 750 loss: 2.063 r2: -393679710144099008.000\n",
            "Epoch 760 loss: 2.063 r2: -393330390794066880.000\n",
            "Epoch 770 loss: 2.063 r2: -392981317024281728.000\n",
            "Epoch 780 loss: 2.063 r2: -392632462347658560.000\n",
            "Epoch 790 loss: 2.063 r2: -392283790423683648.000\n",
            "Epoch 800 loss: 2.063 r2: -391935369967343744.000\n",
            "Epoch 810 loss: 2.063 r2: -391587116523854272.000\n",
            "Epoch 820 loss: 2.063 r2: -391239063625961344.000\n",
            "Epoch 830 loss: 2.063 r2: -390891228980172800.000\n",
            "Epoch 840 loss: 2.063 r2: -390543644205255296.000\n",
            "Epoch 850 loss: 2.063 r2: -390196244096337280.000\n",
            "Epoch 860 loss: 2.063 r2: -389849063557003776.000\n",
            "Epoch 870 loss: 2.063 r2: -389502033747614720.000\n",
            "Epoch 880 loss: 2.063 r2: -389155271749351808.000\n",
            "Epoch 890 loss: 2.063 r2: -388808731254808064.000\n",
            "Epoch 900 loss: 2.063 r2: -388462339928618240.000\n",
            "Epoch 910 loss: 2.063 r2: -388116200856340736.000\n",
            "Epoch 920 loss: 2.063 r2: -387770297753746304.000\n",
            "Epoch 930 loss: 2.063 r2: -387424564784712576.000\n",
            "Epoch 940 loss: 2.063 r2: -387079068119449280.000\n",
            "Epoch 950 loss: 2.063 r2: -386733756658145664.000\n",
            "Epoch 960 loss: 2.063 r2: -386388670194460480.000\n",
            "Epoch 970 loss: 2.063 r2: -386043762006444352.000\n",
            "Epoch 980 loss: 2.063 r2: -385699078398817920.000\n",
            "Epoch 990 loss: 2.063 r2: -385354615159545536.000\n",
            "Weights: [-21.945837988687213, -30.1731410561124, -22.266778055713704, -7.807904264876617, 7.255327616367009, -2.052693481363323, -0.22536194737806517, -0.25011146811799434]\n",
            "Bias: [-7.744449964552358, -24.842639673438306, 9.650478354508877]\n",
            "Learning rate: 0.7\n",
            "Loss for test dataset\n",
            "Loss: 1.749, r2: -322981965035451520.000\n",
            "Loss for training dataset\n",
            "Loss: 2.063, r2: -385044766706658816.000\n",
            "\n",
            "\n",
            "Epoch 0 loss: 2.063 r2: -385005442002252224.000\n",
            "Epoch 10 loss: 2.063 r2: -384612249708905792.000\n",
            "Epoch 20 loss: 2.063 r2: -384219354593983744.000\n",
            "Epoch 30 loss: 2.063 r2: -383826736515119552.000\n",
            "Epoch 40 loss: 2.063 r2: -383434373717467456.000\n",
            "Epoch 50 loss: 2.063 r2: -383042297713946560.000\n",
            "Epoch 60 loss: 2.063 r2: -382650480778749440.000\n",
            "Epoch 70 loss: 2.063 r2: -382258900619875520.000\n",
            "Epoch 80 loss: 2.063 r2: -381867644915582400.000\n",
            "Epoch 90 loss: 2.063 r2: -381476645646038272.000\n",
            "Epoch 100 loss: 2.063 r2: -381085934063515328.000\n",
            "Epoch 110 loss: 2.063 r2: -380695498140611328.000\n",
            "Epoch 120 loss: 2.063 r2: -380305285121428736.000\n",
            "Epoch 130 loss: 2.063 r2: -379915365884930240.000\n",
            "Epoch 140 loss: 2.063 r2: -379525706400643520.000\n",
            "Epoch 150 loss: 2.063 r2: -379136365981983424.000\n",
            "Epoch 160 loss: 2.063 r2: -378747268576927872.000\n",
            "Epoch 170 loss: 2.063 r2: -378358415019748224.000\n",
            "Epoch 180 loss: 2.063 r2: -377969884144797248.000\n",
            "Epoch 190 loss: 2.063 r2: -377581566817907072.000\n",
            "Epoch 200 loss: 2.063 r2: -377193570480582272.000\n",
            "Epoch 210 loss: 2.063 r2: -376805820529345472.000\n",
            "Epoch 220 loss: 2.063 r2: -376418359100058624.000\n",
            "Epoch 230 loss: 2.063 r2: -376031146175396352.000\n",
            "Epoch 240 loss: 2.063 r2: -375644258514722048.000\n",
            "Epoch 250 loss: 2.063 r2: -375257581763669312.000\n",
            "Epoch 260 loss: 2.063 r2: -374871216250719744.000\n",
            "Epoch 270 loss: 2.063 r2: -374485063207498176.000\n",
            "Epoch 280 loss: 2.063 r2: -374099206376513728.000\n",
            "Epoch 290 loss: 2.063 r2: -373713672453568512.000\n",
            "Epoch 300 loss: 2.063 r2: -373328336062749760.000\n",
            "Epoch 310 loss: 2.063 r2: -372943329190246592.000\n",
            "Epoch 320 loss: 2.063 r2: -372558521517603776.000\n",
            "Epoch 330 loss: 2.063 r2: -372174026415218112.000\n",
            "Epoch 340 loss: 2.063 r2: -371789821202258624.000\n",
            "Epoch 350 loss: 2.063 r2: -371405881884749056.000\n",
            "Epoch 360 loss: 2.063 r2: -371022162133760448.000\n",
            "Epoch 370 loss: 2.063 r2: -370638751874803776.000\n",
            "Epoch 380 loss: 2.063 r2: -370255634206820352.000\n",
            "Epoch 390 loss: 2.063 r2: -369872721570266176.000\n",
            "Epoch 400 loss: 2.063 r2: -369490091021197120.000\n",
            "Epoch 410 loss: 2.063 r2: -369107800974790656.000\n",
            "Epoch 420 loss: 2.063 r2: -368725698739104320.000\n",
            "Epoch 430 loss: 2.063 r2: -368343912673837504.000\n",
            "Epoch 440 loss: 2.063 r2: -367962342673616064.000\n",
            "Epoch 450 loss: 2.063 r2: -367581073007592832.000\n",
            "Epoch 460 loss: 2.063 r2: -367200112615798720.000\n",
            "Epoch 470 loss: 2.063 r2: -366819389195264320.000\n",
            "Epoch 480 loss: 2.063 r2: -366438890942474304.000\n",
            "Epoch 490 loss: 2.063 r2: -366058729455767168.000\n",
            "Epoch 500 loss: 2.063 r2: -365678808808356608.000\n",
            "Epoch 510 loss: 2.063 r2: -365299142071032256.000\n",
            "Epoch 520 loss: 2.063 r2: -364919756950672256.000\n",
            "Epoch 530 loss: 2.063 r2: -364540622645238976.000\n",
            "Epoch 540 loss: 2.063 r2: -364161776389171264.000\n",
            "Epoch 550 loss: 2.063 r2: -363783183145288896.000\n",
            "Epoch 560 loss: 2.063 r2: -363404886641402816.000\n",
            "Epoch 570 loss: 2.063 r2: -363026829393124480.000\n",
            "Epoch 580 loss: 2.063 r2: -362649060659834048.000\n",
            "Epoch 590 loss: 2.063 r2: -362271525771379072.000\n",
            "Epoch 600 loss: 2.063 r2: -361894281005438592.000\n",
            "Epoch 610 loss: 2.063 r2: -361517319608396992.000\n",
            "Epoch 620 loss: 2.063 r2: -361140598184495616.000\n",
            "Epoch 630 loss: 2.063 r2: -360764148627652672.000\n",
            "Epoch 640 loss: 2.063 r2: -360387969362109312.000\n",
            "Epoch 650 loss: 2.063 r2: -360012045815987136.000\n",
            "Epoch 660 loss: 2.063 r2: -359636408735298112.000\n",
            "Epoch 670 loss: 2.063 r2: -359261061019648704.000\n",
            "Epoch 680 loss: 2.063 r2: -358885908649703552.000\n",
            "Epoch 690 loss: 2.063 r2: -358511102792242624.000\n",
            "Epoch 700 loss: 2.063 r2: -358136495588717632.000\n",
            "Epoch 710 loss: 2.063 r2: -357762190246873216.000\n",
            "Epoch 720 loss: 2.063 r2: -357388142968291968.000\n",
            "Epoch 730 loss: 2.063 r2: -357014369662577344.000\n",
            "Epoch 740 loss: 2.063 r2: -356640826075268288.000\n",
            "Epoch 750 loss: 2.063 r2: -356267598144692224.000\n",
            "Epoch 760 loss: 2.063 r2: -355894587557065472.000\n",
            "Epoch 770 loss: 2.063 r2: -355521878890339712.000\n",
            "Epoch 780 loss: 2.063 r2: -355149416958271808.000\n",
            "Epoch 790 loss: 2.063 r2: -354777213905310720.000\n",
            "Epoch 800 loss: 2.063 r2: -354405313941235776.000\n",
            "Epoch 810 loss: 2.063 r2: -354033674127738240.000\n",
            "Epoch 820 loss: 2.063 r2: -353662296136943424.000\n",
            "Epoch 830 loss: 2.063 r2: -353291149016027840.000\n",
            "Epoch 840 loss: 2.063 r2: -352920265255665792.000\n",
            "Epoch 850 loss: 2.063 r2: -352549701366336640.000\n",
            "Epoch 860 loss: 2.063 r2: -352179338689234048.000\n",
            "Epoch 870 loss: 2.063 r2: -351809267407169216.000\n",
            "Epoch 880 loss: 2.063 r2: -351439457593195648.000\n",
            "Epoch 890 loss: 2.063 r2: -351069926977244608.000\n",
            "Epoch 900 loss: 2.063 r2: -350700700732491904.000\n",
            "Epoch 910 loss: 2.063 r2: -350331624357359808.000\n",
            "Epoch 920 loss: 2.063 r2: -349962880067485504.000\n",
            "Epoch 930 loss: 2.063 r2: -349594430307995712.000\n",
            "Epoch 940 loss: 2.063 r2: -349226187541193728.000\n",
            "Epoch 950 loss: 2.063 r2: -348858263717445760.000\n",
            "Epoch 960 loss: 2.063 r2: -348490520677724992.000\n",
            "Epoch 970 loss: 2.063 r2: -348123152687324992.000\n",
            "Epoch 980 loss: 2.063 r2: -347755936996694592.000\n",
            "Epoch 990 loss: 2.063 r2: -347389059480690944.000\n",
            "Weights: [-21.945838335364712, -30.173140902758046, -22.266778079071315, -7.8079043009216615, 7.255327651216317, -2.0526934911318833, -0.23048882826761494, -0.24294666436250148]\n",
            "Bias: [-7.744449634131981, -24.84263964752598, 9.64329320440962]\n",
            "Learning rate: 0.8\n",
            "Loss for test dataset\n",
            "Loss: 1.749, r2: -294404305804121920.000\n",
            "Loss for training dataset\n",
            "Loss: 2.063, r2: -347059087882036800.000\n",
            "\n",
            "\n",
            "Epoch 0 loss: 2.063 r2: -347017833945740864.000\n",
            "Epoch 10 loss: 2.063 r2: -346605719108150080.000\n",
            "Epoch 20 loss: 2.063 r2: -346193860660398080.000\n",
            "Epoch 30 loss: 2.063 r2: -345782400515370240.000\n",
            "Epoch 40 loss: 2.063 r2: -345371276808975104.000\n",
            "Epoch 50 loss: 2.063 r2: -344960416337287232.000\n",
            "Epoch 60 loss: 2.063 r2: -344549948599208704.000\n",
            "Epoch 70 loss: 2.063 r2: -344139786723272640.000\n",
            "Epoch 80 loss: 2.063 r2: -343729991527690752.000\n",
            "Epoch 90 loss: 2.063 r2: -343320498974697344.000\n",
            "Epoch 100 loss: 2.063 r2: -342911322460369216.000\n",
            "Epoch 110 loss: 2.063 r2: -342502515184243776.000\n",
            "Epoch 120 loss: 2.063 r2: -342093999830714112.000\n",
            "Epoch 130 loss: 2.063 r2: -341685841021699840.000\n",
            "Epoch 140 loss: 2.063 r2: -341278040400599104.000\n",
            "Epoch 150 loss: 2.063 r2: -340870542568090496.000\n",
            "Epoch 160 loss: 2.063 r2: -340463349957497920.000\n",
            "Epoch 170 loss: 2.063 r2: -340056532950107072.000\n",
            "Epoch 180 loss: 2.063 r2: -339650003798634240.000\n",
            "Epoch 190 loss: 2.063 r2: -339243850616278336.000\n",
            "Epoch 200 loss: 2.063 r2: -338838027449842496.000\n",
            "Epoch 210 loss: 2.063 r2: -338432485502827072.000\n",
            "Epoch 220 loss: 2.063 r2: -338027301068248704.000\n",
            "Epoch 230 loss: 2.063 r2: -337622454852957888.000\n",
            "Epoch 240 loss: 2.063 r2: -337217927733493568.000\n",
            "Epoch 250 loss: 2.063 r2: -336813720644836736.000\n",
            "Epoch 260 loss: 2.063 r2: -336409866789588736.000\n",
            "Epoch 270 loss: 2.063 r2: -336006343134859456.000\n",
            "Epoch 280 loss: 2.063 r2: -335603115256883328.000\n",
            "Epoch 290 loss: 2.063 r2: -335200239741836096.000\n",
            "Epoch 300 loss: 2.063 r2: -334797712646042240.000\n",
            "Epoch 310 loss: 2.063 r2: -334395492256561088.000\n",
            "Epoch 320 loss: 2.063 r2: -333993574668913472.000\n",
            "Epoch 330 loss: 2.063 r2: -333592044832448320.000\n",
            "Epoch 340 loss: 2.063 r2: -333190824302841280.000\n",
            "Epoch 350 loss: 2.063 r2: -332789891307369920.000\n",
            "Epoch 360 loss: 2.063 r2: -332389323174688384.000\n",
            "Epoch 370 loss: 2.063 r2: -331989106075209856.000\n",
            "Epoch 380 loss: 2.063 r2: -331589175076808960.000\n",
            "Epoch 390 loss: 2.063 r2: -331189572747841344.000\n",
            "Epoch 400 loss: 2.063 r2: -330790309051879360.000\n",
            "Epoch 410 loss: 2.063 r2: -330391369621373760.000\n",
            "Epoch 420 loss: 2.063 r2: -329992771447740288.000\n",
            "Epoch 430 loss: 2.063 r2: -329594488534766656.000\n",
            "Epoch 440 loss: 2.063 r2: -329196522449453440.000\n",
            "Epoch 450 loss: 2.063 r2: -328798920580339008.000\n",
            "Epoch 460 loss: 2.063 r2: -328401609166003904.000\n",
            "Epoch 470 loss: 2.063 r2: -328004618279316992.000\n",
            "Epoch 480 loss: 2.063 r2: -327607976406499392.000\n",
            "Epoch 490 loss: 2.063 r2: -327211652211267968.000\n",
            "Epoch 500 loss: 2.063 r2: -326815697778640064.000\n",
            "Epoch 510 loss: 2.063 r2: -326419997409510912.000\n",
            "Epoch 520 loss: 2.063 r2: -326024652554232128.000\n",
            "Epoch 530 loss: 2.063 r2: -325629615346273088.000\n",
            "Epoch 540 loss: 2.063 r2: -325234959193501440.000\n",
            "Epoch 550 loss: 2.063 r2: -324840607668997248.000\n",
            "Epoch 560 loss: 2.063 r2: -324446549901437696.000\n",
            "Epoch 570 loss: 2.063 r2: -324052812674056192.000\n",
            "Epoch 580 loss: 2.063 r2: -323659435056890944.000\n",
            "Epoch 590 loss: 2.063 r2: -323266375501098624.000\n",
            "Epoch 600 loss: 2.063 r2: -322873637304353280.000\n",
            "Epoch 610 loss: 2.063 r2: -322481233958921088.000\n",
            "Epoch 620 loss: 2.063 r2: -322089139806732800.000\n",
            "Epoch 630 loss: 2.063 r2: -321697378633880064.000\n",
            "Epoch 640 loss: 2.063 r2: -321305941019733568.000\n",
            "Epoch 650 loss: 2.063 r2: -320914823555658624.000\n",
            "Epoch 660 loss: 2.063 r2: -320524030890227648.000\n",
            "Epoch 670 loss: 2.063 r2: -320133548616445952.000\n",
            "Epoch 680 loss: 2.063 r2: -319743415599531456.000\n",
            "Epoch 690 loss: 2.063 r2: -319353579305306816.000\n",
            "Epoch 700 loss: 2.063 r2: -318964094369562048.000\n",
            "Epoch 710 loss: 2.063 r2: -318574907098824768.000\n",
            "Epoch 720 loss: 2.063 r2: -318186057303544192.000\n",
            "Epoch 730 loss: 2.063 r2: -317797559391745984.000\n",
            "Epoch 740 loss: 2.063 r2: -317409347909145472.000\n",
            "Epoch 750 loss: 2.063 r2: -317021464643000064.000\n",
            "Epoch 760 loss: 2.063 r2: -316633894255102848.000\n",
            "Epoch 770 loss: 2.063 r2: -316246650867870080.000\n",
            "Epoch 780 loss: 2.063 r2: -315859757841510080.000\n",
            "Epoch 790 loss: 2.063 r2: -315473156624089280.000\n",
            "Epoch 800 loss: 2.063 r2: -315086895753831424.000\n",
            "Epoch 810 loss: 2.063 r2: -314700925306397312.000\n",
            "Epoch 820 loss: 2.063 r2: -314315294683954048.000\n",
            "Epoch 830 loss: 2.063 r2: -313929980746391616.000\n",
            "Epoch 840 loss: 2.063 r2: -313545010794398528.000\n",
            "Epoch 850 loss: 2.063 r2: -313160330242127424.000\n",
            "Epoch 860 loss: 2.063 r2: -312776016954435392.000\n",
            "Epoch 870 loss: 2.063 r2: -312391977229315648.000\n",
            "Epoch 880 loss: 2.063 r2: -312008273355516608.000\n",
            "Epoch 890 loss: 2.063 r2: -311624902075003264.000\n",
            "Epoch 900 loss: 2.063 r2: -311241838458339200.000\n",
            "Epoch 910 loss: 2.063 r2: -310859094414181888.000\n",
            "Epoch 920 loss: 2.063 r2: -310476659704705088.000\n",
            "Epoch 930 loss: 2.063 r2: -310094581487248384.000\n",
            "Epoch 940 loss: 2.063 r2: -309712818752801536.000\n",
            "Epoch 950 loss: 2.063 r2: -309331355223525568.000\n",
            "Epoch 960 loss: 2.063 r2: -308950212386533312.000\n",
            "Epoch 970 loss: 2.063 r2: -308569392918816768.000\n",
            "Epoch 980 loss: 2.063 r2: -308188894517360960.000\n",
            "Epoch 990 loss: 2.063 r2: -307808740154342016.000\n",
            "Weights: [-21.94583875212094, -30.173140718456267, -22.266778107089582, -7.80790434098452, 7.255327689949819, -2.0526935019888493, -0.23653270797834072, -0.2345638963332272]\n",
            "Bias: [-7.7444492369391265, -24.842639618726764, 9.634887797853326]\n",
            "Learning rate: 0.9\n",
            "Loss for test dataset\n",
            "Loss: 1.749, r2: -263637229368497696.000\n",
            "Loss for training dataset\n",
            "Loss: 2.063, r2: -307466857100662720.000\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Changing the neural network to accomodate ReLu activation function since the variables are overflowing.\n",
        "\n"
      ],
      "metadata": {
        "id": "QpCTlWcV9OUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = OurNeuralNetwork('reLu')\n",
        "for i in range(1,10):\n",
        "  weights = network.train_relu(np.array(X_train), np.array(Y_train),i/1000,1000)\n",
        "  print(\"Weights:\", weights[:8])\n",
        "  print(\"Bias:\",weights[8:])\n",
        "  print(\"Learning rate:\",i/1000)\n",
        "  print(\"Loss for test dataset\")\n",
        "  network.test(np.array(X_test), np.array(Y_test), weights)\n",
        "  print(\"Loss for training dataset\")\n",
        "  network.test(np.array(X_train), np.array(Y_train), weights)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL6UQHvk8uHk",
        "outputId": "4f4c8c78-1049-4683-8ff9-4e0f7a7b87d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: -0.06124247898885655 -0.32844299188181375 0.2157351850382089 -0.0654941459335875 0.5428034203824185 1.4423194919587998 0.9177770342003398 -0.8847451896359655\n",
            "Bias: 1.0700869532860755 -1.6109197644330917 -0.5743119408009657\n",
            "Epoch 0 loss: 0.901 r2: -26.061\n",
            "Epoch 10 loss: 0.873 r2: -6.116\n",
            "Epoch 20 loss: 0.871 r2: -7.321\n",
            "Epoch 30 loss: 0.867 r2: -8.263\n",
            "Epoch 40 loss: 0.865 r2: -8.595\n",
            "Epoch 50 loss: 0.864 r2: -8.659\n",
            "Epoch 60 loss: 0.864 r2: -8.637\n",
            "Epoch 70 loss: 0.864 r2: -8.604\n",
            "Epoch 80 loss: 0.864 r2: -8.584\n",
            "Epoch 90 loss: 0.864 r2: -8.573\n",
            "Epoch 100 loss: 0.865 r2: -8.567\n",
            "Epoch 110 loss: 0.865 r2: -8.572\n",
            "Epoch 120 loss: 0.865 r2: -8.589\n",
            "Epoch 130 loss: 0.865 r2: -8.607\n",
            "Epoch 140 loss: 0.865 r2: -8.629\n",
            "Epoch 150 loss: 0.865 r2: -8.656\n",
            "Epoch 160 loss: 0.865 r2: -8.681\n",
            "Epoch 170 loss: 0.865 r2: -8.705\n",
            "Epoch 180 loss: 0.865 r2: -8.725\n",
            "Epoch 190 loss: 0.865 r2: -8.741\n",
            "Epoch 200 loss: 0.865 r2: -8.755\n",
            "Epoch 210 loss: 0.865 r2: -8.764\n",
            "Epoch 220 loss: 0.865 r2: -8.772\n",
            "Epoch 230 loss: 0.865 r2: -8.778\n",
            "Epoch 240 loss: 0.865 r2: -8.783\n",
            "Epoch 250 loss: 0.865 r2: -8.786\n",
            "Epoch 260 loss: 0.865 r2: -8.789\n",
            "Epoch 270 loss: 0.865 r2: -8.791\n",
            "Epoch 280 loss: 0.865 r2: -8.793\n",
            "Epoch 290 loss: 0.865 r2: -8.795\n",
            "Epoch 300 loss: 0.865 r2: -8.797\n",
            "Epoch 310 loss: 0.865 r2: -8.797\n",
            "Epoch 320 loss: 0.865 r2: -8.794\n",
            "Epoch 330 loss: 0.864 r2: -8.792\n",
            "Epoch 340 loss: 0.864 r2: -8.788\n",
            "Epoch 350 loss: 0.864 r2: -8.784\n",
            "Epoch 360 loss: 0.864 r2: -8.779\n",
            "Epoch 370 loss: 0.864 r2: -8.774\n",
            "Epoch 380 loss: 0.864 r2: -8.769\n",
            "Epoch 390 loss: 0.864 r2: -8.765\n",
            "Epoch 400 loss: 0.864 r2: -8.760\n",
            "Epoch 410 loss: 0.864 r2: -8.755\n",
            "Epoch 420 loss: 0.864 r2: -8.751\n",
            "Epoch 430 loss: 0.864 r2: -8.746\n",
            "Epoch 440 loss: 0.864 r2: -8.742\n",
            "Epoch 450 loss: 0.864 r2: -8.738\n",
            "Epoch 460 loss: 0.864 r2: -8.733\n",
            "Epoch 470 loss: 0.865 r2: -8.729\n",
            "Epoch 480 loss: 0.865 r2: -8.725\n",
            "Epoch 490 loss: 0.865 r2: -8.720\n",
            "Epoch 500 loss: 0.865 r2: -8.715\n",
            "Epoch 510 loss: 0.865 r2: -8.711\n",
            "Epoch 520 loss: 0.865 r2: -8.706\n",
            "Epoch 530 loss: 0.865 r2: -8.702\n",
            "Epoch 540 loss: 0.865 r2: -8.697\n",
            "Epoch 550 loss: 0.865 r2: -8.693\n",
            "Epoch 560 loss: 0.865 r2: -8.688\n",
            "Epoch 570 loss: 0.865 r2: -8.684\n",
            "Epoch 580 loss: 0.865 r2: -8.679\n",
            "Epoch 590 loss: 0.865 r2: -8.675\n",
            "Epoch 600 loss: 0.865 r2: -8.671\n",
            "Epoch 610 loss: 0.865 r2: -8.666\n",
            "Epoch 620 loss: 0.865 r2: -8.662\n",
            "Epoch 630 loss: 0.865 r2: -8.657\n",
            "Epoch 640 loss: 0.865 r2: -8.653\n",
            "Epoch 650 loss: 0.865 r2: -8.649\n",
            "Epoch 660 loss: 0.865 r2: -8.644\n",
            "Epoch 670 loss: 0.865 r2: -8.640\n",
            "Epoch 680 loss: 0.865 r2: -8.636\n",
            "Epoch 690 loss: 0.865 r2: -8.632\n",
            "Epoch 700 loss: 0.865 r2: -8.627\n",
            "Epoch 710 loss: 0.865 r2: -8.624\n",
            "Epoch 720 loss: 0.865 r2: -8.620\n",
            "Epoch 730 loss: 0.865 r2: -8.616\n",
            "Epoch 740 loss: 0.865 r2: -8.612\n",
            "Epoch 750 loss: 0.865 r2: -8.608\n",
            "Epoch 760 loss: 0.865 r2: -8.604\n",
            "Epoch 770 loss: 0.865 r2: -8.600\n",
            "Epoch 780 loss: 0.865 r2: -8.596\n",
            "Epoch 790 loss: 0.865 r2: -8.592\n",
            "Epoch 800 loss: 0.865 r2: -8.589\n",
            "Epoch 810 loss: 0.865 r2: -8.585\n",
            "Epoch 820 loss: 0.865 r2: -8.581\n",
            "Epoch 830 loss: 0.865 r2: -8.577\n",
            "Epoch 840 loss: 0.865 r2: -8.573\n",
            "Epoch 850 loss: 0.865 r2: -8.569\n",
            "Epoch 860 loss: 0.865 r2: -8.565\n",
            "Epoch 870 loss: 0.865 r2: -8.561\n",
            "Epoch 880 loss: 0.865 r2: -8.557\n",
            "Epoch 890 loss: 0.865 r2: -8.553\n",
            "Epoch 900 loss: 0.865 r2: -8.550\n",
            "Epoch 910 loss: 0.865 r2: -8.546\n",
            "Epoch 920 loss: 0.865 r2: -8.542\n",
            "Epoch 930 loss: 0.865 r2: -8.538\n",
            "Epoch 940 loss: 0.865 r2: -8.535\n",
            "Epoch 950 loss: 0.865 r2: -8.531\n",
            "Epoch 960 loss: 0.865 r2: -8.527\n",
            "Epoch 970 loss: 0.865 r2: -8.522\n",
            "Epoch 980 loss: 0.865 r2: -8.518\n",
            "Epoch 990 loss: 0.865 r2: -8.513\n",
            "Weights: [1.5494029683852046, 1.7617387539136853, 2.338017339387036, -2.1641817860760706, -2.286362672802908, -1.289629006291386, 0.18488149857671837, -0.1563793224547858]\n",
            "Bias: [0.62749651273537, -2.1495415363342736, -0.23499094988604702]\n",
            "Learning rate: 0.001\n",
            "Loss for test dataset\n",
            "Loss: 0.675, r2: -5.186\n",
            "Loss for training dataset\n",
            "Loss: 0.865, r2: -8.509\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.865 r2: -8.694\n",
            "Epoch 10 loss: 0.865 r2: -8.654\n",
            "Epoch 20 loss: 0.865 r2: -8.621\n",
            "Epoch 30 loss: 0.865 r2: -8.589\n",
            "Epoch 40 loss: 0.865 r2: -8.558\n",
            "Epoch 50 loss: 0.865 r2: -8.529\n",
            "Epoch 60 loss: 0.865 r2: -8.501\n",
            "Epoch 70 loss: 0.865 r2: -8.475\n",
            "Epoch 80 loss: 0.865 r2: -8.449\n",
            "Epoch 90 loss: 0.865 r2: -8.425\n",
            "Epoch 100 loss: 0.865 r2: -8.403\n",
            "Epoch 110 loss: 0.865 r2: -8.381\n",
            "Epoch 120 loss: 0.865 r2: -8.361\n",
            "Epoch 130 loss: 0.865 r2: -8.342\n",
            "Epoch 140 loss: 0.865 r2: -8.323\n",
            "Epoch 150 loss: 0.865 r2: -8.305\n",
            "Epoch 160 loss: 0.866 r2: -8.288\n",
            "Epoch 170 loss: 0.866 r2: -8.270\n",
            "Epoch 180 loss: 0.866 r2: -8.253\n",
            "Epoch 190 loss: 0.866 r2: -8.237\n",
            "Epoch 200 loss: 0.866 r2: -8.220\n",
            "Epoch 210 loss: 0.866 r2: -8.204\n",
            "Epoch 220 loss: 0.866 r2: -8.187\n",
            "Epoch 230 loss: 0.866 r2: -8.172\n",
            "Epoch 240 loss: 0.866 r2: -8.156\n",
            "Epoch 250 loss: 0.866 r2: -8.140\n",
            "Epoch 260 loss: 0.866 r2: -8.125\n",
            "Epoch 270 loss: 0.866 r2: -8.110\n",
            "Epoch 280 loss: 0.866 r2: -8.095\n",
            "Epoch 290 loss: 0.866 r2: -8.080\n",
            "Epoch 300 loss: 0.866 r2: -8.065\n",
            "Epoch 310 loss: 0.866 r2: -8.051\n",
            "Epoch 320 loss: 0.866 r2: -8.036\n",
            "Epoch 330 loss: 0.866 r2: -8.021\n",
            "Epoch 340 loss: 0.866 r2: -8.006\n",
            "Epoch 350 loss: 0.866 r2: -7.992\n",
            "Epoch 360 loss: 0.867 r2: -7.977\n",
            "Epoch 370 loss: 0.867 r2: -7.962\n",
            "Epoch 380 loss: 0.867 r2: -7.948\n",
            "Epoch 390 loss: 0.867 r2: -7.934\n",
            "Epoch 400 loss: 0.867 r2: -7.919\n",
            "Epoch 410 loss: 0.867 r2: -7.905\n",
            "Epoch 420 loss: 0.867 r2: -7.892\n",
            "Epoch 430 loss: 0.867 r2: -7.878\n",
            "Epoch 440 loss: 0.867 r2: -7.865\n",
            "Epoch 450 loss: 0.867 r2: -7.852\n",
            "Epoch 460 loss: 0.867 r2: -7.839\n",
            "Epoch 470 loss: 0.867 r2: -7.826\n",
            "Epoch 480 loss: 0.867 r2: -7.812\n",
            "Epoch 490 loss: 0.867 r2: -7.799\n",
            "Epoch 500 loss: 0.867 r2: -7.786\n",
            "Epoch 510 loss: 0.867 r2: -7.773\n",
            "Epoch 520 loss: 0.867 r2: -7.760\n",
            "Epoch 530 loss: 0.867 r2: -7.747\n",
            "Epoch 540 loss: 0.867 r2: -7.735\n",
            "Epoch 550 loss: 0.867 r2: -7.722\n",
            "Epoch 560 loss: 0.867 r2: -7.709\n",
            "Epoch 570 loss: 0.868 r2: -7.697\n",
            "Epoch 580 loss: 0.868 r2: -7.684\n",
            "Epoch 590 loss: 0.868 r2: -7.672\n",
            "Epoch 600 loss: 0.868 r2: -7.660\n",
            "Epoch 610 loss: 0.868 r2: -7.647\n",
            "Epoch 620 loss: 0.868 r2: -7.635\n",
            "Epoch 630 loss: 0.868 r2: -7.623\n",
            "Epoch 640 loss: 0.868 r2: -7.611\n",
            "Epoch 650 loss: 0.868 r2: -7.599\n",
            "Epoch 660 loss: 0.868 r2: -7.588\n",
            "Epoch 670 loss: 0.868 r2: -7.576\n",
            "Epoch 680 loss: 0.868 r2: -7.564\n",
            "Epoch 690 loss: 0.868 r2: -7.552\n",
            "Epoch 700 loss: 0.868 r2: -7.540\n",
            "Epoch 710 loss: 0.868 r2: -7.529\n",
            "Epoch 720 loss: 0.868 r2: -7.517\n",
            "Epoch 730 loss: 0.868 r2: -7.505\n",
            "Epoch 740 loss: 0.868 r2: -7.494\n",
            "Epoch 750 loss: 0.868 r2: -7.482\n",
            "Epoch 760 loss: 0.868 r2: -7.470\n",
            "Epoch 770 loss: 0.868 r2: -7.458\n",
            "Epoch 780 loss: 0.868 r2: -7.445\n",
            "Epoch 790 loss: 0.868 r2: -7.433\n",
            "Epoch 800 loss: 0.868 r2: -7.421\n",
            "Epoch 810 loss: 0.868 r2: -7.409\n",
            "Epoch 820 loss: 0.868 r2: -7.396\n",
            "Epoch 830 loss: 0.868 r2: -7.384\n",
            "Epoch 840 loss: 0.868 r2: -7.371\n",
            "Epoch 850 loss: 0.868 r2: -7.359\n",
            "Epoch 860 loss: 0.868 r2: -7.346\n",
            "Epoch 870 loss: 0.868 r2: -7.333\n",
            "Epoch 880 loss: 0.868 r2: -7.320\n",
            "Epoch 890 loss: 0.868 r2: -7.307\n",
            "Epoch 900 loss: 0.868 r2: -7.294\n",
            "Epoch 910 loss: 0.868 r2: -7.281\n",
            "Epoch 920 loss: 0.868 r2: -7.268\n",
            "Epoch 930 loss: 0.868 r2: -7.253\n",
            "Epoch 940 loss: 0.868 r2: -7.239\n",
            "Epoch 950 loss: 0.868 r2: -7.225\n",
            "Epoch 960 loss: 0.868 r2: -7.210\n",
            "Epoch 970 loss: 0.868 r2: -7.196\n",
            "Epoch 980 loss: 0.868 r2: -7.181\n",
            "Epoch 990 loss: 0.868 r2: -7.166\n",
            "Weights: [3.2472183539508443, 2.7995837213323167, 5.32137736045481, -3.3055158514587815, -4.3384528183343996, -2.0368283635712916, 0.13222963051316738, -0.11658179589597495]\n",
            "Bias: [-1.6363520940954845, -4.946337873093112, -0.20827544851625024]\n",
            "Learning rate: 0.002\n",
            "Loss for test dataset\n",
            "Loss: 0.673, r2: -4.010\n",
            "Loss for training dataset\n",
            "Loss: 0.868, r2: -7.153\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.868 r2: -6.615\n",
            "Epoch 10 loss: 0.869 r2: -6.597\n",
            "Epoch 20 loss: 0.869 r2: -6.578\n",
            "Epoch 30 loss: 0.869 r2: -6.554\n",
            "Epoch 40 loss: 0.869 r2: -6.528\n",
            "Epoch 50 loss: 0.869 r2: -6.497\n",
            "Epoch 60 loss: 0.869 r2: -6.464\n",
            "Epoch 70 loss: 0.869 r2: -6.428\n",
            "Epoch 80 loss: 0.869 r2: -6.391\n",
            "Epoch 90 loss: 0.870 r2: -6.354\n",
            "Epoch 100 loss: 0.870 r2: -6.315\n",
            "Epoch 110 loss: 0.870 r2: -6.277\n",
            "Epoch 120 loss: 0.870 r2: -6.239\n",
            "Epoch 130 loss: 0.870 r2: -6.201\n",
            "Epoch 140 loss: 0.870 r2: -6.164\n",
            "Epoch 150 loss: 0.870 r2: -6.127\n",
            "Epoch 160 loss: 0.870 r2: -6.091\n",
            "Epoch 170 loss: 0.870 r2: -6.056\n",
            "Epoch 180 loss: 0.871 r2: -6.022\n",
            "Epoch 190 loss: 0.871 r2: -5.988\n",
            "Epoch 200 loss: 0.871 r2: -5.955\n",
            "Epoch 210 loss: 0.871 r2: -5.922\n",
            "Epoch 220 loss: 0.871 r2: -5.887\n",
            "Epoch 230 loss: 0.871 r2: -5.852\n",
            "Epoch 240 loss: 0.871 r2: -5.817\n",
            "Epoch 250 loss: 0.872 r2: -5.783\n",
            "Epoch 260 loss: 0.872 r2: -5.749\n",
            "Epoch 270 loss: 0.872 r2: -5.716\n",
            "Epoch 280 loss: 0.872 r2: -5.684\n",
            "Epoch 290 loss: 0.872 r2: -5.653\n",
            "Epoch 300 loss: 0.872 r2: -5.622\n",
            "Epoch 310 loss: 0.872 r2: -5.592\n",
            "Epoch 320 loss: 0.872 r2: -5.562\n",
            "Epoch 330 loss: 0.872 r2: -5.532\n",
            "Epoch 340 loss: 0.872 r2: -5.503\n",
            "Epoch 350 loss: 0.873 r2: -5.474\n",
            "Epoch 360 loss: 0.873 r2: -5.446\n",
            "Epoch 370 loss: 0.873 r2: -5.419\n",
            "Epoch 380 loss: 0.873 r2: -5.392\n",
            "Epoch 390 loss: 0.873 r2: -5.365\n",
            "Epoch 400 loss: 0.873 r2: -5.339\n",
            "Epoch 410 loss: 0.873 r2: -5.314\n",
            "Epoch 420 loss: 0.873 r2: -5.289\n",
            "Epoch 430 loss: 0.873 r2: -5.265\n",
            "Epoch 440 loss: 0.873 r2: -5.242\n",
            "Epoch 450 loss: 0.873 r2: -5.219\n",
            "Epoch 460 loss: 0.873 r2: -5.197\n",
            "Epoch 470 loss: 0.873 r2: -5.175\n",
            "Epoch 480 loss: 0.874 r2: -5.155\n",
            "Epoch 490 loss: 0.874 r2: -5.135\n",
            "Epoch 500 loss: 0.874 r2: -5.116\n",
            "Epoch 510 loss: 0.874 r2: -5.098\n",
            "Epoch 520 loss: 0.874 r2: -5.080\n",
            "Epoch 530 loss: 0.874 r2: -5.063\n",
            "Epoch 540 loss: 0.874 r2: -5.047\n",
            "Epoch 550 loss: 0.874 r2: -5.031\n",
            "Epoch 560 loss: 0.874 r2: -5.016\n",
            "Epoch 570 loss: 0.874 r2: -5.002\n",
            "Epoch 580 loss: 0.874 r2: -4.988\n",
            "Epoch 590 loss: 0.874 r2: -4.974\n",
            "Epoch 600 loss: 0.874 r2: -4.962\n",
            "Epoch 610 loss: 0.874 r2: -4.949\n",
            "Epoch 620 loss: 0.874 r2: -4.938\n",
            "Epoch 630 loss: 0.874 r2: -4.926\n",
            "Epoch 640 loss: 0.874 r2: -4.916\n",
            "Epoch 650 loss: 0.874 r2: -4.906\n",
            "Epoch 660 loss: 0.874 r2: -4.897\n",
            "Epoch 670 loss: 0.874 r2: -4.888\n",
            "Epoch 680 loss: 0.874 r2: -4.880\n",
            "Epoch 690 loss: 0.875 r2: -4.873\n",
            "Epoch 700 loss: 0.875 r2: -4.866\n",
            "Epoch 710 loss: 0.875 r2: -4.860\n",
            "Epoch 720 loss: 0.875 r2: -4.854\n",
            "Epoch 730 loss: 0.875 r2: -4.848\n",
            "Epoch 740 loss: 0.875 r2: -4.843\n",
            "Epoch 750 loss: 0.875 r2: -4.837\n",
            "Epoch 760 loss: 0.875 r2: -4.832\n",
            "Epoch 770 loss: 0.875 r2: -4.828\n",
            "Epoch 780 loss: 0.875 r2: -4.823\n",
            "Epoch 790 loss: 0.875 r2: -4.819\n",
            "Epoch 800 loss: 0.875 r2: -4.815\n",
            "Epoch 810 loss: 0.875 r2: -4.812\n",
            "Epoch 820 loss: 0.875 r2: -4.809\n",
            "Epoch 830 loss: 0.875 r2: -4.807\n",
            "Epoch 840 loss: 0.876 r2: -4.804\n",
            "Epoch 850 loss: 0.876 r2: -4.802\n",
            "Epoch 860 loss: 0.876 r2: -4.800\n",
            "Epoch 870 loss: 0.876 r2: -4.798\n",
            "Epoch 880 loss: 0.876 r2: -4.796\n",
            "Epoch 890 loss: 0.876 r2: -4.794\n",
            "Epoch 900 loss: 0.876 r2: -4.793\n",
            "Epoch 910 loss: 0.876 r2: -4.792\n",
            "Epoch 920 loss: 0.876 r2: -4.791\n",
            "Epoch 930 loss: 0.876 r2: -4.790\n",
            "Epoch 940 loss: 0.876 r2: -4.790\n",
            "Epoch 950 loss: 0.876 r2: -4.790\n",
            "Epoch 960 loss: 0.876 r2: -4.790\n",
            "Epoch 970 loss: 0.876 r2: -4.790\n",
            "Epoch 980 loss: 0.877 r2: -4.791\n",
            "Epoch 990 loss: 0.877 r2: -4.791\n",
            "Weights: [7.507864836758406, 8.102175343888463, 16.192760328319803, -6.9146311423748745, -5.443845386341704, -3.3182603927665824, 0.09948875143957629, -0.13982527968677014]\n",
            "Bias: [-13.735146958356319, -14.244529104785505, -0.1623768625726691]\n",
            "Learning rate: 0.003\n",
            "Loss for test dataset\n",
            "Loss: 0.697, r2: -2.352\n",
            "Loss for training dataset\n",
            "Loss: 0.877, r2: -4.792\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.885 r2: -3.857\n",
            "Epoch 10 loss: 0.882 r2: -4.113\n",
            "Epoch 20 loss: 0.880 r2: -4.305\n",
            "Epoch 30 loss: 0.879 r2: -4.444\n",
            "Epoch 40 loss: 0.879 r2: -4.545\n",
            "Epoch 50 loss: 0.879 r2: -4.619\n",
            "Epoch 60 loss: 0.878 r2: -4.673\n",
            "Epoch 70 loss: 0.878 r2: -4.713\n",
            "Epoch 80 loss: 0.878 r2: -4.743\n",
            "Epoch 90 loss: 0.879 r2: -4.767\n",
            "Epoch 100 loss: 0.879 r2: -4.786\n",
            "Epoch 110 loss: 0.879 r2: -4.804\n",
            "Epoch 120 loss: 0.879 r2: -4.819\n",
            "Epoch 130 loss: 0.879 r2: -4.833\n",
            "Epoch 140 loss: 0.880 r2: -4.846\n",
            "Epoch 150 loss: 0.880 r2: -4.858\n",
            "Epoch 160 loss: 0.880 r2: -4.870\n",
            "Epoch 170 loss: 0.881 r2: -4.880\n",
            "Epoch 180 loss: 0.881 r2: -4.891\n",
            "Epoch 190 loss: 0.881 r2: -4.902\n",
            "Epoch 200 loss: 0.882 r2: -4.912\n",
            "Epoch 210 loss: 0.882 r2: -4.922\n",
            "Epoch 220 loss: 0.882 r2: -4.931\n",
            "Epoch 230 loss: 0.883 r2: -4.939\n",
            "Epoch 240 loss: 0.883 r2: -4.948\n",
            "Epoch 250 loss: 0.884 r2: -4.955\n",
            "Epoch 260 loss: 0.884 r2: -4.963\n",
            "Epoch 270 loss: 0.884 r2: -4.969\n",
            "Epoch 280 loss: 0.885 r2: -4.976\n",
            "Epoch 290 loss: 0.885 r2: -4.983\n",
            "Epoch 300 loss: 0.885 r2: -4.989\n",
            "Epoch 310 loss: 0.886 r2: -4.996\n",
            "Epoch 320 loss: 0.886 r2: -5.002\n",
            "Epoch 330 loss: 0.886 r2: -5.008\n",
            "Epoch 340 loss: 0.887 r2: -5.014\n",
            "Epoch 350 loss: 0.887 r2: -5.019\n",
            "Epoch 360 loss: 0.888 r2: -5.023\n",
            "Epoch 370 loss: 0.888 r2: -5.026\n",
            "Epoch 380 loss: 0.888 r2: -5.026\n",
            "Epoch 390 loss: 0.889 r2: -5.028\n",
            "Epoch 400 loss: 0.889 r2: -5.031\n",
            "Epoch 410 loss: 0.890 r2: -5.033\n",
            "Epoch 420 loss: 0.890 r2: -5.034\n",
            "Epoch 430 loss: 0.891 r2: -5.035\n",
            "Epoch 440 loss: 0.891 r2: -5.036\n",
            "Epoch 450 loss: 0.891 r2: -5.035\n",
            "Epoch 460 loss: 0.892 r2: -5.034\n",
            "Epoch 470 loss: 0.892 r2: -5.033\n",
            "Epoch 480 loss: 0.893 r2: -5.031\n",
            "Epoch 490 loss: 0.893 r2: -5.028\n",
            "Epoch 500 loss: 0.894 r2: -5.024\n",
            "Epoch 510 loss: 0.894 r2: -5.020\n",
            "Epoch 520 loss: 0.895 r2: -5.015\n",
            "Epoch 530 loss: 0.895 r2: -5.010\n",
            "Epoch 540 loss: 0.896 r2: -5.004\n",
            "Epoch 550 loss: 0.896 r2: -4.997\n",
            "Epoch 560 loss: 0.897 r2: -4.989\n",
            "Epoch 570 loss: 0.897 r2: -4.981\n",
            "Epoch 580 loss: 0.898 r2: -4.973\n",
            "Epoch 590 loss: 0.898 r2: -4.963\n",
            "Epoch 600 loss: 0.899 r2: -4.953\n",
            "Epoch 610 loss: 0.899 r2: -4.943\n",
            "Epoch 620 loss: 0.900 r2: -4.932\n",
            "Epoch 630 loss: 0.900 r2: -4.920\n",
            "Epoch 640 loss: 0.901 r2: -4.908\n",
            "Epoch 650 loss: 0.901 r2: -4.895\n",
            "Epoch 660 loss: 0.902 r2: -4.882\n",
            "Epoch 670 loss: 0.902 r2: -4.868\n",
            "Epoch 680 loss: 0.903 r2: -4.854\n",
            "Epoch 690 loss: 0.903 r2: -4.840\n",
            "Epoch 700 loss: 0.904 r2: -4.825\n",
            "Epoch 710 loss: 0.905 r2: -4.810\n",
            "Epoch 720 loss: 0.905 r2: -4.794\n",
            "Epoch 730 loss: 0.906 r2: -4.779\n",
            "Epoch 740 loss: 0.906 r2: -4.767\n",
            "Epoch 750 loss: 0.906 r2: -4.754\n",
            "Epoch 760 loss: 0.907 r2: -4.742\n",
            "Epoch 770 loss: 0.907 r2: -4.729\n",
            "Epoch 780 loss: 0.907 r2: -4.716\n",
            "Epoch 790 loss: 0.908 r2: -4.703\n",
            "Epoch 800 loss: 0.908 r2: -4.691\n",
            "Epoch 810 loss: 0.908 r2: -4.678\n",
            "Epoch 820 loss: 0.909 r2: -4.665\n",
            "Epoch 830 loss: 0.909 r2: -4.651\n",
            "Epoch 840 loss: 0.909 r2: -4.636\n",
            "Epoch 850 loss: 0.910 r2: -4.622\n",
            "Epoch 860 loss: 0.910 r2: -4.608\n",
            "Epoch 870 loss: 0.910 r2: -4.594\n",
            "Epoch 880 loss: 0.911 r2: -4.581\n",
            "Epoch 890 loss: 0.911 r2: -4.569\n",
            "Epoch 900 loss: 0.911 r2: -4.558\n",
            "Epoch 910 loss: 0.912 r2: -4.548\n",
            "Epoch 920 loss: 0.912 r2: -4.539\n",
            "Epoch 930 loss: 0.912 r2: -4.531\n",
            "Epoch 940 loss: 0.912 r2: -4.523\n",
            "Epoch 950 loss: 0.913 r2: -4.515\n",
            "Epoch 960 loss: 0.913 r2: -4.509\n",
            "Epoch 970 loss: 0.913 r2: -4.503\n",
            "Epoch 980 loss: 0.913 r2: -4.498\n",
            "Epoch 990 loss: 0.914 r2: -4.494\n",
            "Weights: [11.71788245189623, 18.824305014506805, 36.16755538413983, -21.56292611901975, -0.5996693993491311, -7.364810115663866, 0.10423925177310187, -0.07700419988156132]\n",
            "Bias: [-41.24189862688072, -31.93382799411674, -0.026443819302470063]\n",
            "Learning rate: 0.004\n",
            "Loss for test dataset\n",
            "Loss: 0.864, r2: -6.767\n",
            "Loss for training dataset\n",
            "Loss: 0.914, r2: -4.491\n",
            "\n",
            "\n",
            "Epoch 0 loss: 0.977 r2: -2.135\n",
            "Epoch 10 loss: 0.929 r2: -3.592\n",
            "Epoch 20 loss: 0.923 r2: -4.159\n",
            "Epoch 30 loss: 0.921 r2: -4.402\n",
            "Epoch 40 loss: 0.920 r2: -4.521\n",
            "Epoch 50 loss: 0.920 r2: -4.587\n",
            "Epoch 60 loss: 0.921 r2: -4.628\n",
            "Epoch 70 loss: 0.921 r2: -4.656\n",
            "Epoch 80 loss: 0.921 r2: -4.677\n",
            "Epoch 90 loss: 0.922 r2: -4.703\n",
            "Epoch 100 loss: 0.923 r2: -4.727\n",
            "Epoch 110 loss: 0.923 r2: -4.751\n",
            "Epoch 120 loss: 0.924 r2: -4.773\n",
            "Epoch 130 loss: 0.925 r2: -4.795\n",
            "Epoch 140 loss: 0.925 r2: -4.815\n",
            "Epoch 150 loss: 0.926 r2: -4.835\n",
            "Epoch 160 loss: 0.927 r2: -4.855\n",
            "Epoch 170 loss: 0.928 r2: -4.875\n",
            "Epoch 180 loss: 0.928 r2: -4.893\n",
            "Epoch 190 loss: 0.929 r2: -4.911\n",
            "Epoch 200 loss: 0.930 r2: -4.929\n",
            "Epoch 210 loss: 0.931 r2: -4.947\n",
            "Epoch 220 loss: 0.932 r2: -4.965\n",
            "Epoch 230 loss: 0.933 r2: -4.984\n",
            "Epoch 240 loss: 0.934 r2: -5.004\n",
            "Epoch 250 loss: 0.935 r2: -5.026\n",
            "Epoch 260 loss: 0.935 r2: -5.053\n",
            "Epoch 270 loss: 0.936 r2: -5.083\n",
            "Epoch 280 loss: 0.937 r2: -5.117\n",
            "Epoch 290 loss: 0.938 r2: -5.153\n",
            "Epoch 300 loss: 0.939 r2: -5.193\n",
            "Epoch 310 loss: 0.940 r2: -5.236\n",
            "Epoch 320 loss: 0.940 r2: -5.282\n",
            "Epoch 330 loss: 0.941 r2: -5.333\n",
            "Epoch 340 loss: 0.942 r2: -5.390\n",
            "Epoch 350 loss: 0.943 r2: -5.453\n",
            "Epoch 360 loss: 0.943 r2: -5.523\n",
            "Epoch 370 loss: 0.944 r2: -5.598\n",
            "Epoch 380 loss: 0.945 r2: -5.677\n",
            "Epoch 390 loss: 0.946 r2: -5.762\n",
            "Epoch 400 loss: 0.947 r2: -5.853\n",
            "Epoch 410 loss: 0.947 r2: -5.948\n",
            "Epoch 420 loss: 0.948 r2: -6.047\n",
            "Epoch 430 loss: 0.949 r2: -6.146\n",
            "Epoch 440 loss: 0.950 r2: -6.245\n",
            "Epoch 450 loss: 0.951 r2: -6.341\n",
            "Epoch 460 loss: 0.953 r2: -6.432\n",
            "Epoch 470 loss: 0.954 r2: -6.517\n",
            "Epoch 480 loss: 0.956 r2: -6.591\n",
            "Epoch 490 loss: 0.958 r2: -6.654\n",
            "Epoch 500 loss: 0.960 r2: -6.703\n",
            "Epoch 510 loss: 0.962 r2: -6.738\n",
            "Epoch 520 loss: 0.965 r2: -6.760\n",
            "Epoch 530 loss: 0.969 r2: -6.777\n",
            "Epoch 540 loss: 0.973 r2: -6.804\n",
            "Epoch 550 loss: 0.976 r2: -6.860\n",
            "Epoch 560 loss: 0.980 r2: -6.981\n",
            "Epoch 570 loss: 0.984 r2: -7.201\n",
            "Epoch 580 loss: 0.987 r2: -7.545\n",
            "Epoch 590 loss: 0.989 r2: -8.017\n",
            "Epoch 600 loss: 0.991 r2: -8.598\n",
            "Epoch 610 loss: 0.992 r2: -9.274\n",
            "Epoch 620 loss: 0.994 r2: -10.031\n",
            "Epoch 630 loss: 0.994 r2: -10.794\n",
            "Epoch 640 loss: 0.995 r2: -11.480\n",
            "Epoch 650 loss: 0.995 r2: -12.057\n",
            "Epoch 660 loss: 0.995 r2: -12.517\n",
            "Epoch 670 loss: 0.995 r2: -12.871\n",
            "Epoch 680 loss: 0.995 r2: -13.137\n",
            "Epoch 690 loss: 0.995 r2: -13.335\n",
            "Epoch 700 loss: 0.995 r2: -13.484\n",
            "Epoch 710 loss: 0.996 r2: -13.642\n",
            "Epoch 720 loss: 0.996 r2: -13.917\n",
            "Epoch 730 loss: 0.997 r2: -14.276\n",
            "Epoch 740 loss: 0.997 r2: -14.669\n",
            "Epoch 750 loss: 0.997 r2: -15.058\n",
            "Epoch 760 loss: 0.998 r2: -15.414\n",
            "Epoch 770 loss: 0.998 r2: -15.715\n",
            "Epoch 780 loss: 0.998 r2: -15.950\n",
            "Epoch 790 loss: 0.999 r2: -16.115\n",
            "Epoch 800 loss: 0.999 r2: -16.210\n",
            "Epoch 810 loss: 0.999 r2: -16.239\n",
            "Epoch 820 loss: 1.000 r2: -16.205\n",
            "Epoch 830 loss: 1.000 r2: -16.112\n",
            "Epoch 840 loss: 1.001 r2: -15.966\n",
            "Epoch 850 loss: 1.001 r2: -15.772\n",
            "Epoch 860 loss: 1.001 r2: -15.540\n",
            "Epoch 870 loss: 1.002 r2: -15.280\n",
            "Epoch 880 loss: 1.003 r2: -14.984\n",
            "Epoch 890 loss: 1.003 r2: -14.625\n",
            "Epoch 900 loss: 1.004 r2: -14.223\n",
            "Epoch 910 loss: 1.005 r2: -13.796\n",
            "Epoch 920 loss: 1.006 r2: -13.367\n",
            "Epoch 930 loss: 1.007 r2: -12.917\n",
            "Epoch 940 loss: 1.008 r2: -12.438\n",
            "Epoch 950 loss: 1.009 r2: -11.961\n",
            "Epoch 960 loss: 1.009 r2: -11.503\n",
            "Epoch 970 loss: 1.010 r2: -11.067\n",
            "Epoch 980 loss: 1.011 r2: -10.656\n",
            "Epoch 990 loss: 1.012 r2: -10.276\n",
            "Weights: [14.714192194159223, 25.09934830779878, 60.267377170312024, -55.335819056462746, -0.16331783791428214, -9.712873466937964, 0.11840093169447817, -0.24615881845745027]\n",
            "Bias: [-70.09246641548563, -83.3854085676576, 0.04688266146537606]\n",
            "Learning rate: 0.005\n",
            "Loss for test dataset\n",
            "Loss: 0.868, r2: -4.110\n",
            "Loss for training dataset\n",
            "Loss: 1.013, r2: -9.960\n",
            "\n",
            "\n",
            "Epoch 0 loss: 1.028 r2: -7.617\n",
            "Epoch 10 loss: 1.033 r2: -8.385\n",
            "Epoch 20 loss: 1.034 r2: -8.690\n",
            "Epoch 30 loss: 1.035 r2: -8.664\n",
            "Epoch 40 loss: 1.036 r2: -8.588\n",
            "Epoch 50 loss: 1.037 r2: -8.521\n",
            "Epoch 60 loss: 1.037 r2: -8.473\n",
            "Epoch 70 loss: 1.038 r2: -8.454\n",
            "Epoch 80 loss: 1.038 r2: -8.467\n",
            "Epoch 90 loss: 1.038 r2: -8.513\n",
            "Epoch 100 loss: 1.038 r2: -8.590\n",
            "Epoch 110 loss: 1.038 r2: -8.696\n",
            "Epoch 120 loss: 1.038 r2: -8.825\n",
            "Epoch 130 loss: 1.038 r2: -8.979\n",
            "Epoch 140 loss: 1.038 r2: -9.149\n",
            "Epoch 150 loss: 1.038 r2: -9.324\n",
            "Epoch 160 loss: 1.038 r2: -9.502\n",
            "Epoch 170 loss: 1.037 r2: -9.682\n",
            "Epoch 180 loss: 1.037 r2: -9.872\n",
            "Epoch 190 loss: 1.037 r2: -10.069\n",
            "Epoch 200 loss: 1.037 r2: -10.269\n",
            "Epoch 210 loss: 1.037 r2: -10.468\n",
            "Epoch 220 loss: 1.036 r2: -10.663\n",
            "Epoch 230 loss: 1.036 r2: -10.851\n",
            "Epoch 240 loss: 1.036 r2: -11.030\n",
            "Epoch 250 loss: 1.036 r2: -11.198\n",
            "Epoch 260 loss: 1.036 r2: -11.356\n",
            "Epoch 270 loss: 1.036 r2: -11.502\n",
            "Epoch 280 loss: 1.036 r2: -11.637\n",
            "Epoch 290 loss: 1.036 r2: -11.761\n",
            "Epoch 300 loss: 1.036 r2: -11.874\n",
            "Epoch 310 loss: 1.036 r2: -11.977\n",
            "Epoch 320 loss: 1.036 r2: -12.070\n",
            "Epoch 330 loss: 1.036 r2: -12.153\n",
            "Epoch 340 loss: 1.036 r2: -12.228\n",
            "Epoch 350 loss: 1.036 r2: -12.337\n",
            "Epoch 360 loss: 1.036 r2: -12.568\n",
            "Epoch 370 loss: 1.035 r2: -12.865\n",
            "Epoch 380 loss: 1.034 r2: -13.196\n",
            "Epoch 390 loss: 1.034 r2: -13.546\n",
            "Epoch 400 loss: 1.033 r2: -13.908\n",
            "Epoch 410 loss: 1.032 r2: -14.282\n",
            "Epoch 420 loss: 1.032 r2: -14.677\n",
            "Epoch 430 loss: 1.031 r2: -15.087\n",
            "Epoch 440 loss: 1.030 r2: -15.507\n",
            "Epoch 450 loss: 1.030 r2: -15.935\n",
            "Epoch 460 loss: 1.029 r2: -16.371\n",
            "Epoch 470 loss: 1.029 r2: -16.820\n",
            "Epoch 480 loss: 1.028 r2: -17.277\n",
            "Epoch 490 loss: 1.028 r2: -17.742\n",
            "Epoch 500 loss: 1.027 r2: -18.215\n",
            "Epoch 510 loss: 1.027 r2: -18.694\n",
            "Epoch 520 loss: 1.026 r2: -19.186\n",
            "Epoch 530 loss: 1.026 r2: -19.689\n",
            "Epoch 540 loss: 1.025 r2: -20.202\n",
            "Epoch 550 loss: 1.025 r2: -20.728\n",
            "Epoch 560 loss: 1.025 r2: -21.267\n",
            "Epoch 570 loss: 1.024 r2: -21.817\n",
            "Epoch 580 loss: 1.024 r2: -22.378\n",
            "Epoch 590 loss: 1.023 r2: -22.948\n",
            "Epoch 600 loss: 1.023 r2: -23.529\n",
            "Epoch 610 loss: 1.023 r2: -24.120\n",
            "Epoch 620 loss: 1.022 r2: -24.721\n",
            "Epoch 630 loss: 1.022 r2: -25.331\n",
            "Epoch 640 loss: 1.021 r2: -25.951\n",
            "Epoch 650 loss: 1.021 r2: -26.580\n",
            "Epoch 660 loss: 1.021 r2: -27.216\n",
            "Epoch 670 loss: 1.020 r2: -27.860\n",
            "Epoch 680 loss: 1.020 r2: -28.510\n",
            "Epoch 690 loss: 1.020 r2: -29.166\n",
            "Epoch 700 loss: 1.020 r2: -29.826\n",
            "Epoch 710 loss: 1.019 r2: -30.491\n",
            "Epoch 720 loss: 1.019 r2: -31.159\n",
            "Epoch 730 loss: 1.019 r2: -31.830\n",
            "Epoch 740 loss: 1.019 r2: -32.501\n",
            "Epoch 750 loss: 1.018 r2: -33.172\n",
            "Epoch 760 loss: 1.018 r2: -33.841\n",
            "Epoch 770 loss: 1.018 r2: -34.506\n",
            "Epoch 780 loss: 1.018 r2: -35.168\n",
            "Epoch 790 loss: 1.018 r2: -35.823\n",
            "Epoch 800 loss: 1.018 r2: -36.471\n",
            "Epoch 810 loss: 1.018 r2: -37.116\n",
            "Epoch 820 loss: 1.017 r2: -37.751\n",
            "Epoch 830 loss: 1.017 r2: -38.374\n",
            "Epoch 840 loss: 1.017 r2: -38.984\n",
            "Epoch 850 loss: 1.017 r2: -39.580\n",
            "Epoch 860 loss: 1.017 r2: -40.161\n",
            "Epoch 870 loss: 1.017 r2: -40.723\n",
            "Epoch 880 loss: 1.017 r2: -41.264\n",
            "Epoch 890 loss: 1.017 r2: -41.786\n",
            "Epoch 900 loss: 1.017 r2: -42.290\n",
            "Epoch 910 loss: 1.017 r2: -42.775\n",
            "Epoch 920 loss: 1.017 r2: -43.241\n",
            "Epoch 930 loss: 1.017 r2: -43.687\n",
            "Epoch 940 loss: 1.017 r2: -44.114\n",
            "Epoch 950 loss: 1.017 r2: -44.521\n",
            "Epoch 960 loss: 1.017 r2: -44.909\n",
            "Epoch 970 loss: 1.017 r2: -45.279\n",
            "Epoch 980 loss: 1.017 r2: -45.631\n",
            "Epoch 990 loss: 1.017 r2: -45.966\n",
            "Weights: [26.652456203004164, 38.187880028816735, 101.06385232324179, -90.32678993567814, 1.8909431366179272, -12.25718580943762, 0.0587332807415025, -0.08313779601267723]\n",
            "Bias: [-123.70279077201842, -135.52215106434107, 0.05465069921750827]\n",
            "Learning rate: 0.006\n",
            "Loss for test dataset\n",
            "Loss: 0.857, r2: -17.436\n",
            "Loss for training dataset\n",
            "Loss: 1.017, r2: -46.255\n",
            "\n",
            "\n",
            "Epoch 0 loss: 1.121 r2: -6.102\n",
            "Epoch 10 loss: 1.120 r2: -24.883\n",
            "Epoch 20 loss: 1.128 r2: -27.517\n",
            "Epoch 30 loss: 1.134 r2: -28.841\n",
            "Epoch 40 loss: 1.141 r2: -29.107\n",
            "Epoch 50 loss: 1.148 r2: -28.810\n",
            "Epoch 60 loss: 1.156 r2: -28.227\n",
            "Epoch 70 loss: 1.164 r2: -27.500\n",
            "Epoch 80 loss: 1.171 r2: -26.697\n",
            "Epoch 90 loss: 1.179 r2: -25.853\n",
            "Epoch 100 loss: 1.187 r2: -24.994\n",
            "Epoch 110 loss: 1.195 r2: -24.132\n",
            "Epoch 120 loss: 1.203 r2: -23.259\n",
            "Epoch 130 loss: 1.211 r2: -22.406\n",
            "Epoch 140 loss: 1.219 r2: -21.581\n",
            "Epoch 150 loss: 1.227 r2: -20.787\n",
            "Epoch 160 loss: 1.235 r2: -20.026\n",
            "Epoch 170 loss: 1.243 r2: -19.301\n",
            "Epoch 180 loss: 1.251 r2: -18.610\n",
            "Epoch 190 loss: 1.258 r2: -17.953\n",
            "Epoch 200 loss: 1.266 r2: -17.331\n",
            "Epoch 210 loss: 1.274 r2: -16.740\n",
            "Epoch 220 loss: 1.282 r2: -16.182\n",
            "Epoch 230 loss: 1.290 r2: -15.654\n",
            "Epoch 240 loss: 1.298 r2: -15.155\n",
            "Epoch 250 loss: 1.305 r2: -14.683\n",
            "Epoch 260 loss: 1.313 r2: -14.236\n",
            "Epoch 270 loss: 1.321 r2: -13.812\n",
            "Epoch 280 loss: 1.329 r2: -13.411\n",
            "Epoch 290 loss: 1.336 r2: -13.025\n",
            "Epoch 300 loss: 1.344 r2: -12.652\n",
            "Epoch 310 loss: 1.352 r2: -12.294\n",
            "Epoch 320 loss: 1.361 r2: -11.948\n",
            "Epoch 330 loss: 1.369 r2: -11.615\n",
            "Epoch 340 loss: 1.377 r2: -11.295\n",
            "Epoch 350 loss: 1.386 r2: -10.987\n",
            "Epoch 360 loss: 1.394 r2: -10.692\n",
            "Epoch 370 loss: 1.403 r2: -10.408\n",
            "Epoch 380 loss: 1.411 r2: -10.137\n",
            "Epoch 390 loss: 1.415 r2: -10.028\n",
            "Epoch 400 loss: 1.414 r2: -10.063\n",
            "Epoch 410 loss: 1.414 r2: -10.104\n",
            "Epoch 420 loss: 1.413 r2: -10.153\n",
            "Epoch 430 loss: 1.412 r2: -10.203\n",
            "Epoch 440 loss: 1.411 r2: -10.256\n",
            "Epoch 450 loss: 1.410 r2: -10.312\n",
            "Epoch 460 loss: 1.409 r2: -10.369\n",
            "Epoch 470 loss: 1.407 r2: -10.430\n",
            "Epoch 480 loss: 1.406 r2: -10.493\n",
            "Epoch 490 loss: 1.405 r2: -10.558\n",
            "Epoch 500 loss: 1.403 r2: -10.626\n",
            "Epoch 510 loss: 1.402 r2: -10.697\n",
            "Epoch 520 loss: 1.400 r2: -10.770\n",
            "Epoch 530 loss: 1.399 r2: -10.846\n",
            "Epoch 540 loss: 1.397 r2: -10.925\n",
            "Epoch 550 loss: 1.395 r2: -11.007\n",
            "Epoch 560 loss: 1.394 r2: -11.091\n",
            "Epoch 570 loss: 1.392 r2: -11.178\n",
            "Epoch 580 loss: 1.390 r2: -11.269\n",
            "Epoch 590 loss: 1.388 r2: -11.362\n",
            "Epoch 600 loss: 1.386 r2: -11.458\n",
            "Epoch 610 loss: 1.384 r2: -11.558\n",
            "Epoch 620 loss: 1.382 r2: -11.661\n",
            "Epoch 630 loss: 1.380 r2: -11.767\n",
            "Epoch 640 loss: 1.378 r2: -11.877\n",
            "Epoch 650 loss: 1.376 r2: -11.991\n",
            "Epoch 660 loss: 1.373 r2: -12.108\n",
            "Epoch 670 loss: 1.371 r2: -12.228\n",
            "Epoch 680 loss: 1.369 r2: -12.353\n",
            "Epoch 690 loss: 1.367 r2: -12.482\n",
            "Epoch 700 loss: 1.364 r2: -12.615\n",
            "Epoch 710 loss: 1.362 r2: -12.752\n",
            "Epoch 720 loss: 1.359 r2: -12.894\n",
            "Epoch 730 loss: 1.357 r2: -13.040\n",
            "Epoch 740 loss: 1.354 r2: -13.191\n",
            "Epoch 750 loss: 1.352 r2: -13.347\n",
            "Epoch 760 loss: 1.349 r2: -13.508\n",
            "Epoch 770 loss: 1.347 r2: -13.674\n",
            "Epoch 780 loss: 1.344 r2: -13.845\n",
            "Epoch 790 loss: 1.341 r2: -14.022\n",
            "Epoch 800 loss: 1.338 r2: -14.205\n",
            "Epoch 810 loss: 1.336 r2: -14.393\n",
            "Epoch 820 loss: 1.333 r2: -14.588\n",
            "Epoch 830 loss: 1.330 r2: -14.789\n",
            "Epoch 840 loss: 1.327 r2: -14.996\n",
            "Epoch 850 loss: 1.324 r2: -15.209\n",
            "Epoch 860 loss: 1.322 r2: -15.430\n",
            "Epoch 870 loss: 1.319 r2: -15.657\n",
            "Epoch 880 loss: 1.316 r2: -15.892\n",
            "Epoch 890 loss: 1.313 r2: -16.133\n",
            "Epoch 900 loss: 1.310 r2: -16.382\n",
            "Epoch 910 loss: 1.307 r2: -16.639\n",
            "Epoch 920 loss: 1.304 r2: -16.903\n",
            "Epoch 930 loss: 1.301 r2: -17.174\n",
            "Epoch 940 loss: 1.298 r2: -17.454\n",
            "Epoch 950 loss: 1.295 r2: -17.741\n",
            "Epoch 960 loss: 1.293 r2: -18.036\n",
            "Epoch 970 loss: 1.290 r2: -18.339\n",
            "Epoch 980 loss: 1.287 r2: -18.651\n",
            "Epoch 990 loss: 1.284 r2: -18.970\n",
            "Weights: [38.73734577154552, 51.87100862890156, 139.6183009339893, -117.03943621667004, 1.7941858210701362, -10.903864839387703, 0.020233993153609468, 0.09892895454862989]\n",
            "Bias: [-175.09180075524094, -171.7637420953913, 0.06694300531835429]\n",
            "Learning rate: 0.007\n",
            "Loss for test dataset\n",
            "Loss: 1.113, r2: -15.973\n",
            "Loss for training dataset\n",
            "Loss: 1.281, r2: -19.263\n",
            "\n",
            "\n",
            "Epoch 0 loss: 1.439 r2: -8.562\n",
            "Epoch 10 loss: 1.360 r2: -14.047\n",
            "Epoch 20 loss: 1.357 r2: -13.742\n",
            "Epoch 30 loss: 1.359 r2: -13.716\n",
            "Epoch 40 loss: 1.357 r2: -13.914\n",
            "Epoch 50 loss: 1.349 r2: -14.296\n",
            "Epoch 60 loss: 1.342 r2: -14.710\n",
            "Epoch 70 loss: 1.335 r2: -15.062\n",
            "Epoch 80 loss: 1.330 r2: -15.326\n",
            "Epoch 90 loss: 1.326 r2: -15.484\n",
            "Epoch 100 loss: 1.322 r2: -15.668\n",
            "Epoch 110 loss: 1.319 r2: -15.636\n",
            "Epoch 120 loss: 1.318 r2: -15.271\n",
            "Epoch 130 loss: 1.319 r2: -14.958\n",
            "Epoch 140 loss: 1.315 r2: -14.900\n",
            "Epoch 150 loss: 1.184 r2: -14.999\n",
            "Epoch 160 loss: 1.021 r2: 0.000\n",
            "Epoch 170 loss: 1.021 r2: 0.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-5889eb419d3d>:146: RuntimeWarning: overflow encountered in double_scalars\n",
            "  sum_o1 = self.w7 * h1 + self.w8 * h2 + self.b3\n",
            "<ipython-input-9-5889eb419d3d>:140: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.w3 * x[2] + self.b1\n",
            "<ipython-input-9-5889eb419d3d>:143: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  sum_h2 = self.w4 * x[0] + self.w5 * x[1] + self.w6 * x[2] + self.b2\n",
            "<ipython-input-9-5889eb419d3d>:146: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  sum_o1 = self.w7 * h1 + self.w8 * h2 + self.b3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 180 loss: 1.021 r2: 0.000\n",
            "Epoch 190 loss: 1.021 r2: 0.000\n",
            "Epoch 200 loss: 1.021 r2: 0.000\n",
            "Epoch 210 loss: 1.021 r2: 0.000\n",
            "Epoch 220 loss: 1.021 r2: 0.000\n",
            "Epoch 230 loss: 1.021 r2: 0.000\n",
            "Epoch 240 loss: 1.021 r2: 0.000\n",
            "Epoch 250 loss: 1.021 r2: 0.000\n",
            "Epoch 260 loss: 1.021 r2: 0.000\n",
            "Epoch 270 loss: 1.021 r2: 0.000\n",
            "Epoch 280 loss: 1.021 r2: 0.000\n",
            "Epoch 290 loss: 1.021 r2: 0.000\n",
            "Epoch 300 loss: 1.021 r2: 0.000\n",
            "Epoch 310 loss: 1.021 r2: 0.000\n",
            "Epoch 320 loss: 1.021 r2: 0.000\n",
            "Epoch 330 loss: 1.021 r2: 0.000\n",
            "Epoch 340 loss: 1.021 r2: 0.000\n",
            "Epoch 350 loss: 1.021 r2: 0.000\n",
            "Epoch 360 loss: 1.021 r2: 0.000\n",
            "Epoch 370 loss: 1.021 r2: 0.000\n",
            "Epoch 380 loss: 1.021 r2: 0.000\n",
            "Epoch 390 loss: 1.021 r2: 0.000\n",
            "Epoch 400 loss: 1.021 r2: 0.000\n",
            "Epoch 410 loss: 1.021 r2: 0.000\n",
            "Epoch 420 loss: 1.021 r2: 0.000\n",
            "Epoch 430 loss: 1.021 r2: 0.000\n",
            "Epoch 440 loss: 1.021 r2: 0.000\n",
            "Epoch 450 loss: 1.021 r2: 0.000\n",
            "Epoch 460 loss: 1.021 r2: 0.000\n",
            "Epoch 470 loss: 1.021 r2: 0.000\n",
            "Epoch 480 loss: 1.021 r2: 0.000\n",
            "Epoch 490 loss: 1.021 r2: 0.000\n",
            "Epoch 500 loss: 1.021 r2: 0.000\n",
            "Epoch 510 loss: 1.021 r2: 0.000\n",
            "Epoch 520 loss: 1.021 r2: 0.000\n",
            "Epoch 530 loss: 1.021 r2: 0.000\n",
            "Epoch 540 loss: 1.021 r2: 0.000\n",
            "Epoch 550 loss: 1.021 r2: 0.000\n",
            "Epoch 560 loss: 1.021 r2: 0.000\n",
            "Epoch 570 loss: 1.021 r2: 0.000\n",
            "Epoch 580 loss: 1.021 r2: 0.000\n",
            "Epoch 590 loss: 1.021 r2: 0.000\n",
            "Epoch 600 loss: 1.021 r2: 0.000\n",
            "Epoch 610 loss: 1.021 r2: 0.000\n",
            "Epoch 620 loss: 1.021 r2: 0.000\n",
            "Epoch 630 loss: 1.021 r2: 0.000\n",
            "Epoch 640 loss: 1.021 r2: 0.000\n",
            "Epoch 650 loss: 1.021 r2: 0.000\n",
            "Epoch 660 loss: 1.021 r2: 0.000\n",
            "Epoch 670 loss: 1.021 r2: 0.000\n",
            "Epoch 680 loss: 1.021 r2: 0.000\n",
            "Epoch 690 loss: 1.021 r2: 0.000\n",
            "Epoch 700 loss: 1.021 r2: 0.000\n",
            "Epoch 710 loss: 1.021 r2: 0.000\n",
            "Epoch 720 loss: 1.021 r2: 0.000\n",
            "Epoch 730 loss: 1.021 r2: 0.000\n",
            "Epoch 740 loss: 1.021 r2: 0.000\n",
            "Epoch 750 loss: 1.021 r2: 0.000\n",
            "Epoch 760 loss: 1.021 r2: 0.000\n",
            "Epoch 770 loss: 1.021 r2: 0.000\n",
            "Epoch 780 loss: 1.021 r2: 0.000\n",
            "Epoch 790 loss: 1.021 r2: 0.000\n",
            "Epoch 800 loss: 1.021 r2: 0.000\n",
            "Epoch 810 loss: 1.021 r2: 0.000\n",
            "Epoch 820 loss: 1.021 r2: 0.000\n",
            "Epoch 830 loss: 1.021 r2: 0.000\n",
            "Epoch 840 loss: 1.021 r2: 0.000\n",
            "Epoch 850 loss: 1.021 r2: 0.000\n",
            "Epoch 860 loss: 1.021 r2: 0.000\n",
            "Epoch 870 loss: 1.021 r2: 0.000\n",
            "Epoch 880 loss: 1.021 r2: 0.000\n",
            "Epoch 890 loss: 1.021 r2: 0.000\n",
            "Epoch 900 loss: 1.021 r2: 0.000\n",
            "Epoch 910 loss: 1.021 r2: 0.000\n",
            "Epoch 920 loss: 1.021 r2: 0.000\n",
            "Epoch 930 loss: 1.021 r2: 0.000\n",
            "Epoch 940 loss: 1.021 r2: 0.000\n",
            "Epoch 950 loss: 1.021 r2: 0.000\n",
            "Epoch 960 loss: 1.021 r2: 0.000\n",
            "Epoch 970 loss: 1.021 r2: 0.000\n",
            "Epoch 980 loss: 1.021 r2: 0.000\n",
            "Epoch 990 loss: 1.021 r2: 0.000\n",
            "Weights: [nan, nan, nan, nan, nan, nan, nan, nan]\n",
            "Bias: [nan, nan, nan]\n",
            "Learning rate: 0.008\n",
            "Loss for test dataset\n",
            "Loss: 0.914, r2: 0.000\n",
            "Loss for training dataset\n",
            "Loss: 1.021, r2: 0.000\n",
            "\n",
            "\n",
            "Epoch 0 loss: 1.021 r2: 0.000\n",
            "Epoch 10 loss: 1.021 r2: 0.000\n",
            "Epoch 20 loss: 1.021 r2: 0.000\n",
            "Epoch 30 loss: 1.021 r2: 0.000\n",
            "Epoch 40 loss: 1.021 r2: 0.000\n",
            "Epoch 50 loss: 1.021 r2: 0.000\n",
            "Epoch 60 loss: 1.021 r2: 0.000\n",
            "Epoch 70 loss: 1.021 r2: 0.000\n",
            "Epoch 80 loss: 1.021 r2: 0.000\n",
            "Epoch 90 loss: 1.021 r2: 0.000\n",
            "Epoch 100 loss: 1.021 r2: 0.000\n",
            "Epoch 110 loss: 1.021 r2: 0.000\n",
            "Epoch 120 loss: 1.021 r2: 0.000\n",
            "Epoch 130 loss: 1.021 r2: 0.000\n",
            "Epoch 140 loss: 1.021 r2: 0.000\n",
            "Epoch 150 loss: 1.021 r2: 0.000\n",
            "Epoch 160 loss: 1.021 r2: 0.000\n",
            "Epoch 170 loss: 1.021 r2: 0.000\n",
            "Epoch 180 loss: 1.021 r2: 0.000\n",
            "Epoch 190 loss: 1.021 r2: 0.000\n",
            "Epoch 200 loss: 1.021 r2: 0.000\n",
            "Epoch 210 loss: 1.021 r2: 0.000\n",
            "Epoch 220 loss: 1.021 r2: 0.000\n",
            "Epoch 230 loss: 1.021 r2: 0.000\n",
            "Epoch 240 loss: 1.021 r2: 0.000\n",
            "Epoch 250 loss: 1.021 r2: 0.000\n",
            "Epoch 260 loss: 1.021 r2: 0.000\n",
            "Epoch 270 loss: 1.021 r2: 0.000\n",
            "Epoch 280 loss: 1.021 r2: 0.000\n",
            "Epoch 290 loss: 1.021 r2: 0.000\n",
            "Epoch 300 loss: 1.021 r2: 0.000\n",
            "Epoch 310 loss: 1.021 r2: 0.000\n",
            "Epoch 320 loss: 1.021 r2: 0.000\n",
            "Epoch 330 loss: 1.021 r2: 0.000\n",
            "Epoch 340 loss: 1.021 r2: 0.000\n",
            "Epoch 350 loss: 1.021 r2: 0.000\n",
            "Epoch 360 loss: 1.021 r2: 0.000\n",
            "Epoch 370 loss: 1.021 r2: 0.000\n",
            "Epoch 380 loss: 1.021 r2: 0.000\n",
            "Epoch 390 loss: 1.021 r2: 0.000\n",
            "Epoch 400 loss: 1.021 r2: 0.000\n",
            "Epoch 410 loss: 1.021 r2: 0.000\n",
            "Epoch 420 loss: 1.021 r2: 0.000\n",
            "Epoch 430 loss: 1.021 r2: 0.000\n",
            "Epoch 440 loss: 1.021 r2: 0.000\n",
            "Epoch 450 loss: 1.021 r2: 0.000\n",
            "Epoch 460 loss: 1.021 r2: 0.000\n",
            "Epoch 470 loss: 1.021 r2: 0.000\n",
            "Epoch 480 loss: 1.021 r2: 0.000\n",
            "Epoch 490 loss: 1.021 r2: 0.000\n",
            "Epoch 500 loss: 1.021 r2: 0.000\n",
            "Epoch 510 loss: 1.021 r2: 0.000\n",
            "Epoch 520 loss: 1.021 r2: 0.000\n",
            "Epoch 530 loss: 1.021 r2: 0.000\n",
            "Epoch 540 loss: 1.021 r2: 0.000\n",
            "Epoch 550 loss: 1.021 r2: 0.000\n",
            "Epoch 560 loss: 1.021 r2: 0.000\n",
            "Epoch 570 loss: 1.021 r2: 0.000\n",
            "Epoch 580 loss: 1.021 r2: 0.000\n",
            "Epoch 590 loss: 1.021 r2: 0.000\n",
            "Epoch 600 loss: 1.021 r2: 0.000\n",
            "Epoch 610 loss: 1.021 r2: 0.000\n",
            "Epoch 620 loss: 1.021 r2: 0.000\n",
            "Epoch 630 loss: 1.021 r2: 0.000\n",
            "Epoch 640 loss: 1.021 r2: 0.000\n",
            "Epoch 650 loss: 1.021 r2: 0.000\n",
            "Epoch 660 loss: 1.021 r2: 0.000\n",
            "Epoch 670 loss: 1.021 r2: 0.000\n",
            "Epoch 680 loss: 1.021 r2: 0.000\n",
            "Epoch 690 loss: 1.021 r2: 0.000\n",
            "Epoch 700 loss: 1.021 r2: 0.000\n",
            "Epoch 710 loss: 1.021 r2: 0.000\n",
            "Epoch 720 loss: 1.021 r2: 0.000\n",
            "Epoch 730 loss: 1.021 r2: 0.000\n",
            "Epoch 740 loss: 1.021 r2: 0.000\n",
            "Epoch 750 loss: 1.021 r2: 0.000\n",
            "Epoch 760 loss: 1.021 r2: 0.000\n",
            "Epoch 770 loss: 1.021 r2: 0.000\n",
            "Epoch 780 loss: 1.021 r2: 0.000\n",
            "Epoch 790 loss: 1.021 r2: 0.000\n",
            "Epoch 800 loss: 1.021 r2: 0.000\n",
            "Epoch 810 loss: 1.021 r2: 0.000\n",
            "Epoch 820 loss: 1.021 r2: 0.000\n",
            "Epoch 830 loss: 1.021 r2: 0.000\n",
            "Epoch 840 loss: 1.021 r2: 0.000\n",
            "Epoch 850 loss: 1.021 r2: 0.000\n",
            "Epoch 860 loss: 1.021 r2: 0.000\n",
            "Epoch 870 loss: 1.021 r2: 0.000\n",
            "Epoch 880 loss: 1.021 r2: 0.000\n",
            "Epoch 890 loss: 1.021 r2: 0.000\n",
            "Epoch 900 loss: 1.021 r2: 0.000\n",
            "Epoch 910 loss: 1.021 r2: 0.000\n",
            "Epoch 920 loss: 1.021 r2: 0.000\n",
            "Epoch 930 loss: 1.021 r2: 0.000\n",
            "Epoch 940 loss: 1.021 r2: 0.000\n",
            "Epoch 950 loss: 1.021 r2: 0.000\n",
            "Epoch 960 loss: 1.021 r2: 0.000\n",
            "Epoch 970 loss: 1.021 r2: 0.000\n",
            "Epoch 980 loss: 1.021 r2: 0.000\n",
            "Epoch 990 loss: 1.021 r2: 0.000\n",
            "Weights: [nan, nan, nan, nan, nan, nan, nan, nan]\n",
            "Bias: [nan, nan, nan]\n",
            "Learning rate: 0.009\n",
            "Loss for test dataset\n",
            "Loss: 0.914, r2: 0.000\n",
            "Loss for training dataset\n",
            "Loss: 1.021, r2: 0.000\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}